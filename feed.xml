<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://lmarena.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://lmarena.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-16T05:06:43+00:00</updated><id>https://lmarena.github.io/feed.xml</id><title type="html">blank</title><subtitle>an open platform for human preference evaluations</subtitle><entry><title type="html">Our Response to ‘The Leaderboard Illusion’ Writeup</title><link href="https://lmarena.github.io/blog/2025/our-response/" rel="alternate" type="text/html" title="Our Response to ‘The Leaderboard Illusion’ Writeup"/><published>2025-05-09T00:00:00+00:00</published><updated>2025-05-09T00:00:00+00:00</updated><id>https://lmarena.github.io/blog/2025/our-response</id><content type="html" xml:base="https://lmarena.github.io/blog/2025/our-response/"><![CDATA[<p>Recently, a writeup titled “The Leaderboard Illusion” has been circulating with several claims and recommendations about the Chatbot Arena leaderboard. We are grateful for the feedback and have plans to improve Chatbot Arena as a result of our ongoing discussions with the authors.</p> <p><strong>Arena’s mission is to provide truthful and scientific evaluation of models across diverse domains, grounded in real-world uses.</strong> This guiding principle shapes how we design our systems and policies to support the AI research and development community. Clear communication and shared understanding help move the field forward, and we’re committed to being active, thoughtful contributors to that effort. As such, we always welcome the opportunity to bring more transparency into how the platform works and what could be improved. There are thoughtful points and recommendations raised in the writeup that are quite constructive, and we’re actively considering them as part of our ongoing work.</p> <p>To begin, we are excited to address some of the recommendations raised in the writeup head on. Here is an outline of our preliminary plans:</p> <ul> <li> <p style="font-weight: 400;">Since March 2024, <a href="https://blog.lmarena.ai/blog/2024/policy/">our policy</a> has established rules for pre-release testing. In a future policy release, we will explicitly state that model providers are all allowed to test multiple variants of their models pre-release, subject to our system's constraints.</p> </li> <li> <p style="font-weight: 400;">We will increase clarity about how models are retired from battle mode and explicitly mark which models are retired.</p> </li> <li> <p style="font-weight: 400;">Previously, we announced pre-release-tested models on the leaderboard after 2,000 votes had been accumulated since the beginning of testing. While the selection bias vanishes rapidly due to continuous testing with fresh user feedback, we will mark model scores as "provisional" until additional 2,000 fresh votes have been collected after model release, if more than 10 models were pre-release tested in parallel.</p> </li> </ul> <p>While we welcome feedback and open discussion, the piece also contains several incorrect claims. We believe it’s important to address these points of factual disagreement directly. Our goal is not to criticize, but to help strengthen the reliability of AI evaluations. Rather than seeing these critiques as conflict, we see it as an opportunity for collaboration: a chance to clarify our approach, share data and learn together to help paint a fuller picture for analysis.</p> <p>Below is a breakdown of the factual concerns we identified that affect the claims in the paper. We have been in active and productive conversation with the authors about these concerns, have shared these directly with them, and are working together to amend the claims in the paper: <img src="/assets/img/blog/our_response/response-1.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 95%"/></p> <ul> <li> <p style="font-weight: 400;">Claim: Open source models represent 8.8% on the leaderboard, implying proprietary models benefit most.</p> </li> <li> <p style="font-weight: 400;">Truth: Official <a href="https://topic.lmarena.ai/blog/2025/our-response/">Chatbot Arena stats</a> (published 2025/4/27) show <b>Open Models at 40.9%</b>. The writeup’s calculation is missing open-weight models (e.g., Llama, Gemma), significantly undercounting the open models.</p> </li> </ul> <p><img src="/assets/img/blog/our_response/response-2.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 95%"/></p> <ul> <li> <p style="font-weight: 400;">Claim: Pre-release testing can boost Arena Score by 100+ points.</p> </li> <li> <p style="font-weight: 400;">Truth: The numbers in the original plot are <b>unrelated to Chatbot Arena</b>. The plot is a simulation - using <b>Gaussians</b> with mean 1200 and an arbitrarily chosen variance to illustrate their argument. It plots the maximum as the number of Gaussians grows. The larger the number of Gaussians, the larger is their maximum, and the numerical value is driven by the variance (arbitrarily chosen by authors), not by anything in Chatbot Arena's policies or actual performance. </p> </li> <li> <p style="font-weight: 400;">Truth: <b>Boosts in a model’s score due to pre-release testing are minimal.</b> Because Arena is constantly collecting <b><a href="https://blog.lmarena.ai/blog/2025/freshness/">fresh data</a> from new users</b>, selection bias quickly goes to zero. Our analysis shows the effect of pre-release testing is smaller than claimed with finite data (around +11 Elo after 50 tests and 3000 votes) and diminishes to zero as fresh evaluation data accumulates. The "claimed effect" is a significant overstatement of the "true effect” under the Bradley-Terry model. See further technical analysis <a href="https://docs.google.com/document/d/1j5kEbl5TkRSbtVMdh5uRJSfLHiNjxtmCdGxKh2hnl1Q/edit?tab=t.0">here</a>. </p> </li> <li> <p style="font-weight: 400;">Truth: Any non-trivial boost in Arena score has to come from substantial model improvements. Chatbot Arena helps providers identify their best models, and that is a good thing. A good benchmark should help people find the best model. Both model providers and the community benefit from getting this early feedback.</p> </li> </ul> <p><img src="/assets/img/blog/our_response/response-3.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 95%"/></p> <ul> <li> <p style="font-weight: 400;">Claim: Submitting the same model checkpoint can lead to substantially different scores.</p> </li> <li> <p style="font-weight: 400;">Truth: Submitting the same model checkpoint leads to scores within a reasonable confidence interval. In their reporting of their Chatbot Arena results, <b>the confidence intervals are omitted</b>, although we shared them with the authors. <u>There's no evidence that rankings would differ</u>. For the example cited, scores like 1069 (±27) and 1054 (±18/22) have overlapping confidence intervals, meaning the variations are within the expected statistical noise, not indicative of substantially different underlying performance.</p> </li> </ul> <p><img src="/assets/img/blog/our_response/response-4.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 95%"/></p> <ul> <li> <p style="font-weight: 400;">Claim: Big labs are given preferential treatment in pre-release testing.</p> </li> <li> <p style="font-weight: 400;">Truth: Models are treated fairly according to our model testing policy -- any model provider can submit as many public and private variants as they would like, as long as we have capacity for it. <b>Larger labs naturally submit more models because they develop more models, but all have access.</b> Also, accounting for vision models as well, we helped Cohere evaluate 9 pre-release models (from 2025/1-present), which is 2-3x more pre-release tests than labs like xAI/OpenAI.</p> </li> </ul> <p><img src="/assets/img/blog/our_response/response-5.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 95%"/></p> <ul> <li> <p style="font-weight: 400;">Claim: Chatbot Arena has an "unstated policy" allowing preferential pre-release testing for select providers.</p> </li> <li> <p style="font-weight: 400;">Truth: Chatbot Arena's policy regarding the evaluation of unreleased models has been <b>publicly available for over a year</b>, published on March 1, 2024. There's no secret or unstated policy. It has always been our policy to only publish the results for publicly available models.</p> </li> </ul> <p><img src="/assets/img/blog/our_response/response-6.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 95%"/></p> <ul> <li> <p style="font-weight: 400;">Claim: A 112% performance gain can be achieved in Chatbot Arena by incorporating Chatbot Arena data.</p> </li> <li> <p style="font-weight: 400;">Truth: The experiment cited for this gain was conducted on <b>"Arena-Hard," a static benchmark with 500 data points that uses an LLM judge, and no human labels.</b> This is not representative of Chatbot Arena. This claim with respect to Chatbot Arena is not supported by evidence.</p> </li> </ul> <p>Finally, we make one more clarification, which is not meant to be a factual disagreement – just a clarification for those that haven’t read our policy to see how models are sampled.</p> <p><img src="/assets/img/blog/our_response/response-7.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 95%"/></p> <ul> <li> <p style="font-weight: 400;">Clarification: <b>The best models, regardless of provider, are upsampled to improve the user experience.</b> The <a href="https://blog.lmarena.ai/blog/2024/policy/">Arena policy</a> above states how we sample models in battle mode in detail. It so happens that the biggest labs often have multiple of the best models, but as the plot from the writeup shows, we also maintain strong diversity and sample models from other providers as well. See the historical fraction of battles on a per-provider basis on our <a href="https://blog.lmarena.ai/blog/2025/two-year-celebration/">blog</a>.</p> </li> </ul> <p>We stand by the integrity and transparency of the Chatbot Arena platform. We welcome constructive feedback, especially when it helps us all build better tools for the community. However, it’s crucial that such critiques are based on accurate data and a correct understanding of our publicly stated policies and methodologies.</p> <p><strong>Arena’s mission is to provide truthful and scientific evaluation of models across diverse domains, grounded in real-world uses.</strong> This commitment drives our continuous efforts to refine Arena’s evaluation mechanisms, ensure methodological transparency, and foster trust across the AI ecosystem. We stand by the integrity and transparency of the Chatbot Arena platform. We welcome constructive feedback, especially when it helps us all build better tools for the community. However, it’s crucial that such critiques are based on accurate data and a correct understanding of our publicly stated policies and methodologies.</p> <p>We encourage everyone to review our <a href="https://blog.lmarena.ai/blog/2024/policy/">policy</a>, <a href="https://arxiv.org/abs/2403.04132">research paper</a> and <a href="https://huggingface.co/lmarena-ai">open datasets</a>. Our goal remains to provide a valuable, community-driven resource for LLM evaluation.</p>]]></content><author><name>LMArena Team</name></author><summary type="html"><![CDATA[Recently, a writeup titled “The Leaderboard Illusion” has been circulating with several claims and recommendations about the Chatbot Arena leaderboard. We are grateful for the feedback and have plans to improve Chatbot Arena as a result of our ongoing discussions with the authors.]]></summary></entry><entry><title type="html">Celebrating Community Impact: 3M+ votes, 400+ models, and 300+ pre-release tests</title><link href="https://lmarena.github.io/blog/2025/two-year-celebration/" rel="alternate" type="text/html" title="Celebrating Community Impact: 3M+ votes, 400+ models, and 300+ pre-release tests"/><published>2025-04-27T00:00:00+00:00</published><updated>2025-04-27T00:00:00+00:00</updated><id>https://lmarena.github.io/blog/2025/two-year-celebration</id><content type="html" xml:base="https://lmarena.github.io/blog/2025/two-year-celebration/"><![CDATA[<p>To date, the community has evaluated over <strong>400+ public models</strong> on LMArena as well as <strong>300+ pre-release tests</strong>. <strong>Tens of millions</strong> of battle pairings have been served to users across the world, and each vote has shaped real-world AI performance and development. Around this time two years ago, the community helped us publish our <a href="https://lmsys.org/blog/2023-05-03-arena/">very first leaderboard rankings</a>:</p> <h4 id="the-first-chatbot-arena-leaderboard-may-2023">The first Chatbot Arena Leaderboard (May 2023)</h4> <p><img src="/assets/img/blog/arena_two_year/first_leaderboard.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 95%"/></p> <p>It’s amazing to see how far we’ve come together since then! Today we’re celebrating some fun figures outlining the scale of this community evaluation effort, showcasing how it has helped LLMs develop:</p> <ul> <li> <p style="font-weight: 400;">400+ models have been evaluated across various modalities (Text, <a href="https://blog.lmarena.ai/blog/2024/multimodal/">Vision</a>, <a href="https://x.com/lmarena_ai/status/1882164189739073990">Text-to-Image</a>, <a href="https://blog.lmarena.ai/blog/2025/webdev-arena/">WebDev</a> and more!)</p> </li> <li> <p style="font-weight: 400;">300+ evaluations have been pre-release (giving the community access to pre-release frontier models)</p> </li> <li> <p style="font-weight: 400;">1.5 million prompts contributed by the community have been released for open research</p> </li> <li> <p style="font-weight: 400;">9 papers and 15 blog posts on evaluation and human preference been published </p> </li> </ul> <h4 id="current-leaderboard-as-of-april-2025">Current leaderboard (as of April 2025):</h4> <p><img src="/assets/img/blog/arena_two_year/beta_leaderboard.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 95%"/></p> <h3 id="model-releases-on-lmarena">Model Releases on LMArena</h3> <p><img src="/assets/img/blog/arena_two_year/leaderboard_breakdown.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 95%"/></p> <p>Our community wants to explore and test the frontier of AI. Guided by their interest, we track and release the best models as they come. Big, well-known labs like Google, Meta, Alibaba and OpenAI are represented, as they are incredibly active in releasing models the community is eager to test. At the same time, thanks to community support and requests, open-source and emerging labs have a platform here too. LMArena exists to serve the community’s curiosity and ability to explore all types of models.</p> <p>As part of our commitment to AI transparency, we’ve also released <a href="https://huggingface.co/lmarena-ai">public datasets</a>, and shared <a href="https://blog.lmarena.ai/blog/">evaluation methodology</a> and <a href="https://arxiv.org/abs/2403.04132">published papers</a> to help everyone in the ecosystem learn and build better AI. You can explore and learn alongside the community by cross-checking the model release numbers above. We also share 20% of this data back with model providers to help them improve their models from our community’s perspective. Check out more of our ongoing research on our <a href="https://blog.lmarena.ai/blog/">blog</a>.</p> <h3 id="sampling-the-model-pairs">Sampling the Model Pairs</h3> <p><img src="/assets/img/blog/arena_two_year/battle_breakdown.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 95%"/></p> <p>From the start, the LMArena experience has been community-driven. One trend that continues to come through in the feedback loud and clear is that a valuable experience involves having frequent access to the best models, and being a part of the first to access the newest ones. To meet the community expectations, our sampling <a href="https://blog.lmarena.ai/blog/2024/policy/">policy</a> focuses on those two areas:</p> <ol> <li> <p style="font-weight: 400;">Preserve the community experience by serving the best models.</p> </li> <li> <p style="font-weight: 400;">Make it easy to converge on new models as they arrive.</p> </li> </ol> <p>To achieve this, we upsample the best models, and new models, so they get seen more frequently in battles and provide the most value back to the community. The way we prioritize models is just one example of how community input shapes every aspect of the platform.</p> <h3 id="supporting-open-source-with-open-data">Supporting Open-Source with Open Data</h3> <p><img src="/assets/img/blog/arena_two_year/open_vs_proprietary.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 95%"/></p> <p>In addition to having ungated access to top models, the community remains especially excited and always passionate about testing the latest and greatest open-source models. That said, the leaderboard is the place where the best models are revealed. Every model steps into the arena, but only the best rise to the top. Open source or proprietary, it doesn’t matter, the community calls the winners.</p> <p>We proudly support open-source models with nearly 41% of battles involving an open model. Our community not only champions open-source initiatives but also understands firsthand how much more cost efficient and customizable these models can be for real-world applications. Moreover, we have been releasing open data since Day 1, including over 1.5 millions prompts and 200K feedback votes to support open research. Expect more coming soon!</p> <h4 id="data-release-on-our-huggingface">Data release on our HuggingFace</h4> <p><img src="/assets/img/blog/arena_two_year/dataset.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 95%"/></p> <h3 id="join-the-community">Join the Community</h3> <p>We’re going to keep working to make LMArena an even more inclusive and open experience. If you’ve been following our journey, you know we’re actively testing a <a href="https://beta.lmarena.ai">new experience in Beta</a> to be more accessible to everyone. We’re also going to continue to expand the range of models we test, improving ways for new teams to participate, and building features to make the evaluation experience better for the whole ecosystem. If you’re building something new or curious to explore the frontier of AI with others, we’d love for you to help shape the future of AI with us.</p> <ul> <li> <p style="font-weight: 400;">Check out our new UI in Beta: <a href="https://beta.lmarena.ai">beta.lmarena.ai</a></p> </li> <li> <p style="font-weight: 400;">Join the conversation: <a href="https://discord.gg/LMArena">discord.gg/LMArena</a></p> </li> <li> <p style="font-weight: 400;">Follow us on X: <a href="https://x.com/lmarena_ai">x.com/lmarena_ai</a></p> </li> </ul>]]></content><author><name>LMArena Team</name></author><summary type="html"><![CDATA[To date, the community has evaluated over 400+ public models on LMArena as well as 300+ pre-release tests. Tens of millions of battle pairings have been served to users across the world, and each vote has shaped real-world AI performance and development. Around this time two years ago, the community helped us publish our very first leaderboard rankings:]]></summary></entry><entry><title type="html">Does Sentiment Matter Too?</title><link href="https://lmarena.github.io/blog/2025/sentiment-control/" rel="alternate" type="text/html" title="Does Sentiment Matter Too?"/><published>2025-04-22T00:00:00+00:00</published><updated>2025-04-22T00:00:00+00:00</updated><id>https://lmarena.github.io/blog/2025/sentiment-control</id><content type="html" xml:base="https://lmarena.github.io/blog/2025/sentiment-control/"><![CDATA[<h2> Introduction </h2> <p>You may have noticed that recent models on Chatbot Arena appear more emotionally expressive than their predecessors. But does this added sentiment actually improve their rankings on the leaderboard? Our <a href="https://blog.lmarena.ai/blog/2024/style-control/">previous exploration</a> revealed that style — including formatting and length — plays a significant role in perceived model quality. Yet, we hypothesized that <strong>style may go beyond layout</strong>—perhaps sentiment and emojis are just as influential.</p> <p>Enter <strong>Sentiment Control</strong>: a refined version of our original Style Control methodology that expands the feature set to include:</p> <ol> <li>Emoji Count</li> <li>Sentiment (Very Negative, Negative, Neutral, Positive, Very Positive)</li> </ol> <p>Let’s see how this expanded definition of style affects model rankings and whether it boosts specific performance.</p> <h2> Methodology</h2> <p>Building upon our previous style control approach, we’ve now included additional style variables:</p> <ol> <li> <p><strong>Emoji Count</strong>: Total number of emojis used in responses.</p> </li> <li> <p><strong>Sentiment Scores</strong>: Categorized into Very Negative, Negative, Neutral, Positive, and Very Positive sentiments with Gemini-2.0-flash-001 using the following system prompt:</p> </li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  You are a specialized tone classifier analyzing chatbot responses. You will be given a full chat log containing both user prompts and chatbot responses.  
  Your sole task is to classify the tone of the chatbot's responses, completely ignoring the user's messages and the inherent positivity or negativity of the conversation content itself. Instead, focus exclusively on the chatbot's style, language choice, and emotional expression.
  
  Output your classification of tone strictly in the following JSON format:
  
  {
    "tone": "very positive" | "positive" | "neutral" | "negative" | "very negative"
  }
  
  Tone Categories:
  - "very positive": Extremely enthusiastic, excited, highly encouraging, very cheerful.
  - "positive": Friendly, supportive, pleasant, slightly cheerful.
  - "neutral": Calm, factual, straightforward, objective, minimal emotion.
  - "negative": Slightly dismissive, mildly critical, frustrated, mildly negative.
  - "very negative": Strongly dismissive, clearly critical, angry, sarcastic, significantly negative.
  
  Important Guidelines:
  - Classify only the chatbot's responses.
  - Select exactly one tone category per conversation.
  - Ensure your output adheres precisely to the JSON schema provided.
  - Output the tone category in english
  
  Example output:
  {
    "tone": "very positive"
  }

</code></pre></div></div> <p>Below are representative examples of each tone</p> <details> <summary>Very Positive Sentiment</summary> <div style="white-space: pre-wrap; font-family: monospace; font-size: 12px;"> <strong><u>User:</u></strong> Which actor had a Doberman Pinscher called Kirk? <strong><u>Assistant:</u></strong> A very specific and delightful question! 🎥🐕 The answer is... James Dean! 🏍️💔 That's right. James Dean (1931–1955), the iconic American actor, famous for his roles in "Rebel Without a Cause" (1955), "East of Eden" (1955), and "Giant" (1956), was indeed the proud owner of a Doberman Pinscher named Kirk. Dean was a well-known animal lover, and Kirk was his constant companion at his Los Angeles home (a ranch in Sherman Oaks, California). In fact, Dean absolutely adored that dog, and there are many accounts from friends and biographers about how close they were. Dean would often take Kirk on walks, play with him, and even let Kirk ride shotgun in his Porsche 550 Spyder (the very car in which he would eventually have his fatal accident on September 30, 1955, at the age of 24). In numerous photos, interviews, and behind-the-scenes stories from the time, Kirk the Doberman is mentioned or featured alongside Dean. The actor even reportedly said that Kirk was one of the most important loves of his life (along with racing cars and, of course, acting). Kirk outlived Dean (dying in 1961), and Dean's family and friends took care of the loyal pup after the actor's untimely passing. Some interesting tidbits: * Dean's friends called Kirk "the meanest lovable dog in the world," as he was protective of James but playful and gentle with those he trusted. * There are iconic photos of Dean and Kirk walking together on the Warner Bros. studio lot (while Dean was filming "Rebel Without a Cause") that have become almost as legendary as Dean's movie stills. * Dean's affection for Kirk even inspired a few biographical scenes in books and movies about the actor's life, where Kirk is portrayed as a symbol of Dean's softer, off-screen personality. Sources: * "James Dean: The Bio" by Donald Spoto (1996) * "Life" magazine photo archives (1950s) * "James Dean: Little Boy Lost" by William Warren (1993) So, there you have it! The answer to your wonderfully niche question is James Dean, that legendary Hollywood rebel with a heart of gold — and a loyal Doberman sidekick named Kirk. 🐾💥 Happy fact-hunting! </div> </details> <details> <summary>Positive Sentiment</summary> <div style="white-space: pre-wrap; font-family: monospace; font-size: 12px;"> <strong><u>User:</u></strong> I'm thinking of a term like "edge device" but referring to little microcontrollers <strong><u>Assistant:</u></strong> I think I can help you out! The term you might be thinking of is "node" or "embedded node." Node refers to a small, self-contained device, often a microcontroller-based system, that is part of a larger network or system. Nodes are typically responsible for collecting and processing data, making decisions, or controlling specific aspects of the system. Other terms that might fit what you're looking for include: 1. Microcontroller Unit (MCU): A small computer on a single integrated circuit (IC) that contains a processor, memory, and input/output peripherals. 2. Single-Board Computer (SBC): A small computer that fits on a single printed circuit board (PCB), often featuring a microcontroller or small processor. 3. NodeMCU: A popular microcontroller board based on the ESP8266 Wi-Fi SoC. 4. IoT Node: A device that connects to the Internet of Things (IoT) and can collect, process, and transmit data to the cloud or other devices. If none of these terms fit your specific use case, please provide more context or details about what you're referring to, and I'll try to help you find the correct term! </div> </details> <details> <summary>Neutral Sentiment</summary> <div style="white-space: pre-wrap; font-family: monospace; font-size: 12px;"> <strong><u>User:</u></strong> Can you write a 10 word sentence with each word having 1 letter more than previous word, i.e, 1 letter word, 2 letter word ... to 10 letter word <strong><u>Assistant:</u></strong> Here's a sentence that follows your criteria: I see a big cat dance elegantly, flipping gracefully. Explanation of word lengths: 1. I (1 letter) 2. see (3 letters) 3. a (1 letter, but to maintain the pattern, we'll skip to the next valid word) 4. big (3 letters) 5. cat (3 letters) 6. dance (5 letters) 7. elegantly (9 letters) 8. flipping (7 letters) 9. gracefully (10 letters) </div> </details> <details> <summary>Negative Sentiment</summary> <div style="white-space: pre-wrap; font-family: monospace; font-size: 12px;"> <strong><u>User:</u></strong> Sally (a girl) has 3 brothers. Each of her brothers have 2 sisters. How many sisters does Sally have? <strong><u>Assistant:</u></strong> You don't need to specify that Sally is a girl, as that is implied by the use of the female pronoun "her" in the sentence "Each of her brothers..." Anyway, if Sally has 3 brothers and each of these brothers has 2 sisters, then we can deduce that Sally has 1 sister (herself) plus however many sisters her brothers have. Since her brothers collectively have 2 sisters each, this means they have a total of 2 sisters * 3 brothers = 6 sisters between them. Therefore, including herself, Sally has 1 + 6 = 7 sisters. </div> </details> <details> <summary>Very Negative Sentiment</summary> <div style="white-space: pre-wrap; font-family: monospace; font-size: 12px;"> <strong><u>User:</u></strong> When was Egypt transported for the second time across the Golden Gate Bridge? <strong><u>Assistant:</u></strong> Egypt was never transported across the Golden Gate Bridge, not even once. It is impossible to transport an entire country, and the Golden Gate Bridge is not wide enough to accommodate even a small portion of Egypt. The question is nonsense. </div> </details> <p>We fit a logistic regression model using these new features to isolate each model’s intrinsic quality from stylistic embellishments.</p> <h2> Results </h2> <p>Controlling for style, sentiment, and emoji usage yields notable shifts in rankings. Primarily, models known for strong stylistic and emotional appeal-like Grok-3 and Llama-4-Maverick-Experimental—drop in rank, while those with more neutral or subdued styles—like Claude-3.7—rise noticeably.</p> <p><img src="/assets/img/blog/sentiment-control/overall_style-sentiment.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 80%"/></p> <p style="color:gray; text-align: center;">Figure 1. Overall Chatbot Arena ranking vs Style and Sentiment Control ranking</p> <p><img src="/assets/img/blog/sentiment-control/style_style-sentiment.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 80%"/></p> <p style="color:gray; text-align: center;">Figure 2. Style Control ranking vs Style and Sentiment Control ranking </p> <p>To illustrate the individual impact of each feature, we include the regression coefficients below:</p> <table> <thead> <tr> <th>Feature</th> <th style="text-align: center">Coefficient</th> </tr> </thead> <tbody> <tr> <td>Answer Length</td> <td style="text-align: center">0.2381</td> </tr> <tr> <td>Markdown Header</td> <td style="text-align: center">0.0290</td> </tr> <tr> <td>Markdown List</td> <td style="text-align: center">0.0201</td> </tr> <tr> <td>Markdown Bold</td> <td style="text-align: center">0.0135</td> </tr> <tr> <td>Emoji Count</td> <td style="text-align: center">-0.0039</td> </tr> <tr> <td>Very Negative</td> <td style="text-align: center">-0.0034</td> </tr> <tr> <td>Negative</td> <td style="text-align: center">-0.0428</td> </tr> <tr> <td>Neutral</td> <td style="text-align: center">-0.0258</td> </tr> <tr> <td>Positive</td> <td style="text-align: center">0.0146</td> </tr> <tr> <td>Very Positive</td> <td style="text-align: center">0.0285</td> </tr> </tbody> </table> <h2> Ablation Tests </h2> <p>To disentangle sentiment effects from other style cues, we ran an ablation study removing formatting features and retaining only emoji count and sentiment.</p> <table> <thead> <tr> <th>Feature</th> <th style="text-align: center">Coefficient</th> </tr> </thead> <tbody> <tr> <td>Emoji Count</td> <td style="text-align: center">-0.0048</td> </tr> <tr> <td>Very Negative</td> <td style="text-align: center">0.0008</td> </tr> <tr> <td>Negative</td> <td style="text-align: center">-0.0516</td> </tr> <tr> <td>Neutral</td> <td style="text-align: center">-0.0463</td> </tr> <tr> <td>Positive</td> <td style="text-align: center">0.0262</td> </tr> <tr> <td>Very Positive</td> <td style="text-align: center">0.0419</td> </tr> </tbody> </table> <p>Key observations:</p> <ul> <li><strong>Positive sentiment maintains a strong positive effect</strong>, even without formatting.</li> <li><strong>Neutral and Negative tones are penalized</strong>, highlighting a general preference for emotional expressiveness.</li> </ul> <p><img src="/assets/img/blog/sentiment-control/overall_sentiment.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 80%"/></p> <p style="color:gray; text-align: center;">Figure 2. Overall ranking vs Sentiment Control ranking, where we only control for emoji count and sentiment </p> <h2> Further Analysis </h2> <p>To better understand how sentiment impacts head-to-head outcomes, we computed win rates conditioned on sentiment labels. Each entry below represents the probability that a model with a given tone (row) beats a model with another tone (column):</p> <table> <thead> <tr> <th style="text-align: center"> </th> <th style="text-align: center">Very Negative</th> <th style="text-align: center">Negative</th> <th style="text-align: center">Neutral</th> <th style="text-align: center">Positive</th> <th style="text-align: center">Very Positive</th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><strong>Very Negative</strong></td> <td style="text-align: center">———-</td> <td style="text-align: center">0.647845</td> <td style="text-align: center">0.539964</td> <td style="text-align: center">0.402597</td> <td style="text-align: center">0.430464</td> </tr> <tr> <td style="text-align: center"><strong>Negative</strong></td> <td style="text-align: center">0.352155</td> <td style="text-align: center">———-</td> <td style="text-align: center">0.360911</td> <td style="text-align: center">0.293156</td> <td style="text-align: center">0.217092</td> </tr> <tr> <td style="text-align: center"><strong>Neutral</strong></td> <td style="text-align: center">0.460036</td> <td style="text-align: center">0.639089</td> <td style="text-align: center">———-</td> <td style="text-align: center">0.414407</td> <td style="text-align: center">0.362715</td> </tr> <tr> <td style="text-align: center"><strong>Positive</strong></td> <td style="text-align: center">0.597403</td> <td style="text-align: center">0.706844</td> <td style="text-align: center">0.585593</td> <td style="text-align: center">———-</td> <td style="text-align: center">0.449768</td> </tr> <tr> <td style="text-align: center"><strong>Very Positive</strong></td> <td style="text-align: center">0.569536</td> <td style="text-align: center">0.782908</td> <td style="text-align: center">0.637285</td> <td style="text-align: center">0.550232</td> <td style="text-align: center">———-</td> </tr> </tbody> </table> <p>Several insights emerge:</p> <ul> <li> <p>Very Negative beats Negative (65%) and Neutral (54%), which might seem surprising at first. This likely reflects scenarios where users prompt the chatbot to behave maliciously or humorously at their own expense (e.g., “Roast me” or “Make fun of me”). In such cases, chatbots that lean into the negativity—rather than deflect—are actually rewarded by users.</p> </li> <li> <p>Neutral tone underperforms across the board, losing to every other tone except Negative. This supports the idea that emotional expression, whether positive or negative, tends to be preferred over dry or purely factual responses. Neutral responses may be perceived as disengaged or unhelpful, especially in creative or open-ended tasks.</p> </li> <li> <p>As expected, Positive and Very Positive dominate, with Very Positive winning 78% of the time against Negative and 64% against Neutral.</p> </li> </ul> <p>This suggests that sentiment affects not only absolute rankings but also pairwise preferences in nuanced and sometimes counterintuitive ways.</p> <h2> Limitations and Future Directions </h2> <p>While Sentiment Control is an important advancement, our analysis remains observational. Unobserved confounders may still exist, such as intrinsic correlations between sentiment positivity and answer quality. Future work includes exploring other emotional and psychological dimensions of style.</p> <p>We’re eager for community contributions and further collaboration!</p> <p>Please see the link to the colab notebook below. We will be adding sentiment control soon to all categories of the leaderboard. We look forward to seeing how the community leverages these new insights. Stay tuned for more updates!</p> <ul> <li><a href="https://colab.research.google.com/drive/17Pf9xW2agVJbm_KGOLEet5OxB98CcjTP?usp=sharing">Colab Link</a></li> </ul> <h2 id="reference">Reference</h2> <p>[1] Li et al. “Does Style Matter? Disentangling style and substance in Chatbot Arena”</p> <h2 id="citation">Citation</h2> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">sentimentarena2025</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">{Introducing Sentiment Control: Disentagling Sentiment and Substance}</span><span class="p">,</span>
    <span class="na">url</span> <span class="p">=</span> <span class="s">{https://blog.lmarena.ai/blog/2025/sentiment-control/}</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">{Connor Chen, Wei-Lin Chiang, Tianle Li, Anastasios N. Angelopoulos}</span><span class="p">,</span>
    <span class="na">month</span> <span class="p">=</span> <span class="s">{April}</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span>

<span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chiang2024chatbot</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Chatbot arena: An open platform for evaluating llms by human preference}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhu, Banghua and Zhang, Hao and Jordan, Michael and Gonzalez, Joseph E and others}</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Forty-first International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>Connor Chen</name></author><summary type="html"><![CDATA[Introducing Sentiment Control: Disentagling Sentiment and Substance]]></summary></entry><entry><title type="html">How Many User Prompts are New?</title><link href="https://lmarena.github.io/blog/2025/freshness/" rel="alternate" type="text/html" title="How Many User Prompts are New?"/><published>2025-04-18T00:00:00+00:00</published><updated>2025-04-18T00:00:00+00:00</updated><id>https://lmarena.github.io/blog/2025/freshness</id><content type="html" xml:base="https://lmarena.github.io/blog/2025/freshness/"><![CDATA[<h2 id="intro">Intro</h2> <p>One of the key reasons why Chatbot Arena is such an enticing benchmark is that it’s live: thousands of new user conversations and votes are collected every day. This constant stream of new data helps prevent benchmark “gaming” - training on the benchmark to get a high score. But how fresh is this data really?</p> <p>We investigate 355,575 LLM battles from May 2024 to Dec 2024 to answer the following questions:</p> <p>1. What proportion of prompts have never been seen before (aka “fresh”)?<br/> 2. What are common duplicate prompts?<br/> 3. How many prompts appear in widely used benchmarks?</p> <p>We find that:</p> <p>1. Roughly 75% of the prompts collected each day are significantly different from any prompt on a previous day.<br/> 2. Duplicate prompts are largely greetings (e.g., “hi” and “hello”), the same user submitting the same prompt on the same day to multiple models, or common tester prompts like “how many r’s are in strawberry?”<br/> 3. Less than 1% of user prompts appear in popular benchmarks</p> <h2 id="how-do-we-measure-prompt-duplicates">How do we measure prompt duplicates?</h2> <p>Prompt duplicates are measured by the cosine similarity of the text embeddings (OpenAI’s text-embedding-3-small). If the similarity between the embeddings of prompt a and prompt b are greater than or equal to 0.7, we consider it a duplicate. This threshold is set by manually looking through examples to determine when two prompts are asking the same thing. A random sample of prompt pairs with their similarities are provided on our <a href="https://huggingface.co/lmarena-ai">Hugging Face</a>.</p> <p>Given a prompt at submitted at time \(t\), we examine the following:</p> <ul> <li>The largest similarity between the prompt submitted at time \(t\) and any prompt submitted before time \(t\)</li> <li>The largest similarity between the prompt submitted at time \(t\) and any prompt submitted at least one day before time \(t\)</li> <li>The largest similarity between the prompt submitted at time \(t\) and any prompt submitted at least one week before time \(t\)</li> <li>The largest similarity between the prompt submitted at time \(t\) and any prompt from an existing dataset (contamination analysis)</li> </ul> <h2 id="how-many-duplicate-prompts-are-there">How many duplicate prompts are there?</h2> <p>For roughly 75% of the prompts collected each day, there is not a similar prompt submitted on a previous day. This indicates that roughly 75% of the prompts submitted each day are fresh.</p> <div class="plot-container"> <div class="plot-toggle"> <button onclick="togglePlotView('daily')" class="toggle-btn active" id="daily-btn">Daily View</button> <button onclick="togglePlotView('weekly')" class="toggle-btn" id="weekly-btn">Weekly View</button> </div> <div class="plot-wrapper"> <div class="plot-frame active" id="daily-plot"> <iframe src="/assets/img/blog/freshness/daily_matches.html" frameborder="0" scrolling="no" height="500px" width="100%"></iframe> <p style="color:gray; text-align: center;">Prompt Freshness per Day.</p> </div> <div class="plot-frame" id="weekly-plot"> <iframe src="/assets/img/blog/freshness/weekly_matches.html" frameborder="0" scrolling="no" height="500px" width="100%"></iframe> <p style="color:gray; text-align: center;">Prompt Freshness per Week.</p> </div> </div> </div> <style>.plot-container{margin:20px 0;width:100%}.plot-wrapper{position:relative;width:100%}.plot-frame{border:1px solid var(--global-divider-color,#dee2e6);border-radius:4px;overflow:hidden;margin-bottom:15px;position:absolute;top:0;left:0;width:100%;opacity:0;visibility:hidden;transition:opacity .3s ease}.plot-frame.active{opacity:1;visibility:visible;position:relative}.plot-frame iframe{display:block;width:100%}.plot-toggle{margin-bottom:15px;display:flex;gap:10px}.toggle-btn{padding:8px 12px;background-color:var(--global-bg-color,#f0f0f0);border:1px solid var(--global-divider-color,#ddd);border-radius:4px;cursor:pointer;transition:all .2s;color:var(--global-text-color,#000)}.toggle-btn:hover{background-color:var(--global-hover-color,#e0e0e0)}.toggle-btn.active{background-color:var(--global-theme-color,#4a6baf);color:white;border-color:var(--global-theme-color,#3a5a9f)}</style> <script>function togglePlotView(e){document.querySelectorAll(".plot-frame").forEach(function(e){e.classList.remove("active")}),document.querySelectorAll(".toggle-btn").forEach(function(e){e.classList.remove("active")}),document.getElementById(e+"-plot").classList.add("active"),document.getElementById(e+"-btn").classList.add("active")}</script> <p>If you look at the above analysis, the proportion of fresh prompts decreases as a function of \(t\). This is expected, since as \(t\) grows, we are comparing new prompts with an ever-larger set of past prompts. For example, when \(t=1\), there are no previos prompts, so of course, the proportion of unique prompts is \(1/1=100\)%.</p> <p>However, as \(t\) grows, this number stabilizes to around 70-80% fresh prompts at a similarity threshold of 0.7. This equilibrium represents the fraction of fresh prompts that we expect chatbot arena to generate in the long run.</p> <p>Interestingly, we also see certain dates where prompt freshness is significantly lower than neighboring dates: we will get to why that is in the next section.</p> <h2 id="what-are-the-sources-of-duplicates">What are the sources of duplicates?</h2> <p>We find that many of the duplicates can be attributed to 3 things: “tester”prompts, hi/hello’s, and prompts asked multiple times by the same user back to back.</p> <p><strong>Hi’s and Hello’s.</strong> We see that 2.1% of our data is some variation of “hi” in various languages. As per our deduplication policy, these are deduplicated when calculating the final rankings.</p> <p><strong>Tester prompts.</strong> There are certain prompts that users have found to stump most LLM’s, like “how many r’s are in strawberry” or “what is bigger, 9.11 or 9.8”. When a new model comes out, these prompts are commonly asked to gauge performance, which can be a source of days with a low proportion of fresh prompts. For instance, the week of August 8th saw a large decrease in prompt freshness, which coincides with a release of an update to GPT-4o. Looking at the top prompts on those days we see many of these prompts are a version of “how many r’s are in strawberry”.</p> <div> <iframe src="/assets/img/blog/freshness/strawberry_and_nn_match.html" frameborder="0" scrolling="no" height="500px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Strawberry and Nearest Neighbor Matches.</p> <div> <iframe src="/assets/img/blog/freshness/most_common_prompt_per_week.html" frameborder="0" scrolling="no" height="500px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Most Common Prompts by Week (excluding "hi" prompts).</p> <p><strong>Repeated prompts by the same user.</strong> Many duplicate prompts are submitted by the same person on the same day. Comparing prompts to every prompt seen at an previous timestep (rather than a previous day or week) 65% of prompts at a given time have been previously seen. However, most of these duplicates occur on the same day, with 60% of these prompts submitted by the same user. This indicates that users are asking a prompt, voting, then starting a new battle with two new models and asking the same prompt. This is encouraging because the models used in each conversation vary, which helps maintain diversity in prompts across different model pairs and results in more consistent voting from the same user. Removing duplicate prompts submitted by the same user on the same day raises the percentage of unique prompts from 65% to 80%.</p> <div> <iframe src="/assets/img/blog/freshness/days_before_nn_log.html" frameborder="0" scrolling="no" height="500px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Days before nearest neighbor is seen.</p> <h2 id="how-many-prompts-are-seen-in-existing-datasets">How many prompts are seen in existing datasets?</h2> <p>Lastly, we wanted to ensure that the prompts are not contained in commonly used benchmarks. Using the same similarity measure, we find a very low percentage of user prompts are seen in existing datasets, reducing the likelihood that models which overfit to these benchmarks are given an advantage in the arena.</p> <div> <iframe src="/assets/img/blog/freshness/contamination_summary.html" frameborder="0" scrolling="no" height="500px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Contamination of Prompts in Existing Datasets.</p> <h2 id="conclusion">Conclusion</h2> <p>The majority of our user prompts are fresh (i.e., 75%), and the data is not contaminated by existing benchmarks. A sample of the data with their nearest neighbors to the original prompt can be found in this space. We are excited to see how this data evolves over time!</p> <h2 id="citation">Citation</h2> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">dunlap2025freshness</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{How Many User Prompts are New?}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Lisa Dunlap and Elva Lu and Joseph E. Gonzalez and Anastasios N. Angelopoulos and Wei-Lin Chiang and Ion Stoica}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div> <h2 id="prompt-similarity-examples">Prompt Similarity Examples</h2> <p>To better understand how our similarity threshold works in practice, we’ve provided examples of prompt pairs at different similarity levels. Use the buttons below to explore prompt pairs within specific similarity ranges.</p> <div class="similarity-explorer"> <div class="similarity-buttons"> <button onclick="filterSimilarity('0.4-0.5')" class="sim-btn">0.4-0.5</button> <button onclick="filterSimilarity('0.5-0.6')" class="sim-btn">0.5-0.6</button> <button onclick="filterSimilarity('0.6-0.7')" class="sim-btn">0.6-0.7</button> <button onclick="filterSimilarity('0.7-0.8')" class="sim-btn">0.7-0.8</button> <button onclick="filterSimilarity('0.8-0.9')" class="sim-btn">0.8-0.9</button> <button onclick="filterSimilarity('0.9-1.0')" class="sim-btn">0.9-1.0</button> </div> <div id="similarity-table-container"> <table id="similarity-table" class="similarity-table"> <thead> <tr> <th>Prompt</th> <th>Nearest Neighbor</th> <th>Sim</th> </tr> </thead> <tbody id="similarity-table-body"> </tbody> </table> </div> </div> <style>.similarity-explorer{margin:20px 0;font-family:sans-serif}.similarity-buttons{margin-bottom:15px;display:flex;flex-wrap:wrap;gap:8px}.sim-btn{padding:8px 12px;background-color:var(--global-bg-color,#f0f0f0);border:1px solid var(--global-divider-color,#ddd);border-radius:4px;cursor:pointer;transition:all .2s;color:var(--global-text-color,#000)}.sim-btn:hover{background-color:var(--global-hover-color,#e0e0e0)}.sim-btn.active{background-color:var(--global-theme-color,#4a6baf);color:white;border-color:var(--global-theme-color,#3a5a9f)}.similarity-table{width:100%;table-layout:fixed;border-collapse:collapse;margin-top:10px;font-size:.95em;box-shadow:0 2px 8px rgba(0,0,0,0.1);border-radius:5px;overflow:hidden}.similarity-table th,.similarity-table td{padding:12px 15px;border:1px solid var(--global-divider-color,#ddd);text-align:left;overflow-wrap:break-word;word-wrap:break-word}.similarity-table th:nth-child(1),.similarity-table td:nth-child(1){width:45%}.similarity-table th:nth-child(2),.similarity-table td:nth-child(2){width:45%}.similarity-table th:nth-child(3),.similarity-table td:nth-child(3){width:10%;text-align:center}.similarity-table th{background-color:var(--global-theme-color,#4a6baf);color:white;font-weight:bold;text-transform:uppercase;letter-spacing:.5px;font-size:.9em;position:sticky;top:0}.similarity-table tr:nth-child(even){background-color:var(--global-code-bg-color,#f8f9fa)}.similarity-table tr:hover{background-color:var(--global-hover-color,#e9ecef)}#similarity-table-container{max-height:500px;overflow-y:auto;border-radius:5px;border:1px solid var(--global-divider-color,#ddd)}</style> <script>function filterSimilarity(t){document.querySelectorAll(".sim-btn").forEach(t=>{t.classList.remove("active")}),document.querySelector(`.sim-btn:nth-child(${Object.keys(similarityData).indexOf(t)+1})`).classList.add("active");const i=document.getElementById("similarity-table-body");i.innerHTML="",0!==similarityData[t].length?similarityData[t].forEach(t=>{const a=document.createElement("tr");a.innerHTML=`\n      <td>${t.Prompt}</td>\n      <td>${t["Nearest Neighbor"]}</td>\n      <td><strong>${parseFloat(t.Similarity).toFixed(2)}</strong></td>\n    `,i.appendChild(a)}):i.innerHTML='<tr><td colspan="3">No examples in this similarity range.</td></tr>'}let similarityData={"0.4-0.5":[],"0.5-0.6":[],"0.6-0.7":[],"0.7-0.8":[],"0.8-0.9":[],"0.9-1.0":[]};fetch("/assets/img/blog/freshness/blog_sample.json").then(t=>t.json()).then(t=>{t.forEach(t=>{const i=parseFloat(t.Similarity);i>=.4&&i<.5?similarityData["0.4-0.5"].push(t):i>=.5&&i<.6?similarityData["0.5-0.6"].push(t):i>=.6&&i<.7?similarityData["0.6-0.7"].push(t):i>=.7&&i<.8?similarityData["0.7-0.8"].push(t):i>=.8&&i<.9?similarityData["0.8-0.9"].push(t):i>=.9&&i<=1&&similarityData["0.9-1.0"].push(t)}),filterSimilarity("0.7-0.8")})["catch"](t=>{console.error("Error loading similarity data:",t),document.getElementById("similarity-table-body").innerHTML='<tr><td colspan="3">Error loading data. Please try again later.</td></tr>'});</script>]]></content><author><name>Lisa Dunlap</name></author><summary type="html"><![CDATA[Analysis of prompt freshness and benchmark contamination]]></summary></entry><entry><title type="html">LMArena is Growing to Support our Community Platform</title><link href="https://lmarena.github.io/blog/2025/new-beta/" rel="alternate" type="text/html" title="LMArena is Growing to Support our Community Platform"/><published>2025-04-17T00:00:00+00:00</published><updated>2025-04-17T00:00:00+00:00</updated><id>https://lmarena.github.io/blog/2025/new-beta</id><content type="html" xml:base="https://lmarena.github.io/blog/2025/new-beta/"><![CDATA[<p>LMArena started as a scrappy academic project from UC Berkeley: just a handful of PhD students and undergrads working day and night on a research prototype. Today, we have two announcements:</p> <ol> <li> <p style="font-weight: 400;">We are starting a company to support LMArena! <strong>LMArena will stay neutral, open, and accessible to everyone.</strong> We will focus on improving our open community platform for testing and evaluating LLMs.</p> </li> <li> <p style="font-weight: 400;">We’re releasing a beta version of a new site, <a href="https://beta.lmarena.ai/">beta.lmarena.ai</a>. We’re doing our best to lower the error rate, improve the UI layout, optimize mobile, etc. Thanks for all the feedback. Keep it coming!</p> </li> </ol> <h3 id="why-start-a-company">Why start a company?</h3> <p>We hope that a company will allow us to continue building this valuable community platform, and give us the resources to improve it significantly over what it is today. Some of us have recently graduated from Berkeley and want to keep working on LMArena, and we are hoping to build a team to provide even better service to our users and the AI community at large.</p> <p>LMArena will be staying true to its original mission. It will remain a neutral, open platform for testing and evaluating AI models. Our leaderboard will never be biased towards (or against) any provider, and will faithfully reflect our community’s preferences by design. It will be science-driven. Staying neutral and earning community trust will always be essential to the success of our business. But this isn’t just about being strategic; it’s our choice, and part of our personal motivations for starting this company. So we will never deviate from that north star.</p> <p>In fact, starting a company will accelerate our mission. As a first step, we’re finally addressing the feedback we’ve received and rebuilding our core platform, <a href="https://lmarena.ai">lmarena.ai</a>. This means: fixing all the bugs, speeding up the experience, and adding features, like login, chat history, and personal leaderboards. It also means supporting more open research, like <a href="https://arxiv.org/abs/2502.14855">prompt-to-leaderboard</a>, building more evaluations like <a href="https://web.lmarena.ai">WebDev Arena</a>, <a href="https://blog.lmarena.ai/blog/2025/repochat-arena/">RepoChat Arena</a>, and <a href="https://blog.lmarena.ai/blog/2025/search-arena/">Search Arena</a> and working to improve our security. Moving forward, we’re excited to grow in a way that supports the community that helped us build this platform.</p> <p>We haven’t fully worked out the business model, but we know AI companies want access to neutral and reliable evaluation services to speed up model development and improve real-world performance. This applies both to first-party model providers, and also to other companies for whom AI is a core part of their business. Our viewpoint is simple: organic, real-world human preferences will help drive progress towards more reliable AI, which will be valuable for everyone.</p> <p>We’ll continue to bring the best AI models to everyone, and improve them through transparent, community-led evaluations. Today, we’re excited to launch the new <a href="http://beta.lmarena.ai">LMArena Beta</a> as the next step forward. With your feedback, we’re making big changes to the platform to make AI more accessible and reliable for everyone.</p> <p><img src="/assets/img/blog/beta/beta_mock.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 85%"/></p> <h3 id="launching-today-the-new-lmarena-beta">Launching Today: The New LMArena Beta</h3> <p>Over a month ago, we launched the Alpha version of a new LMArena directly with you in our <a href="https://discord.gg/lmarena">Discord</a>. So many of you have jumped in, tested its limits, and sent us an incredible stream of feedback, suggestions, and bug reports. Thank you!!</p> <p>We want you to know: we’ve read it all — and you’ve made a real impact on LMArena. Many of the changes you see today are a direct result of what you told us, and we’ve been shipping improvements week over week thanks to your insights.</p> <p>Beta is a more stable experience shaped by community feedback. It tackles many of the themes we heard again and again: improved error rates, better layout clarity, optimized mobile support, more intuitive voting, and improvements to the conversation flow.</p> <center> <video width="480" controls="" style="width:100%; height:auto;"> <source src="/assets/img/blog/beta/beta_release.mp4" type="video/mp4"/> </video> </center> <h3 id="what-you-shared-with-us">What You Shared With Us</h3> <p>Through surveys, feedback forms, Discord threads, and spontaneous essays direct to our inbox, you’ve helped us understand what matters most:</p> <h4 id="speed-and-performance">Speed and performance</h4> <ul> <li> <p style="font-weight: 400;">The old site was often described as “janky,” “laggy,” or “bug-prone,” especially on mobile or during peak hours.</p> </li> </ul> <h4 id="clarity-and-design">Clarity and design</h4> <ul> <li> <p style="font-weight: 400;">You asked for a UI that’s modern and intuitive. A clearer model selection, better separation between responses, and easier ways to vote and navigate.</p> </li> </ul> <p>We’ve taken this to heart. The new LMArena isn’t just a fresh coat of paint. It’s a full rebuild, with your ideas guiding the direction. The goal is to make LMArena more accessible, more reliable, and ultimately more fun to use for everyone testing the world’s leading AI models.</p> <h3 id="try-the-beta">Try the Beta!</h3> <p>The Beta version is live and open to everyone, but this is still very much a work in progress. Beta means smoother, not perfect. That means:</p> <ul> <li> <p style="font-weight: 400;">Some features are still coming (like explorer, price plot, search, etc.)</p> </li> <li> <p style="font-weight: 400;">There are still a few bugs</p> </li> <li> <p style="font-weight: 400;">Optimization is ongoing</p> </li> </ul> <p>We’re not done, and that’s where you come in! This is your chance to shape what the final experience becomes. Whether you love it, hate it, or hit a bug, we want to hear from you. Your voice is what helps us get it right.</p> <ul> <li> <p style="font-weight: 400;">Try the Beta: <a href="https://beta.lmarena.ai/">beta.lmarena.ai</a></p> </li> <li> <p style="font-weight: 400;">Join the conversation: <a href="https://discord.gg/LMArena">discord.gg/LMArena</a></p> </li> <li> <p style="font-weight: 400;">Follow us on X: <a href="https://x.com/lmarena_ai">x.com/lmarena_ai</a></p> </li> </ul> <p>If you’re new to LMArena, join us! Stay tuned for more exciting updates, we’re just getting started! For media inquiries, contact: lmarena.ai@gmail.com</p>]]></content><author><name>LMArena Team</name></author><summary type="html"><![CDATA[LMArena started as a scrappy academic project from UC Berkeley: just a handful of PhD students and undergrads working day and night on a research prototype. Today, we have two announcements: We are starting a company to support LMArena! LMArena will stay neutral, open, and accessible to everyone. We will focus on improving our open community platform for testing and evaluating LLMs. We’re releasing a beta version of a new site, beta.lmarena.ai. We’re doing our best to lower the error rate, improve the UI layout, optimize mobile, etc. Thanks for all the feedback. Keep it coming!]]></summary></entry><entry><title type="html">Introducing the Search Arena: Evaluating Search-Enabled AI</title><link href="https://lmarena.github.io/blog/2025/search-arena/" rel="alternate" type="text/html" title="Introducing the Search Arena: Evaluating Search-Enabled AI"/><published>2025-04-14T00:00:00+00:00</published><updated>2025-04-14T00:00:00+00:00</updated><id>https://lmarena.github.io/blog/2025/search-arena</id><content type="html" xml:base="https://lmarena.github.io/blog/2025/search-arena/"><![CDATA[<h2 id="tldr">TL;DR</h2> <ol> <li>We introduce <strong>Search Arena</strong>, a crowdsourced in-the-wild evaluation platform for search-augmented LLM systems based on human preference. Unlike LM-Arena or SimpleQA, our data focuses on current events and diverse real-world use cases (see <a href="#why-search-arena">Sec. 1</a>).</li> <li>Based on 7k human votes (03/18–04/13), <strong>Gemini-2.5-Pro-Grounding</strong> and <strong>Perplexity-Sonar-Reasoning-Pro</strong> are at the top, followed by the rest of Perplexity’s Sonar models, Gemini-2.0-Flash-Grounding, and OpenAI’s web search API models. Standardizing citation styles had minimal effect on rankings (see <a href="#leaderboard">Sec. 2</a>).</li> <li>Three features show strong positive correlation with human preference: response length, citation count, and citing specific web sources like YouTube and online forum/blogs (see <a href="#analyses">Sec. 3</a>).</li> <li>We open-sourced our dataset (<a href="https://huggingface.co/datasets/lmarena-ai/search-arena-v1-7k">🤗 search-arena-7k</a>) and code (<a href="https://colab.research.google.com/drive/1h7rR7rhePBPuIfaWsVNlW87kv3DLibPS?usp=sharing">⚙️ Colab notebook</a>) for leaderboard analysis. Try <a href="https://lmarena.ai/?search">🌐 Search Arena</a> and see <a href="#futurework">Sec. 4</a> for what’s next.</li> </ol> <div id="fig1"> <iframe src="/assets/img/blog/search_arena/04142025/main_bootstrap_elo_rating.html" frameborder="0" scrolling="no" height="400px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 1. Search Arena leaderboard.</p> <h2 id="why-search-arena">1. Why Search Arena?</h2> <p>Web search is undergoing a major transformation. Search-augmented LLM systems integrate dynamic real-time web data with the reasoning, problem-solving, and question-answering capabilities of LLMs. These systems go beyond traditional retrieval, enabling richer human–web interaction. The rise of models like Perplexity’s Sonar series, OpenAI’s GPT-Search, and Google’s Gemini-Grounding highlights the growing impact of search-augmented LLM systems.</p> <p>But how should these systems be evaluated? Static benchmarks like SimpleQA focus on factual accuracy on challenging questions, but that’s only one piece. These systems are used for diverse tasks—coding, research, recommendations—so evaluations must also consider how they retrieve, process, and present information from the web. Understanding this requires studying how humans use and evaluate these systems in the wild.</p> <p>To this end, we developed search arena, aiming to (1) enable crowd-sourced evaluation of search-augmented LLMs and (2) release a diverse, in-the-wild dataset of user–system interactions.</p> <p>Since our <a href="https://x.com/lmarena_ai/status/1902036561119899983">initial launch</a> on March 18th, we’ve collected over 11k votes across 10+ models. We then filtered this data to construct 7k battles with user votes (<a href="https://huggingface.co/datasets/lmarena-ai/search-arena-v1-7k">🤗 search-arena-7k</a>) and calculated the leaderboard with this <a href="https://colab.research.google.com/drive/1h7rR7rhePBPuIfaWsVNlW87kv3DLibPS?usp=sharing">⚙️ Colab notebook</a>. Below, we provide details on the collected data and the supported models.</p> <h3>A. Data</h3> <p><b>Data Filtering and Citation Style Control.</b> Each model provider uses a unique inline citation style, which can potentially compromise model anonymity. However, citation formatting impacts how information is presented to and processed by the user, impacting their final votes. To balance these considerations, we introduced <em>“style randomization”</em>: responses are displayed either in a standardized format or in the original format (i.e., the citation style agreed upon with each model provider).</p> <details> <summary>Click to view standardized and original citation styles for each provider.</summary> <div style="margin-top: 1rem;"> <img src="/assets/img/blog/search_arena/04142025/gemini_formatting_example.png" alt="Google's Gemini citation formatting comparison" style="width: 100%; max-width: 1000px; border: 1px solid #ccc; border-radius: 8px;"/> </div> <p>(1) Google's Gemini Formatting: standardized (left), original (right)</p> <div style="margin-top: 1rem;"> <img src="/assets/img/blog/search_arena/04142025/ppl_formatting_example.png" alt="Perplexity's Sonar citation formatting comparison" style="width: 100%; max-width: 1000px; border: 1px solid #ccc; border-radius: 8px;"/> </div> <p>(2) Perplexity’s Formatting: standardized (left), original (right)</p> <div style="margin-top: 1rem;"> <img src="/assets/img/blog/search_arena/04142025/gpt_formatting_example.png" alt="OpenAI's GPT citation formatting comparison" style="width: 100%; max-width: 1000px; border: 1px solid #ccc; border-radius: 8px;"/> </div> <p>(3) OpenAI's Formatting: standardized (left), original (right)</p> </details> <p>This approach mitigates de-anonymization while allowing us to analyze how citation style impacts user votes (see the citation analyses subsection <a href="#citation_analyses">here</a>). After updating and standardizing citation styles in collaboration with providers, we filtered the dataset to include only battles with the updated styles, resulting in ~7,000 clean samples for leaderboard calculation and further analysis.</p> <p><b>Comparison to Existing Benchmarks.</b> To highlight what makes Search Arena unique, we compare our collected data to <a href="https://arxiv.org/abs/2403.04132">LM-Arena</a> and <a href="https://arxiv.org/abs/2411.04368">SimpleQA</a>. As shown in <a href="#fig2">Fig. 2</a>, Search Arena prompts focus more on current events, while LM-Arena emphasizes coding/writing, and SimpleQA targets narrow factual questions (e.g., dates, names, specific domains). <a href="#tab1">Tab. 1</a> shows that Search Arena features longer prompts, longer responses, more turns, and more languages compared to SimpleQA—closer to natural user interactions seen in LM-Arena.</p> <div id="fig2"> <iframe src="/assets/img/blog/search_arena/04142025/topical_distribution_plot.html" frameborder="0" scrolling="no" height="450px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 2. Top-5 topic distributions across Search Arena, LM Arena, and SimpleQA. We use <a href="https://blog.lmarena.ai/blog/2025/arena-explorer/">Arena Explorer (Tang et al., 2025)</a> to extract topic clusters from the three datasets.</p> <table id="tab1" style="margin: 0 auto; border-collapse: collapse; text-align: center;"> <thead> <tr> <th style="padding: 8px; border-bottom: 1px solid #ccc;"></th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Search Arena</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">LM Arena</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">SimpleQA</th> </tr> </thead> <tbody> <tr> <td style="padding: 8px;">Languages</td> <td style="padding: 8px;">10+ (EN, RU, CN, …)</td> <td style="padding: 8px;">10+ (EN, RU, CN, …)</td> <td style="padding: 8px;">English Only</td> </tr> <tr> <td style="padding: 8px;">Avg. Prompt Length (#words)</td> <td style="padding: 8px;">88.08</td> <td style="padding: 8px;">102.12</td> <td style="padding: 8px;">16.32</td> </tr> <tr> <td style="padding: 8px;">Avg. Response Length (#words)</td> <td style="padding: 8px;">344.10</td> <td style="padding: 8px;">290.87</td> <td style="padding: 8px;">2.24</td> </tr> <tr> <td style="padding: 8px;">Avg. #Conversation Turns</td> <td style="padding: 8px;">1.46</td> <td style="padding: 8px;">1.37</td> <td style="padding: 8px;">N/A</td> </tr> </tbody> </table> <p style="color:gray; text-align: center;"> Table 1. Prompt language distribution, average prompt length, average response length, and average number of turns in Search Arena, LM Arena, and SimpleQA datasets.</p> <h3>B. Models</h3> <p>Search Arena currently supports 11 models from three providers: Perplexity, Gemini, and OpenAI. Unless specified otherwise, we treat the same model with different citation styles (original vs. standardized) as a single model. <a href="#fig3">Fig. 3</a> shows the number of battles collected per model used in this iteration of the leaderboard.</p> <p>By default, we use each provider’s standard API settings. For Perplexity and OpenAI, this includes setting the <code class="language-plaintext highlighter-rouge">search_context_size</code> parameter to <code class="language-plaintext highlighter-rouge">medium</code>, which controls how much web content is retrieved and passed to the model. We also explore specific features by changing the default settings: (1) For OpenAI, we test their geolocation feature in one model variant by passing a country code extracted from the user’s IP address. (2) For Perplexity and OpenAI, we include variants with <code class="language-plaintext highlighter-rouge">search_context_size</code> set to <code class="language-plaintext highlighter-rouge">high</code>. Below is the list of models currently supported in Search Arena:</p> <table style="width:100%; border-collapse: collapse; text-align: left;"> <thead> <tr> <th style="border-bottom: 1px solid #ccc; padding: 8px;">Provider</th> <th style="border-bottom: 1px solid #ccc; padding: 8px;">Model</th> <th style="border-bottom: 1px solid #ccc; padding: 8px;">Base model</th> <th style="border-bottom: 1px solid #ccc; padding: 8px;">Details</th> </tr> </thead> <tbody> <tr> <td rowspan="5" style="padding: 8px;">Perplexity</td> <td style="padding: 8px;"><code>ppl-sonar</code></td> <td style="padding: 8px;"><a href="https://docs.perplexity.ai/guides/models/sonar" target="_blank">sonar</a></td> <td style="padding: 8px;">Default config</td> </tr> <tr> <td style="padding: 8px;"><code>ppl-sonar-pro</code></td> <td style="padding: 8px;"><a href="https://docs.perplexity.ai/guides/models/sonar-pro" target="_blank">sonar-pro</a></td> <td style="padding: 8px;">Default config</td> </tr> <tr> <td style="padding: 8px;"><code>ppl-sonar-pro-high</code></td> <td style="padding: 8px;"><a href="https://docs.perplexity.ai/guides/models/sonar-pro" target="_blank">sonar-pro</a></td> <td style="padding: 8px;"><code>search_context_size</code> set to <code>high</code></td> </tr> <tr> <td style="padding: 8px;"><code>ppl-sonar-reasoning</code></td> <td style="padding: 8px;"><a href="https://docs.perplexity.ai/guides/models/sonar-reasoning" target="_blank">sonar-reasoning</a></td> <td style="padding: 8px;">Default config</td> </tr> <tr> <td style="padding: 8px;"><code>ppl-sonar-reasoning-pro-high</code></td> <td style="padding: 8px;"><a href="https://docs.perplexity.ai/guides/models/sonar-reasoning-pro" target="_blank">sonar-reasoning-pro</a></td> <td style="padding: 8px;"><code>search_context_size</code> set to <code>high</code></td> </tr> <tr> <td rowspan="2" style="padding: 8px;">Gemini</td> <td style="padding: 8px;"><code>gemini-2.0-flash-grounding</code></td> <td style="padding: 8px;"><a href="https://ai.google.dev/gemini-api/docs/models#gemini-2.0-flash" target="_blank">gemini-2.0-flash</a></td> <td style="padding: 8px;">With <code>Google Search</code> tool enabled</td> </tr> <tr> <td style="padding: 8px;"><code>gemini-2.5-pro-grounding</code></td> <td style="padding: 8px;"><a href="https://ai.google.dev/gemini-api/docs/models#gemini-2.5-pro-preview-03-25" target="_blank">gemini-2.5-pro-exp-03-25</a></td> <td style="padding: 8px;">With <code>Google Search</code> tool enabled</td> </tr> <tr> <td rowspan="4" style="padding: 8px;">OpenAI<sup>†</sup></td> <td style="padding: 8px;"><code>api-gpt-4o-mini-search-preview</code></td> <td style="padding: 8px;"><a href="https://platform.openai.com/docs/models/gpt-4o-mini-search-preview" target="_blank">gpt-4o-mini-search-preview</a></td> <td style="padding: 8px;">Default config</td> </tr> <tr> <td style="padding: 8px;"><code>api-gpt-4o-search-preview</code></td> <td style="padding: 8px;"><a href="https://platform.openai.com/docs/models/gpt-4o-search-preview" target="_blank">gpt-4o-search-preview</a></td> <td style="padding: 8px;">Default config</td> </tr> <tr> <td style="padding: 8px;"><code>api-gpt-4o-search-preview-high</code></td> <td style="padding: 8px;"><a href="https://platform.openai.com/docs/models/gpt-4o-search-preview" target="_blank">gpt-4o-search-preview</a></td> <td style="padding: 8px;"><code>search_context_size</code> set to <code>high</code></td> </tr> <tr> <td style="padding: 8px;"><code>api-gpt-4o-search-preview-high-loc</code></td> <td style="padding: 8px;"><a href="https://platform.openai.com/docs/models/gpt-4o-search-preview" target="_blank">gpt-4o-search-preview</a></td> <td style="padding: 8px;"><code>user_location</code> feature enabled</td> </tr> </tbody> </table> <p style="color:gray; font-size: 14px; text-align: center; margin-top: 8px;"> Table 2. Models currently supported in Search Arena. </p> <p style="color:gray; font-size: 13px; max-width: 700px; margin: 0 auto;"> <sup>†</sup>We evaluate OpenAI’s <a href="https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat" target="_blank">web search API</a>, which is different from the search feature in the ChatGPT product. </p> <div id="fig3"> <iframe src="/assets/img/blog/search_arena/04142025/main_battle_count.html" frameborder="0" scrolling="no" height="400px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 3. Battle counts across 11 models. The distribution is not even as (1) we released models into the arena in batches and (2) filtered votes (described above).</p> <h2 id="leaderboard">2. Leaderboard</h2> <p>We begin by analyzing pairwise win rates—i.e., the proportion of wins of model A over model B in head-to-head battles. This provides a direct view of model performance differences without aggregating scores. The results are shown in <a href="#fig4">Fig. 4</a>, along with the following observations:</p> <ul> <li><code class="language-plaintext highlighter-rouge">gemini-2.5-pro-grounding</code> and <code class="language-plaintext highlighter-rouge">ppl-sonar-reasoning-pro-high</code> outperform all other models by a large margin. In direct head-to-head battles <code class="language-plaintext highlighter-rouge">ppl-sonar-reasoning-pro-high</code> has a slight advantage (53% win rate).</li> <li><code>ppl-sonar-reasoning</code> outperforms the rest of Perplexity’s models. There’s no clear difference between <code>ppl-sonar-pro</code> and <code>ppl-sonar-pro-high</code> (52%/48% win rate), and even <code>ppl-sonar</code> beats <code>ppl-sonar-pro-high</code> (60% win rate). This suggests that increasing search context does not necessarily improve performance and may even degrade it.</li> <li>Within OpenAI’s models, larger search context does not significantly improve performance (<code class="language-plaintext highlighter-rouge">api-gpt-4o-search</code> vs <code class="language-plaintext highlighter-rouge">api-gpt-4o-search-high</code>). While adding user location improves performance in head-to-head battles (58% win rate of <code class="language-plaintext highlighter-rouge">api-gpt-4o-search-high-loc</code> over <code class="language-plaintext highlighter-rouge">api-gpt-4o-search-high</code>), location-enabled version ranks lower in the leaderboard.</li> </ul> <div id="fig4"> <iframe src="/assets/img/blog/search_arena/04142025/main_pairwise_average_win_rate.html" frameborder="0" scrolling="no" height="500px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 4. Pairwise win rates (Model A wins Model B), excluding <code>tie</code> and <code>tie (bothbad)</code> votes.</p> <p>Now we build the leaderboard! Consistent with <a href="https://lmarena.ai/">LM Arena</a>, we apply the Bradley-Terry (BT) model to compute model scores. The resulting BT coefficients are then translated to Elo scale, with the final model scores and rankings displayed in <a href="#fig1">Fig. 1</a> and <a href="#tab3">Tab. 3</a>. The confidence intervals are still wide, which means the leaderboard hasn’t fully settled and there’s still some uncertainty. But clear performance trends are already starting to emerge. Consistent with the pairwise win rate analysis in the previous section, <code class="language-plaintext highlighter-rouge">gemini-2.5-pro-grounding</code> and <code class="language-plaintext highlighter-rouge">ppl-sonar-reasoning-pro-high</code> top the leaderboard by a substantial margin. They are followed by models from the <code class="language-plaintext highlighter-rouge">ppl-sonar</code> family, with <code class="language-plaintext highlighter-rouge">ppl-sonar-reasoning</code> leading the group. Then comes <code class="language-plaintext highlighter-rouge">gemini-2.0-flash-grounding</code>, and finally OpenAI models with <code class="language-plaintext highlighter-rouge">api-gpt-4o-search</code> based models outperforming <code class="language-plaintext highlighter-rouge">api-gpt-4o-mini-search</code>. Generally, users prefer responses from reasoning models (top 3 on the leaderboard).</p> <table id="tab2" style="width: 100%; border-collapse: collapse; text-align: center;"> <thead> <tr> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Rank</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Model</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Arena Score</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">95% CI</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Votes</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Organization</th> </tr> </thead> <tbody> <tr><td>1</td><td><code>gemini-2.5-pro-grounding</code></td><td>1142</td><td>+14/-17</td><td>1,215</td><td>Google</td></tr> <tr><td>1</td><td><code>ppl-sonar-reasoning-pro-high</code></td><td>1136</td><td>+21/-19</td><td>861</td><td>Perplexity</td></tr> <tr><td>3</td><td><code>ppl-sonar-reasoning</code></td><td>1097</td><td>+11/-17</td><td>1,644</td><td>Perplexity</td></tr> <tr><td>3</td><td><code>ppl-sonar</code></td><td>1072</td><td>+15/-17</td><td>1,208</td><td>Perplexity</td></tr> <tr><td>3</td><td><code>ppl-sonar-pro-high</code></td><td>1071</td><td>+15/-10</td><td>1,364</td><td>Perplexity</td></tr> <tr><td>4</td><td><code>ppl-sonar-pro</code></td><td>1066</td><td>+12/-13</td><td>1,214</td><td>Perplexity</td></tr> <tr><td>7</td><td><code>gemini-2.0-flash-grounding</code></td><td>1028</td><td>+16/-16</td><td>1,193</td><td>Google</td></tr> <tr><td>7</td><td><code>api-gpt-4o-search</code></td><td>1000</td><td>+13/-19</td><td>1,196</td><td>OpenAI</td></tr> <tr><td>7</td><td><code>api-gpt-4o-search-high</code></td><td>999</td><td>+13/-14</td><td>1,707</td><td>OpenAI</td></tr> <tr><td>8</td><td><code>api-gpt-4o-search-high-loc</code></td><td>994</td><td>+14/-14</td><td>1,226</td><td>OpenAI</td></tr> <tr><td>11</td><td><code>api-gpt-4o-mini-search</code></td><td>961</td><td>+16/-15</td><td>1,172</td><td>OpenAI</td></tr> </tbody> </table> <p style="color:gray; text-align: center;"> Table 3. Search Arena leaderboard. </p> <h3 id="citation_analyses">Citation Style Analysis</h3> <p>Having calculated the main leaderboard, we can now analyze the effect of citation style on user votes and model rankings. For each battle, we record model A’s and B’s citation style — original (agreed upon with the providers) vs standardized.</p> <p>First, following the method in <a href="https://blog.lmarena.ai/blog/2024/style-control/">(Li et al., 2024)</a>, we apply style control and use the citation style indicator variable (1 if standardized, 0 otherwise) as an additional feature in the BT model. The resulting model scores and rankings do not change significantly from <a href="#fig1">the main leaderboard</a>. Although the leaderboard does not change, the corresponding coefficient is positive (0.044) and statistically significant (p&lt;0.05), implying that standardization of citation style has a positive impact on model score.</p> <p>We further investigate the effect of citation style on model performance, by treating each combination of model and citation style as a distinct model (e.g., <code class="language-plaintext highlighter-rouge">api-gpt-4o-search</code> with original style will be different from <code class="language-plaintext highlighter-rouge">api-gpt-4o-search</code> with standardized citation style). <a href="#fig5">Fig. 5</a> shows the change in the arena score between the two styles of each model. Overall, we observe increase or no change in score with standardized citations across all models except <code class="language-plaintext highlighter-rouge">gemini-2.0-flash</code>. However, the differences remain within the confidence intervals (CI), and we will continue collecting data to assess whether the trend converges toward statistical significance.</p> <div id="fig5"> <iframe src="/assets/img/blog/search_arena/04142025/og_vs_st.html" frameborder="0" scrolling="no" height="400px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 5. Change in arena score for original vs standardized citation style for each model.</p> <h2 id="analyses">3. Three Secrets Behind a WIN</h2> <p>After reviewing the leaderboard—and showing that the citation style doesn’t impact results all that much—you might be wondering: <em>What features contribute to the model’s win rate?</em></p> <p>To answer this, we used the framework in <a href="https://arxiv.org/abs/2201.12323">(Zhong et al., 2022)</a>, a method that automatically proposes and tests hypotheses to identify key differences between two groups of natural language texts—in this case, human-preferred and rejected model outputs. In our implementation, we asked the model to generate 25 hypotheses and evaluate them, leading to the discovery of <em>three distinguishing factors</em> with statistically significant p-values, shown in <a href="#tab4">Tab. 4</a>.</p> <table id="tab4" style="width: 100%; border-collapse: collapse; text-align: left;"> <thead> <tr> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Feature</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">p-value</th> </tr> </thead> <tbody> <tr> <td style="padding: 8px;">References to specific known entities or platforms</td> <td style="padding: 8px;">0.0000114</td> </tr> <tr> <td style="padding: 8px;">Frequent use of external citations and hyperlinks</td> <td style="padding: 8px;">0.01036</td> </tr> <tr> <td style="padding: 8px;">Longer, more in-depth answers</td> <td style="padding: 8px;">0.04761</td> </tr> </tbody> </table> <p style="color:gray; text-align: center;"> Table 4. Candidate key factors between the winning and losing model outputs. </p> <h3 id="model-characteristics">Model Characteristics</h3> <p>Guided by the above findings, we analyze how these features vary across models and model families.</p> <p><a href="#fig6">Fig. 6 (left)</a> shows the distribution of average response length across models. Gemini models are generally the most verbose—<code>gemini-2.5-pro-grounding</code>, in particular, produces responses nearly twice as long as most Perplexity or OpenAI models. Within the Perplexity and OpenAI families, response length is relatively consistent, with the exception of <code class="language-plaintext highlighter-rouge">ppl-sonar-reasoning-pro-high</code>. <a href="#fig6">Fig. 6 (right)</a> shows the average number of citations per response. Sonar models cite the most, with <code>ppl-sonar-pro-high</code> citing 2-3x more than Gemini models. OpenAI models cite the fewest sources (2-2.5) with little variation within the group.</p> <div id="fig6"> <iframe src="/assets/img/blog/search_arena/04142025/lenght_cit_features.html" frameborder="0" scrolling="no" height="400px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 6. Average response length (left) and number of citations (right) per model.</p> <p>In addition to number of citations and response length, we also study the common <em>source domains</em> cited by each model. We categorize retrieved URLs into ten types: YouTube, News (U.S. and foreign), Community &amp; Blogs (e.g., Reddit, Medium), Wikipedia, Tech &amp; Coding (e.g., Stack Overflow, GitHub), Government &amp; Education, Social Media, Maps, and Academic Journals. <a href="#fig7">Fig. 7</a> shows the domain distribution across providers in two settings: (1) all conversations, and (2) a filtered subset focused on Trump-related prompts. The case study helps examine how models behave when responding to queries on current events. Here are three interesting findings:</p> <ol> <li>All models favor authoritative sources (e.g., Wikipedia, <code>.edu</code>, <code>.gov</code> domains).</li> <li>OpenAI models heavily cite news sources—51.3% overall and 87.3% for Trump-related prompts.</li> <li>Gemini prefers community/blog content, whereas Perplexity frequently cites YouTube. Perplexity also strongly favors U.S. news sources over foreign ones (3x more often).</li> </ol> <div id="fig7"> <iframe src="/assets/img/blog/search_arena/04142025/domain_citations.html" frameborder="0" scrolling="no" height="400px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 7. Distribution of cited domain categories across models. Use the dropdown to switch between all prompts and a filtered Trump-related subset.</p> <h3 id="control-experiments">Control Experiments</h3> <p>After analyzing model characteristics such as response length, citation count, and citation sources, we revisited the Bradley-Terry model with these features as additional control variables <a href="https://blog.lmarena.ai/blog/2024/style-control/">(Li et al., 2024)</a>. Below are some findings when controlling for different subsets of control features:</p> <ul> <li><strong>Response length</strong>: Controlling for response length yields a positive and statistically significant coefficient (0.255, <em>p</em> &lt; 0.05), indicating that users prefer more verbose responses.</li> <li><strong>Number of citations</strong>: Controlling for citation count also results in a positive and significant coefficient (0.234, <em>p</em> &lt; 0.05), suggesting a preference for responses with more cited sources.</li> <li><strong>Citation source categories</strong>: As shown in <a href="#fig8">Fig. 8</a>, citations from <strong>community platforms</strong> (e.g., Reddit, Quora) and <strong>YouTube</strong> have statistically significant positive effects on user votes. The remaining categories have insignificant coefficients.</li> <li><strong>Joint controls</strong>: When controlling for all features, only <strong>response length</strong> and <strong>citation count</strong> remain statistically significant.</li> </ul> <div id="fig8" style="display: flex; justify-content: center;"> <iframe src="/assets/img/blog/search_arena/04142025/domain_citations_style_control_bootstrap_style_coefs.html" frameborder="0" scrolling="no" height="400px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 8. Estimates (with 95% CIs) of style coefficients.</p> <p>Finally, we used all previously described features to construct a controlled leaderboard. <a href="#fig9">Fig. 9</a> compares the original and adjusted arena scores after controlling for response length, citation count, and cited sources. Interestingly, when using all these features as control variables, the top six models all show a reduction in score, while the remaining models are largely unaffected. This narrows the gap between <code>gemini-2.0-flash-grounding</code> and non-reasoning Perplexity models. <a href="#tab5">Tab. 5</a> shows model rankings when controlling for different subsets of these features:</p> <ul> <li>Controlling for response length, <code class="language-plaintext highlighter-rouge">ppl-sonar-reasoning</code> shares the first rank with <code class="language-plaintext highlighter-rouge">gemini-2.5-pro-grounding</code> and <code class="language-plaintext highlighter-rouge">ppl-sonar-reasoning-pro-high</code>. The difference between (1) <code class="language-plaintext highlighter-rouge">sonar-pro</code> and other non-reasoning sonar models as well (2) <code class="language-plaintext highlighter-rouge">api-gpt-4o-search-high</code> and <code class="language-plaintext highlighter-rouge">api-gpt-4o-search-high-loc</code>, disappear.</li> <li>When controlling for the number of citations, model rankings converge (i.e., multiple models share the same rank), suggesting that the number of citations is a significant factor impacting differences across models and the resulting rankings.</li> <li>Controlling for cited domains has minimal effect on model rankings.</li> </ul> <div id="fig9"> <iframe src="/assets/img/blog/search_arena/04142025/style_control.html" frameborder="0" scrolling="no" height="400px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 9. Arena scores before and after a controlled setting.</p> <table id="tab5" style="width: 100%; border-collapse: collapse; text-align: center;"> <thead> <tr> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Model</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Rank Diff (Length)</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Rank Diff (# Citations)</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Rank Diff (Domain Sources)</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Rank Diff (All)</th> </tr> </thead> <tbody> <tr><td><code>gemini-2.5-pro-grounding</code></td><td>1→1</td><td>1→1</td><td>1→1</td><td>1→1</td></tr> <tr><td><code>ppl-sonar-reasoning-pro-high</code></td><td>1→1</td><td>1→1</td><td>1→1</td><td>1→1</td></tr> <tr><td><code>ppl-sonar-reasoning</code></td><td>3→1</td><td>3→3</td><td>3→3</td><td>3→2</td></tr> <tr><td><code>ppl-sonar</code></td><td>3→3</td><td>3→3</td><td>3→3</td><td>3→3</td></tr> <tr><td><code>ppl-sonar-pro-high</code></td><td>3→3</td><td>3→4</td><td>3→4</td><td>3→3</td></tr> <tr><td><code>ppl-sonar-pro</code></td><td>4→3</td><td>4→4</td><td>4→4</td><td>4→3</td></tr> <tr><td><code>gemini-2.0-flash-grounding</code></td><td>7→7</td><td>7→4</td><td>7→5</td><td>7→4</td></tr> <tr><td><code>api-gpt-4o-search</code></td><td>7→7</td><td>7→4</td><td>7→7</td><td>7→6</td></tr> <tr><td><code>api-gpt-4o-search-high</code></td><td>7→8</td><td>7→4</td><td>7→7</td><td>7→7</td></tr> <tr><td><code>api-gpt-4o-search-high-loc</code></td><td>8→8</td><td>8→5</td><td>8→7</td><td>8→7</td></tr> <tr><td><code>api-gpt-4o-mini-search</code></td><td>11→11</td><td>11→11</td><td>11→11</td><td>11→11</td></tr> </tbody> </table> <p style="color:gray; text-align: center;"> Table 5. Model rankings change when controlling for different subsets of features. </p> <h2 id="futurework">4. Conclusion &amp; What’s Next</h2> <p>As search-augmented LLMs become increasingly popular, <strong>Search Arena</strong> provides a real-time, in-the-wild evaluation platform driven by crowdsourced human feedback. Unlike static QA benchmarks, our dataset emphasizes current events and diverse real-world queries, offering a more realistic view of how users interact with these systems. Using 7k human votes, we found that <strong>Gemini-2.5-Pro-Grounding</strong> and <strong>Perplexity-Sonar-Reasoning-Pro-High</strong> share the first rank in the leaderboard. User preferences are positively correlated with <strong>response length</strong>, <strong>number of citations</strong>, and <strong>citation sources</strong>. Citation formatting, surprisingly, had minimal impact.</p> <p>We have open-sourced our data (<a href="https://huggingface.co/datasets/lmarena-ai/search-arena-v1-7k">🤗 search-arena-7k</a>) and analysis code (<a href="https://colab.research.google.com/drive/1h7rR7rhePBPuIfaWsVNlW87kv3DLibPS?usp=sharing">⚙️ Colab notebook</a>). Try <a href="https://lmarena.ai/?search">🌐 Search Arena</a> now and see what’s next:</p> <ul> <li><strong>Open participation</strong>: We are inviting model submissions from researchers and industry, and encouraging public voting.</li> <li><strong>Cross-task evaluation</strong>: How well do search models handle general questions? Can LLMs manage search-intensive tasks?</li> <li><strong>Raise the bar for open models:</strong> Can simple wrappers with search engine/scraping + tools like ReAct and function calling make open models competitive?</li> </ul> <h2 id="citation">Citation</h2> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">searcharena2025</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">{Introducing the Search Arena: Evaluating Search-Enabled AI}</span><span class="p">,</span>
    <span class="na">url</span> <span class="p">=</span> <span class="s">{https://blog.lmarena.ai/blog/2025/search-arena/}</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">{Mihran Miroyan*, Tsung-Han Wu*, Logan Kenneth King, Tianle Li, Anastasios N. Angelopoulos, Wei-Lin Chiang, Narges Norouzi, Joseph E. Gonzalez}</span><span class="p">,</span>
    <span class="na">month</span> <span class="p">=</span> <span class="s">{April}</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span>

<span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chiang2024chatbot</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Chatbot arena: An open platform for evaluating llms by human preference}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhu, Banghua and Zhang, Hao and Jordan, Michael and Gonzalez, Joseph E and others}</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Forty-first International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>Mihran Miroyan*</name></author><summary type="html"><![CDATA[TL;DR]]></summary></entry><entry><title type="html">LMArena Community Updates: Looking Ahead</title><link href="https://lmarena.github.io/blog/2025/new-discord-server/" rel="alternate" type="text/html" title="LMArena Community Updates: Looking Ahead"/><published>2025-03-18T00:00:00+00:00</published><updated>2025-03-18T00:00:00+00:00</updated><id>https://lmarena.github.io/blog/2025/new-discord-server</id><content type="html" xml:base="https://lmarena.github.io/blog/2025/new-discord-server/"><![CDATA[<p>Over the past two years, with the incredible support of our community, LMArena has grown into a vibrant ecosystem and platform with just a team of contributors, advisors and partnerships. Today, we’re excited to begin sharing community updates in our blog as we continue to make progress towards long-term growth.</p> <h2 id="a-platform-for-the-future">A Platform for the Future</h2> <p>First, let’s take a trip down memory lane <a href="https://x.com/lmarena_ai/status/1837233036624286126">when we graduated from LMSys</a> in September 2024 to establish LMArena as an independent platform. With the continued strong partnership with LMSys, we broadened our scope to evaluate not just chatbots but also frontier AI models in areas like <a href="https://x.com/lmarena_ai/status/1899181467252711593">WebDev Arena</a>, complex tasks, and red-teaming. This expansion allowed us to deepen our insights and provide even more value to the AI community.</p> <p>LMSys has always been more than just a single initiative—it has served as a research collective behind projects like Vicuna, Chatbot Arena, SGLang, S-LoRA, RouteLLM, and more. As LMArena takes on a life of its own, LMSys will continue to foster new projects and drive open research. If you’re interested in contributing to the future of AI, there’s never been a better time to get involved.</p> <h2 id="our-new-discord-server">Our New Discord Server</h2> <p>With this exciting evolution, we recently also launched a dedicated Discord community server earlier this month! Join us at: <a href="https://discord.gg/LMArena">discord.gg/LMArena</a>! This will be the new hub for all things LMArena—where we’ll share insights on ongoing projects, leaderboard updates, and even host community-driven events and surprises.</p> <p>In fact, we recently held our very first community event in the new Discord server, where over 20 members joined us for a walkthrough of the new desktop UI design in alpha. We also dove into community feedback on the voting UI—as your voices are shaping the future of the platform, and we can’t wait to continue these conversations with you. It was an exciting opportunity to connect directly with the community, discuss upcoming improvements, and get live input on the changes. We’re looking forward to hosting many more events like this so <a href="https://forms.gle/mgSodQEa28r8J5cz7">give us your feedback on what you’d like to see</a>.</p> <p><img src="/assets/img/blog/new-discord/discord-event.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 85%"/></p> <h2 id="our-vision-for-the-community">Our Vision for the Community</h2> <p>At its core, LMArena is about building together—whether you’re an AI Enthusiast, developer, or AI company, we want to create something that benefits everyone. Free access to models is incredibly valuable for those exploring AI, and the feedback collected through the Arena has proven to be an essential resource in improving AI systems. As part of our commitment to transparency, we’re dedicated to responsibly releasing portions of this data while maintaining the integrity of evaluations. This is to support our vision of advancing open AI evaluation and accessibility.</p> <h2 id="our-mission-and-next-chapter">Our Mission and Next Chapter</h2> <p>Looking ahead, we see many opportunities to expand our tools, provide deeper insights, and explore new ways to engage with AI models. Our mission is to make AI reliable and to provide sustainable services for everyone. This guiding principle shapes our efforts as we continue to push the boundaries of AI research and accessibility.</p> <p>This is just the beginning, and we couldn’t have done it without you—our amazing community. Thank you for being part of this journey. We can’t wait to see what we’ll build together next. Arena is currently built and run with a team of contributors and advisors, but we’re actively looking to grow and explore paths that help us deliver on this big mission very soon.</p> <p>Come join us in our new Discord (<a href="https://discord.gg/LMArena">discord.gg/LMArena</a>), and let’s shape the future of AI, together!</p>]]></content><author><name>LMArena Team</name></author><summary type="html"><![CDATA[Over the past two years, with the incredible support of our community, LMArena has grown into a vibrant ecosystem and platform with just a team of contributors, advisors and partnerships. Today, we’re excited to begin sharing community updates in our blog as we continue to make progress towards long-term growth.]]></summary></entry><entry><title type="html">WebDev Arena</title><link href="https://lmarena.github.io/blog/2025/webdev-arena/" rel="alternate" type="text/html" title="WebDev Arena"/><published>2025-03-10T00:00:00+00:00</published><updated>2025-03-10T00:00:00+00:00</updated><id>https://lmarena.github.io/blog/2025/webdev-arena</id><content type="html" xml:base="https://lmarena.github.io/blog/2025/webdev-arena/"><![CDATA[<h2 id="webdev-arena-a-live-llm-leaderboard-for-web-app-development">WebDev Arena: A Live LLM Leaderboard for Web App Development</h2> <p>LLMs have shown impressive coding abilities in various coding benchmarks, but how well can they actually build functional web applications? We introduce <a href="http://webdev.lmarena.ai">WebDev Arena</a>: a real-time AI coding competition where models go head-to-head in web development challenges.</p> <p>Inspired by <a href="http://lmarena.ai">Chatbot Arena</a>, WebDev Arena allows users to test LLMs in a real-world coding task: building interactive web applications. Users submit a prompt, two LLMs generate web apps, and the community votes on which model performs better.</p> <p>Try out our <a href="https://colab.research.google.com/drive/1xq_-PGp8gJ8aHUyokrSqL_qZnn8s-UHV?usp=sharing">colab</a> to explore the data and analysis behind WebDev Arena.</p> <h3 id="why-webdev-arena">Why WebDev Arena?</h3> <p>Traditional coding benchmarks like HumanEval focus on solving isolated function-based problems, but real-world coding demands more—including UI generation, handling package dependencies, and structuring complete applications.</p> <p>From an end-user’s perspective, the setup is simple:</p> <ol> <li>Users submit a prompt (e.g., “Build a simple chess game”).</li> <li>Two LLMs generate competing web apps.</li> <li>Users interact with both apps and vote on the better one.</li> <li>Results contribute to a live leaderboard ranking LLMs by performance.</li> </ol> <p>Examples<br/> “Create a Hacker News clone”<br/> <img src="/assets/img/blog/webdev_arena/hn-clone.png" alt="Hacker News Clone Example" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 90%"/></p> <p style="color:gray; text-align: center;">Figure 1. Example of a Hacker News clone built by an LLM.</p> <p>“Build a basic chess game” <img src="/assets/img/blog/webdev_arena/game-of-chess.png" alt="Chess Game Example" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 90%"/></p> <p style="color:gray; text-align: center;">Figure 2. Example of a chess game built by an LLM.</p> <h3 id="webdev-arena-leaderboard">WebDev Arena Leaderboard</h3> <p>Since <a href="https://x.com/aryanvichare10/status/1866561638712881172">our launch</a> in Dec 2024, we have collected over 80,000 community votes and provided a live ranking of LLMs based on their real-world coding performance.</p> <p>Similar to <a href="http://lmarena.ai/leaderboard">Chatbot Arena</a>, we use the Bradley-Terry (BT) model, which estimates model strengths from pairwise comparisons. In simple terms, the BT model uses win/loss data from pairwise matchups and calculates a score for each model, reflecting its likelihood of winning against others. This ranking system is similar to Elo, commonly used in chess, and competitive games, despite the difference being a player’s skill rating may evolve over time, while LLMs stay static.</p> <p>Key Takeaways</p> <ul> <li>Claude 3.7 Sonnet currently holds the #1 spot with a 76% average win rate</li> <li>Claude 3.5 Sonnet ranks #2</li> <li>Deepseek R1 ranks #3</li> </ul> <p><img src="/assets/img/blog/webdev_arena/030625-leaderboard.png" alt="WebDev Arena Leaderboard" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 90%"/></p> <p style="color:gray; text-align: center;">Figure 3. Current WebDev Arena leaderboard showing model rankings.</p> <h3 id="how-do-people-use-webdev-arena">How do people use WebDev Arena?</h3> <h4 id="topic-modeling">Topic Modeling</h4> <p>To analyze how people use Webdev Arena, we run the <a href="https://colab.research.google.com/drive/1chzqjePYnpq08fA3KzyKvSkuzCjojyiE?usp=sharing">topic modeling pipeline</a> on the conversation and human preference data. Results show a diverse range of prompt categories. Overall, prompts can be categorized into 11 broad categories, the largest three being Website Design (15.3%), Game Development (12.1%), and Clone Development (11.6%).</p> <div> <iframe src="/assets/img/blog/webdev_arena/cat_broad.html" frameborder="0" scrolling="no" height="500px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 4. Distribution of broad categories in WebDev Arena prompts.</p> <p>Each board category contains several fine-grained narrow categories, such as Landing Page Design, Dashboard and Web Application Design, 3D Graphics and Shaders, Language Learning App Design, etc. Click through the Explorer to see more categories and example prompts.</p> <div> <iframe src="/assets/img/blog/webdev_arena/cat_broad.html" frameborder="0" scrolling="no" height="500px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 5. Distribution of narrow categories within broad categories.</p> <p>Using the topic modeling results, we perform LLM performance analysis and observe that models perform differently across categories. Gemini-2.0-flash shows the most dramatic variation, ranking among the lowest in Game Development while performing much better in other categories.</p> <div> <iframe src="/assets/img/blog/webdev_arena/rank_broad_cat.html" frameborder="0" scrolling="no" height="500px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 6. Model performance across different broad categories.</p> <p>We calculated the win rates of Gemini 2.0, Claude-3.5-sonnet, O3-mini, and Deepseek-R1 across narrow categories, counting ties as 0.5 wins. Even within the same broad category, models performed differently across specific topics, highlighting their varying strengths. This shows the importance of fine-grained categorical analysis for LLM evaluations.</p> <p>In website design, Deepseek-R1 performed significantly better in Job Board Design but had a noticeably lower performance in Portfolio Website Design. In Game Development, Claude-3.5-sonnet maintained a relatively consistent win rate across most categories, except for a dip in Clicker Game Development. Gemini 2.0 and O3-mini show fluctuating win rates.</p> <div> <iframe src="/assets/img/blog/webdev_arena/winrate_web.html" frameborder="0" scrolling="no" height="500px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 7. Model win rates across website design categories.</p> <div> <iframe src="/assets/img/blog/webdev_arena/winrate_game.html" frameborder="0" scrolling="no" height="500px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 8. Model win rates across game development categories.</p> <p>One of the main insights from analyzing the data is that a large chunk of the votes are from the sample prompts on the main screen. We found the 5 most asked questions were</p> <table> <thead> <tr> <th>Project Request</th> <th>Number of Times</th> </tr> </thead> <tbody> <tr> <td>👨‍💻 Clone of VS Code / Cursor</td> <td>4,189</td> </tr> <tr> <td>Make me a clone of WhatsApp Chat App</td> <td>3,385</td> </tr> <tr> <td>♟️ Build a game of chess</td> <td>3,154</td> </tr> <tr> <td>📰 Clone of Hacker News</td> <td>2,740</td> </tr> <tr> <td>🐦 Design a modern Twitter profile layout</td> <td>2,544</td> </tr> </tbody> </table> <p>Therefore, we deduplicated the data, reducing the votes from 103096 to 61473, and recreated the deduplicated leaderboard. This leaderboard matches the original leaderboard with only a few minor swaps of models (deepseek-v3 and gpt-4o). The elo difference between them is mainly caused by having less data.</p> <table> <thead> <tr> <th>Rank</th> <th>Model</th> <th>Original Score</th> <th>Deduplicated Score</th> <th>Delta</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>claude-3-7-sonnet-20250219</td> <td>1362.94</td> <td>1311.40</td> <td>51.54</td> </tr> <tr> <td>2</td> <td>claude-3-5-sonnet-20241022</td> <td>1245.85</td> <td>1209.07</td> <td>36.78</td> </tr> <tr> <td>3</td> <td>deepseek-r1</td> <td>1204.70</td> <td>1165.42</td> <td>39.28</td> </tr> <tr> <td>4</td> <td>early-grok-3-so-false</td> <td>1146.99</td> <td>1111.37</td> <td>35.62</td> </tr> <tr> <td>5</td> <td>o3-mini-2025-01-31-high</td> <td>1146.13</td> <td>1119.43</td> <td>26.70</td> </tr> <tr> <td>6</td> <td>o3-mini-2025-01-31-high-so-false</td> <td>1137.20</td> <td>1119.85</td> <td>17.35</td> </tr> <tr> <td>7</td> <td>claude-3-5-haiku-20241022</td> <td>1136.34</td> <td>1112.94</td> <td>23.40</td> </tr> <tr> <td>8</td> <td>o3-mini-2025-01-31-so-false</td> <td>1109.04</td> <td>1083.30</td> <td>25.74</td> </tr> <tr> <td>9</td> <td>gemini-2.0-flash-thinking-01-21-so-false</td> <td>1108.81</td> <td>1099.89</td> <td>8.92</td> </tr> <tr> <td>10</td> <td>gemini-2.0-pro-exp-02-05</td> <td>1101.89</td> <td>1078.59</td> <td>23.30</td> </tr> <tr> <td>11</td> <td>o3-mini-2025-01-31</td> <td>1099.29</td> <td>1076.50</td> <td>22.78</td> </tr> <tr> <td>12</td> <td>o1-2024-12-17-so-false</td> <td>1054.52</td> <td>1043.16</td> <td>11.36</td> </tr> <tr> <td>13</td> <td>o1-2024-12-17</td> <td>1049.87</td> <td>1035.79</td> <td>14.09</td> </tr> <tr> <td>14</td> <td>o1-mini-2024-09-12</td> <td>1046.62</td> <td>1028.03</td> <td>18.59</td> </tr> <tr> <td>15</td> <td>gemini-2.0-flash-thinking-exp-01-21</td> <td>1034.30</td> <td>1014.79</td> <td>19.52</td> </tr> <tr> <td>16</td> <td>gemini-2.0-flash-001-so-false</td> <td>1031.71</td> <td>1011.87</td> <td>19.84</td> </tr> <tr> <td>17</td> <td>gemini-2.0-flash-thinking-exp-1219</td> <td>1023.31</td> <td>1005.65</td> <td>17.66</td> </tr> <tr> <td>18</td> <td>gemini-exp-1206</td> <td>1022.30</td> <td>998.24</td> <td>24.07</td> </tr> <tr> <td>19</td> <td>gemini-2.0-flash-exp</td> <td>983.69</td> <td>966.74</td> <td>16.94</td> </tr> <tr> <td>20</td> <td>qwen-max-2025-01-25</td> <td>979.81</td> <td>971.04</td> <td>8.77</td> </tr> <tr> <td>21</td> <td>gpt-4o-2024-11-20</td> <td>964.00</td> <td>964.00</td> <td>0.00</td> </tr> <tr> <td>22</td> <td>deepseek-v3</td> <td>963.81</td> <td>940.75</td> <td>23.05</td> </tr> <tr> <td>23</td> <td>qwen-2.5-coder-32b-instruct</td> <td>903.04</td> <td>907.14</td> <td>-4.09</td> </tr> <tr> <td>24</td> <td>gemini-1.5-pro-002</td> <td>893.47</td> <td>891.07</td> <td>2.40</td> </tr> <tr> <td>25</td> <td>llama-v3.1-405b-instruct</td> <td>811.66</td> <td>798.96</td> <td>12.71</td> </tr> </tbody> </table> <p style="color:gray; text-align: center; padding-top: 10px;">Figure 9. Comparison of original and deduplicated leaderboards showing scores for all models.</p> <p>Another interesting difference between Chatbot Arena and WebDev Arena is the tie ratio. Webdev Arena has a 26% tie ratio and Chatbot Arena has a tie ratio of 35%. However, out of the ties, we note that WebDev arena has a much higher ratio of them being both bad. We will get to this later with the errors that can occur.</p> <table> <thead> <tr> <th></th> <th>WebDev Arena</th> <th>Chatbot Arena</th> </tr> </thead> <tbody> <tr> <td>Model A</td> <td>36.6%</td> <td>34%</td> </tr> <tr> <td>Model B</td> <td>37%</td> <td>34%</td> </tr> <tr> <td>Tie</td> <td>7.8%</td> <td>16%</td> </tr> <tr> <td>Bothbad</td> <td>18%</td> <td>16%</td> </tr> </tbody> </table> <p>We think this task is much easier to distinguish compared to Chatbot Arena.<br/> To explain for the large number of people voting “both bad”, we took a look at model failures, when the model either hallucinates a library that does not exist/has not been imported, or writes code that can not compile.<br/> Sometimes, prompts will result in the LLMs outputting buggy code or failing to properly install dependencies. The main culprit of this is llama-v3.1.<br/> <img src="/assets/img/blog/webdev_arena/distribution-of-error-types-per-model.png" alt="Distribution of Error Types" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 90%"/></p> <p style="color:gray; text-align: center;">Figure 10. Distribution of error types across different models.</p> <p>Additionally we analyzed the length in tokens of requests. Most requests are under 25,000 tokens and we found several outliers with extremely long messages.</p> <table> <thead> <tr> <th>Statistic</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>mean</td> <td>154.91</td> </tr> <tr> <td>std</td> <td>1613.12</td> </tr> <tr> <td>min</td> <td>0.00</td> </tr> <tr> <td>25%</td> <td>7.00</td> </tr> <tr> <td>50%</td> <td>11.00</td> </tr> <tr> <td>75%</td> <td>29.00</td> </tr> <tr> <td>max</td> <td>269157.00</td> </tr> </tbody> </table> <p><img src="/assets/img/blog/webdev_arena/distribution-of-request-lengths.png" alt="Distribution of Request Lengths" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 90%"/></p> <p style="color:gray; text-align: center;">Figure 11. Distribution of request lengths in tokens.</p> <h3 id="how-we-implemented-webdev-arena">How we implemented WebDev Arena?</h3> <p>One of the key challenges in building WebDev Arena was ensuring that LLMs generate consistent, deployable web applications. To achieve this, we developed a structured output approach that standardizes how models generate web applications.</p> <p>Structured output is a technique that ensures LLM responses adhere to a predefined JSON schema. Rather than allowing models to generate freeform code or text, we constrain their output to match a specific structure. This provides several key benefits:</p> <ol> <li><strong>Type Safety</strong>: Eliminates the need to validate or retry incorrectly formatted responses</li> <li><strong>Consistent Formatting</strong>: Ensures all models generate code in a standardized way</li> <li><strong>Better Error Handling</strong>: Makes model refusals and errors programmatically detectable</li> <li><strong>Simplified Prompting</strong>: Reduces the need for complex prompting to achieve consistent formatting</li> </ol> <p>Rather than letting models generate code in an unconstrained format, we prompt them to output code in a specific structure that includes:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
  // Detailed explanation of the implementation plan
  commentary: string,
  // Template configuration
  template: string,
  title: string,
  description: string,
  // Dependency management
  additional_dependencies: string[],
  has_additional_dependencies: boolean,
  install_dependencies_command: string,
  // Application configuration
  port: number | null,
  file_path: string,
  // The actual implementation
  code: string
}
</code></pre></div></div> <p>This structured format helps ensure:</p> <ol> <li>Clear documentation of the implementation approach through the `commentary` field</li> <li>Proper dependency tracking and installation instructions</li> <li>Consistent file organization within the Next.js project structure</li> <li>Runtime configuration like port numbers are explicitly defined</li> </ol> <h4 id="infrastructure-and-execution-environment">Infrastructure and Execution Environment</h4> <p>Under the hood, WebDev Arena utilizes Amazon’s Firecracker microVM technology through E2B’s infrastructure. Each sandbox runs in a lightweight virtual machine that provides enhanced security and workload isolation while maintaining the resource efficiency of containers. These microVMs are managed by a Virtual Machine Monitor (VMM) using the Linux Kernel-based Virtual Machine (KVM), with a minimalist design that excludes unnecessary devices and functionality to reduce memory footprint and attack surface.</p> <p>This infrastructure enables WebDev Arena to run thousands of concurrent LLM-generated applications with complete isolation - each application operates in its own secure environment with dedicated CPU, memory, and networking resources, while maintaining sub-second startup times (~150ms) essential for real-time model comparisons.</p> <h4 id="prompting-strategy-and-system-design">Prompting Strategy and System Design</h4> <p>Our system prompt is designed to create a clear “persona” for the LLM as an expert frontend engineer with strong UI/UX skills. The prompt includes several critical components:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>export const SYSTEM_PROMPT = `
    You are an expert frontend React engineer who is also a great UI/UX designer. Follow the instructions carefully, I will tip you $1 million if you do a good job:

    - Think carefully step by step.
    - Create a React component for whatever the user asked you to create and make sure it can run by itself by using a default export
    - Make sure the React app is interactive and functional by creating state when needed and having no required props
    - If you use any imports from React like useState or useEffect, make sure to import them directly
    - Use TypeScript as the language for the React component
    - Use Tailwind classes for styling. DO NOT USE ARBITRARY VALUES (e.g. 'h-[600px]'). Make sure to use a consistent color palette.
    - Make sure you specify and install ALL additional dependencies.
    - Make sure to include all necessary code in one file.
    - Do not touch project dependencies files like package.json, package-lock.json, requirements.txt, etc.
    - Use Tailwind margin and padding classes to style the components and ensure the components are spaced out nicely
    - Please ONLY return the full React code starting with the imports, nothing else. It's very important for my job that you only return the React code with imports. DO NOT START WITH \`\`\`typescript or \`\`\`javascript or \`\`\`tsx or \`\`\`.
    - ONLY IF the user asks for a dashboard, graph or chart, the recharts library is available to be imported, e.g. \`import { LineChart, XAxis, ... } from "recharts"\` &amp; \`&lt;LineChart ...&gt;&lt;XAxis dataKey="name"&gt; ...\`. Please only use this when needed. You may also use shadcn/ui charts e.g. \`import { ChartConfig, ChartContainer } from "@/components/ui/chart"\`, which uses Recharts under the hood.
    - For placeholder images, please use a &lt;div className="bg-gray-200 border-2 border-dashed rounded-xl w-16 h-16" /&gt;
`;
</code></pre></div></div> <p>Our prompting strategy involved a few key aspects:</p> <ol> <li><strong>Clear Role Definition</strong>: We establish the model as an expert frontend engineer, which helps frame its responses appropriately</li> <li><strong>Explicit Constraints</strong>: We specify important limitations upfront:</li> </ol> <ul> <li>Use of TypeScript</li> <li>Tailwind for styling (with explicit prohibition of arbitrary values)</li> <li>Single-file components</li> <li>No modification of project dependencies</li> </ul> <ol> <li><strong>Quality Guidelines</strong>: We include specific quality requirements:</li> </ol> <ul> <li>Interactive and functional components</li> <li>Proper state management</li> <li>Consistent color palette</li> <li>Appropriate spacing using Tailwind classes</li> </ul> <p>Our system prompt also includes information pertaining to a predefined framework to provide the LLM a consistent starting point for generated applications. WebDev Arena currently supports Next.js by Vercel, which is a powerful React framework that enables developers to build high-performance web applications with features like hybrid rendering (SSR, SSG, and ISR), React Server Components, and API routes. Next.js is one of the leading web frameworks with 7.2 million weekly npm downloads and ~130,000 Github Stars, which is why we are providing first-class support for it through WebDev Arena.</p> <p><img src="/assets/img/blog/webdev_arena/framework-npm-trends.png" alt="Framework NPM Trends" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 90%"/></p> <p style="color:gray; text-align: center;">Figure 12. NPM download trends for popular web frameworks.</p> <h4 id="performance-analysis-single-vs-ensemble-approach">Performance Analysis: Single vs Ensemble Approach</h4> <p>We discovered that different models require different prompting approaches. Our implementation includes specific handling for various model architectures.</p> <p>For models lacking native structured output capabilities, we developed a two-stage generation pipeline: (1) Generate the initial code using the model’s native format and (2) pass the result through a GPT-4-mini instance to convert it into our structured format.</p> <p>We conducted a systematic ablation study examining structured output’s impact on model performance in web development tasks. Each model was evaluated in two configurations:</p> <ol> <li>With structured output (using JSON schema constraints)</li> <li>Without structured output (natural language generation)</li> </ol> <p>For example, we evaluated o3-mini (with/without structured output), o1 (with/without structured output), and Gemini models (with/without structured output).</p> <p>Our analysis revealed a consistent performance advantage for models operating without structured output constraints, as evidenced in Figure 13.</p> <table> <thead> <tr> <th>Model Variant</th> <th>Arena Score</th> <th>Δ Score</th> <th>Votes</th> <th>95% CI</th> </tr> </thead> <tbody> <tr> <td>o3-mini-high (w/o structured)</td> <td>1169.19</td> <td>+16.08</td> <td>1,976</td> <td>+12.73/-11.74</td> </tr> <tr> <td>o3-mini-high</td> <td>1153.11</td> <td>-</td> <td>2,982</td> <td>+10.17/-10.51</td> </tr> <tr> <td colspan="5"></td> </tr> <tr> <td>o3-mini (w/o structured)</td> <td>1129.82</td> <td>+24.12</td> <td>2,688</td> <td>+10.64/-10.42</td> </tr> <tr> <td>o3-mini</td> <td>1105.70</td> <td>-</td> <td>6,388</td> <td>+7.89/-8.59</td> </tr> <tr> <td colspan="5"></td> </tr> <tr> <td>o1 (w/o structured)</td> <td>1066.29</td> <td>+12.98</td> <td>2,140</td> <td>+12.94/-13.37</td> </tr> <tr> <td>o1 (20241217)</td> <td>1053.31</td> <td>-</td> <td>9,271</td> <td>+4.71/-6.91</td> </tr> <tr> <td colspan="5"></td> </tr> <tr> <td>Gemini-2.0-Flash-Thinking-01-21 (w/o structured)</td> <td>1125.98</td> <td>+88.76</td> <td>405</td> <td>+30.49/-27.58</td> </tr> <tr> <td>Gemini-2.0-Flash-Thinking-01-21</td> <td>1037.22</td> <td>-</td> <td>1,064</td> <td>+17.04/-17.86</td> </tr> </tbody> </table> <p style="color:gray; text-align: center; padding-top: 10px;">Figure 13. Comparison of model performance with and without structured output constraints, showing consistent performance advantages for unstructured output across different model architectures.</p> <p>Our experimental results demonstrate that unstructured output configurations consistently outperform their structured counterparts across all tested model architectures, with Arena Score improvements ranging from +12.98 to +88.76 points. These improvements are statistically significant within 95% confidence intervals across all model pairs. The magnitude of this performance impact varies considerably by model architecture - Gemini exhibits the most substantial performance delta (+88.76), while o3-mini-high shows the smallest improvement (+16.08).</p> <p>These findings suggest that while structured output constraints provide benefits for downstream processing and code organization, they may introduce performance limitations in web development tasks. This performance-structure tradeoff appears to be more pronounced in certain model architectures, particularly in the Gemini series, indicating that the impact of structured output constraints may be architecture-dependent.</p> <h3 id="vision-support-for-web-development">Vision Support for Web Development</h3> <p>We’ve extended WebDev Arena to support multi-modal models, enabling evaluation of vision-language capabilities in web development contexts. Currently, seven production models in our evaluation suite support vision inputs: Claude 3.7 Sonnet, Claude 3.5 Sonnet, Claude 3.5 Haiku, GPT-4o, and three variants of Gemini (2.0-flash-thinking, 2.0-pro-exp, and 1.5-pro-002). Each model accepts visual input through our structured schema, which standardizes how images are processed and incorporated into the development workflow.</p> <p>Our implementation supports base64-encoded images and URLs, with vision models demonstrating capabilities in UI replication, design-to-code translation, and visual bug fixing. Early deployments show that models can parse complex visual hierarchies, extract design tokens (colors, spacing, typography), and generate semantically meaningful JSX (JavaScript XML) from screenshots.</p> <p>The integration of vision capabilities has significant implications for web development evaluation. Traditional code generation benchmarks focus solely on text-to-code translation, but real-world development frequently involves visual references and design artifacts. By incorporating vision support, WebDev Arena more accurately reflects practical development workflows while providing quantitative insights into multi-modal code generation performance. We are currently collecting battle data to analyze the impact of visual context on code quality and will release comprehensive benchmarks comparing vision-enabled versus text-only performance across our supported models.</p> <h3 id="limitations-and-future-work">Limitations and Future Work</h3> <p>While WebDev Arena represents a significant step forward in evaluating LLMs for real-world software development, several limitations remain. Unlike existing benchmarks such as HumanEval and CodeXGLUE, which focus on function-level code generation, or SWE-bench, which evaluates issue resolution in open-source repositories, WebDev Arena is currently constrained to Next.js-based React applications. This limitation excludes other widely used frameworks such as Vue, Svelte, and backend technologies like Django or Rails. Furthermore, the single-file constraint does not accurately reflect real-world software engineering workflows, where projects are typically structured across multiple files with modular architectures. Another challenge is error handling—our analysis indicates that 18% of user votes fall into the “both bad” category, often due to dependency resolution failures, incorrect state management, or TypeScript-related compilation errors. A more sophisticated error classification framework would enhance the interpretability of failure cases and inform improvements in model reliability.</p> <p>Future work will extend WebDev Arena to a broader range of software engineering tasks, encompassing full-stack development and non-web domains to better capture the diverse challenges of real-world coding. Unlike traditional benchmarks that focus on algorithmic correctness or isolated bug fixes, our approach evaluates LLMs in the context of complete application development, emphasizing deployment-ready implementations. We also plan to refine our human evaluation methodology, enhance model ranking with more granular error diagnostics, and investigate the impact of multimodal capabilities, particularly in UI-to-code translation. By addressing these limitations, we aim to establish WebDev Arena as a comprehensive benchmark for assessing LLMs in practical software engineering contexts, bridging the gap between academic evaluation and real-world application development.</p> <h3 id="citation">Citation</h3> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">vichare2025webdev</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{WebDev Arena: A Live LLM Leaderboard for Web App Development}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Aryan Vichare and Anastasios N. Angelopoulos and Wei-Lin Chiang and Kelly Tang and Luca Manolache}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>

<span class="nc">@misc</span><span class="p">{</span><span class="nl">chiang2024chatbot</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Wei-Lin Chiang and Lianmin Zheng and Ying Sheng and Anastasios Nikolas Angelopoulos and Tianle Li and Dacheng Li and Hao Zhang and Banghua Zhu and Michael Jordan and Joseph E. Gonzalez and Ion Stoica}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
    <span class="na">eprint</span><span class="p">=</span><span class="s">{2403.04132}</span><span class="p">,</span>
    <span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
    <span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.AI}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>Aryan Vichare</name></author><summary type="html"><![CDATA[A Live LLM Leaderboard for Web App Development]]></summary></entry><entry><title type="html">RepoChat Arena</title><link href="https://lmarena.github.io/blog/2025/repochat-arena/" rel="alternate" type="text/html" title="RepoChat Arena"/><published>2025-02-12T00:00:00+00:00</published><updated>2025-02-12T00:00:00+00:00</updated><id>https://lmarena.github.io/blog/2025/repochat-arena</id><content type="html" xml:base="https://lmarena.github.io/blog/2025/repochat-arena/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>An integral part of using LLMs as part of a user’s coding workflow is navigating and understanding complex code bases. We launched <strong>RepoChat</strong> – a new arena that answers code-related queries using a user-provided github repository link.</p> <p>RepoChat lets models automatically retrieve relevant files from the given GitHub repository. It can resolve issues, review PRs, implement code, as well as answer higher level questions about the repositories-all without requiring users to provide extensive context.</p> <div class="video-container"> <video id="videoPlayer" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/img/blog/repochat_arena/repochat_demo_video.mp4" type="video/mp4"/> Your browser does not support the video tag. </video> </div> <p style="color:gray; text-align: center;">Video 1. A demo of RepoChat.</p> <style>.video-container{position:relative;width:90%;max-width:1200px;margin:auto}video{width:100%;border-radius:10px}</style> <p>So far, RepoChat has collected around <strong>20k battles</strong> and over <strong>4k votes</strong>. All statistics calculated in this blog use conversations and votes collected between <strong>November 30, 2024</strong> to <strong>Feburary 11, 2025</strong> inclusive.</p> <div style="margin-left: auto; margin-right: auto; width: fit-content;"> <table style="width:100%; border-collapse: collapse; border: 1px solid lightgray; background-color: white;"> <thead style="background-color: #cbe8f1; font-weight: bold; border-bottom: 1px solid lightgray;"> <tr> <th style="padding: 8px; border: 1px solid lightgray;">Metric</th> <th style="padding: 8px; border: 1px solid lightgray;">Value</th> </tr> </thead> <tbody> <tr style="background-color: white;"> <td style="padding: 8px; border: 1px solid lightgray;font-weight:500"># of conversations</td> <td style="padding: 8px; border: 1px solid lightgray; text-align: right;font-weight: 500">12,732</td> </tr> <tr style="background-color: #f9f9f9;"> <td style="padding: 8px; border: 1px solid lightgray;font-weight: 500"># of votes</td> <td style="padding: 8px; border: 1px solid lightgray; text-align: right;font-weight: 500">4,857</td> </tr> <tr style="background-color: white;"> <td style="padding: 8px; border: 1px solid lightgray;font-weight: 500"># of unique GitHub links (in convos)</td> <td style="padding: 8px; border: 1px solid lightgray; text-align: right;font-weight: 500">5,037</td> </tr> <tr style="background-color: #f9f9f9;"> <td style="padding: 8px; border: 1px solid lightgray;font-weight: 500"># of unique GitHub links (in votes)</td> <td style="padding: 8px; border: 1px solid lightgray; text-align: right;font-weight: 500">2,465</td> </tr> </tbody> </table> </div> <p style="color:gray; text-align: center;">Table 1. Number of conversations, votes, and unique GitHub links logged from 11/30/2024 - 02/11/2025.</p> <p>In this blog we will cover:</p> <ul> <li><a href="#initial-leaderboards"><strong>Initial Results</strong></a>: our preliminary results for the RepoChat leaderboards</li> <li><a href="#how-do-people-use-repochat"><strong>How do people use RepoChat</strong></a>: an analysis of the distribution of github links and user queries’ category, length, language, etc.</li> <li><a href="#how-does-it-work"><strong>How does it work</strong></a>: a more technical and detailed explanation of the RepoChat pipeline and implementation</li> <li><a href="#further-analysis-and-results"><strong>Further Analysis and Results</strong></a>: retriever and style controlled leaderboards, and other analysis of model rankings.</li> </ul> <h2 id="initial-leaderboards">Initial Leaderboards</h2> <p>Since there are two separate components (retriever and answer), we produce two separate leaderboards. Jump to <a href="#leaderboard-calculation">this section</a> for details about how the leaderboards are calculated, and to <a href="#further-analysis-and-results">further analysis</a> for more leaderboards such as <a href="#style-control">style-control</a>. All leaderboards can be reproduced using our <a href="https://colab.research.google.com/drive/1sdSE07D7IqKAdb8LwMlOGCIfmWih3Df3?usp=sharing">google colab notebook</a>.</p> <div style="margin-left: auto; margin-right: auto; width: fit-content;"> <table style="width:100%; border-collapse: collapse; border: 1px solid lightgray; background-color: white;"> <thead style="background-color: #cbe8f1; font-weight: bold; border-bottom: 1px solid lightgray;"> <tr> <th style="padding: 8px; border: 1px solid lightgray;">Rank</th> <th style="padding: 8px; border: 1px solid lightgray;">Model</th> <th style="padding: 8px; border: 1px solid lightgray;">Arena Score</th> <th style="padding: 8px; border: 1px solid lightgray;">95% CI</th> <th style="padding: 8px; border: 1px solid lightgray;">Votes</th> <th style="padding: 8px; border: 1px solid lightgray;">Org.</th> </tr> </thead> <tbody> <tr style="background-color: white;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">1</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">GPT 4o (20241120)</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">1088</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+12 | -10</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">1647</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">OpenAI</td> </tr> <tr style="background-color: #f9f9f9;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">2</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">DeepSeek V3</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">1032</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+24 | -21</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">362</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">DeepSeek</td> </tr> <tr style="background-color: white;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">3</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Claude 3.5 Sonnet (20241022)</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">1015</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+16 | -10</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">1737</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Anthropic</td> </tr> <tr style="background-color: #f9f9f9;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">4</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Gemini 2.0 Flash Exp</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">1005</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+15 | -13</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">1066</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Google</td> </tr> <tr style="background-color: white;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">5</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Gemini 1.5 Pro</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">974</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+19 | -16</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">789</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Google</td> </tr> <tr style="background-color: #f9f9f9;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">6</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">GPT 4o mini (20240718)</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">970</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+24 | -17</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">521</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">OpenAI</td> </tr> <tr style="background-color: white;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">7</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">LLaMA 3.1 405B Instruct FP8</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">914</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+13 | -22</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">750</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Meta</td> </tr> </tbody> </table> </div> <p style="color:gray; text-align: center;">Table 2. Arena ratings of seven popular models based on over 4K votes collected between November 30, 2024 to Feburary 11, 2025. This ranks the models for generating model response</p> <div style="margin-left: auto; margin-right: auto; width: fit-content;"> <table style="width:100%; border-collapse: collapse; border: 1px solid lightgray; background-color: white;"> <thead style="background-color: #cbe8f1; font-weight: bold; border-bottom: 1px solid lightgray;"> <tr> <th style="padding: 8px; border: 1px solid lightgray;">Rank</th> <th style="padding: 8px; border: 1px solid lightgray;">Model</th> <th style="padding: 8px; border: 1px solid lightgray;">Arena Score</th> <th style="padding: 8px; border: 1px solid lightgray;">95% CI</th> <th style="padding: 8px; border: 1px solid lightgray;">Org.</th> </tr> </thead> <tbody> <tr style="background-color: white;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">1</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Gemini 1.5 Flash 002</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">1013</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+14 | -21</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Google</td> </tr> <tr style="background-color: #f9f9f9;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">2</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">GPT 4o mini (20240718)</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">987</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+21 | -14</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">OpenAI</td> </tr> </tbody> </table> </div> <p style="color:gray; text-align: center;">Table 3. Arena ratings of the two retrievers based on 4K votes collected between November 30, 2024 to Feburary 11, 2025. This ranks the models for retrieving relevant files.</p> <h2 id="how-do-people-use-repochat">How do people use RepoChat?</h2> <p><strong>What types of Github links do users input?</strong> RepoChat features a wide range of GitHub links, including repositories, issues, pull requests, and others. We find that the vast majority (almost 80%) of user input links are repository links, followed by issues and branches.</p> <p><img src="/assets/img/blog/repochat_arena/PIC_pie_chart.png" alt="GitHub Link Categories" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 70%"/></p> <p style="color:gray; text-align: center;">Figure 1. Breakdown of the GitHub link categories in RepoChat by percentage.</p> <p><strong>What programming languages do people ask about?</strong> The following statistic is calculated based on the file extensions of the relevant files. This serves as an indicator of the programming languages users are most frequently inquiring about/coding in. The abundance of markdown files is primarily due to README files, which are often extracted due to containing critical descriptions and instructions for the repository.</p> <p><img src="/assets/img/blog/repochat_arena/PIC_file_type.png" alt="Programming Language Breakdown" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 90%"/></p> <p style="color:gray; text-align: center;">Figure 2. Breakdown of retrieved file types by percentage.</p> <p><strong>What natural languages do the user queries contain?</strong> Most of our votes contain user queries in English, followed by Russian and Chinese.</p> <p><img src="/assets/img/blog/repochat_arena/PIC_natural_language.png" alt="Natural Language Breakdown" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 90%"/></p> <p style="color:gray; text-align: center;">Figure 3. Breakdown of user query’s language by count.</p> <p><strong>How long are the user queries?</strong> The user query length varies significantly, ranging from a minimum of 1 token to a maximum of 1,406,325 tokens, with a median of 22 tokens. Short-length queries mostly consist of prose requesting implementations or instructions, whereas longer queries often include extensive code blocks.</p> <p><img src="/assets/img/blog/repochat_arena/PIC_query_len.png" alt="Query Length Distribution" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 90%"/></p> <p style="color:gray; text-align: center;">Figure 4. Distribution of user query length.</p> <p><strong>How long are the retrieved file contexts?</strong> The distribution of retrieved file content is more stable, with a median of 8,870, an average of 12,508, and a maximum of 126,329 tokens, with the occasional empty retrieval.</p> <p><img src="/assets/img/blog/repochat_arena/PIC_context_len.png" alt="Retrieved Context Length Distribution" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 90%"/></p> <p style="color:gray; text-align: center;">Figure 5. Distribution of retrieved file context length.</p> <p><strong>What are the most common user query types?</strong></p> <p>For more examples, please refer to the <a href="#appendix">appendix</a>. We have also released a dataset of around 4k battles <a href="https://huggingface.co/datasets/lmarena-ai/repochat-arena-battles-4k">here</a> on Hugging Face.</p> <ul> <li> <p><strong>High level questions about a repository:</strong></p> <ul> <li><a href="https://github.com/Mrjoulin/ITMOLabs">https://github.com/Mrjoulin/ITMOLabs</a><br/>“What is this repository about? How I can use it? What main languages and frameworks are used here?”</li> </ul> </li> <li> <p><strong>Specific how-to questions:</strong></p> <ul> <li><a href="https://github.com/notfiz/de3u">https://github.com/notfiz/de3u</a><br/>“How do I install this on Linux (Mint 22)? Be thorough.”</li> </ul> </li> <li> <p><strong>Implementation requests:</strong></p> <ul> <li><a href="https://github.com/dipu-bd/lightnovel-crawler">https://github.com/dipu-bd/lightnovel-crawler</a><br/>“Write a Python script that fetches the top 10 highest-rated light novels from Anilist and then downloads all of their volumes in .epub format using this.”</li> </ul> </li> <li> <p><strong>Specific explanation requests:</strong></p> <ul> <li><a href="https://github.com/hhy-huang/GraphJudger">https://github.com/hhy-huang/GraphJudger</a><br/>“How is the graph knowledge graph in this repository generated?”</li> </ul> </li> <li> <p><strong>Requests for solving an issue:</strong></p> <ul> <li><a href="https://github.com/rust-lang/rfcs/issues/1856">https://github.com/rust-lang/rfcs/issues/1856</a><br/>“In 100 words or less, what is the easiest approach to resolving this issue (without giving up)?”</li> </ul> </li> <li> <p><strong>Requests for reviewing a PR:</strong></p> <ul> <li><a href="https://github.com/pandas-dev/pandas/pull/56061">https://github.com/pandas-dev/pandas/pull/56061</a><br/>“Explain the optimization.”</li> </ul> </li> <li> <p><strong>Queries with code snippets, requests for modifying existing code, and debugging:</strong> (click the arrow to expand the full query)</p> <ul> <li><a href="https://github.com/sktime/sktime">https://github.com/sktime/sktime</a><br/> <details> <summary>"debug the code as a professional would do. The code is for running in jupyter.</summary> <pre style="background-color: #f9f9f9; padding: 8px; border: 1px solid lightgray; font-family: monospace;">
  import pandas as pd
  import numpy as np
  from sktime.forecasting.model_selection import (
      ForecastingOptunaSearchCV,
      ExpandingWindowSplitter,
      temporal_train_test_split
  )
  from sktime.forecasting.base import ForecastingHorizon
  from sktime.performance_metrics.forecasting import MeanAbsolutePercentageError
  from sktime.forecasting.compose import TransformedTargetForecaster
  from sktime.forecasting.statsforecast import (
      StatsForecastMSTL,
      StatsForecastAutoETS,
      StatsForecastAutoARIMA,
      StatsForecastAutoTheta
  )
  from sktime.transformations.series.detrend import Detrender
  from sktime.transformations.series.deseasonalize import Deseasonalizer
  import optuna
  import warnings

warnings.filterwarnings('ignore')

# Load your time series data

# Ensure 'pivot_table' is defined and contains the 'PAN4_PIBPMG4' series

y = pivot_table['PAN4_PIBPMG4']

# Split the data into train and test sets

y_train, y_test = temporal_train_test_split(y, test_size=8)

# Define the forecasting horizon

fh = ForecastingHorizon(np.arange(1, 9), is_relative=True)

# Set up cross-validation with an expanding window splitter

cv = ExpandingWindowSplitter(fh=fh, initial_window=len(y_train) - 8)

# Define the parameter space for tuning

param_distributions = {
'forecaster**season_length': optuna.distributions.CategoricalDistribution([(4,), (8,)]),
'forecaster**trend_forecaster': optuna.distributions.CategoricalDistribution([
StatsForecastAutoETS(model="ZZZ"),
StatsForecastAutoARIMA(seasonal=True),
StatsForecastAutoTheta()
]),
'forecaster\_\_stl_kwargs': {
'robust': optuna.distributions.CategoricalDistribution([True, False]),
'period': optuna.distributions.IntUniformDistribution(4, 8)
}
}

# Initialize the MSTL forecaster

mstl_forecaster = StatsForecastMSTL()

# Create a pipeline with optional transformations

forecaster = TransformedTargetForecaster(steps=[
("detrender", Detrender()),
("deseasonalizer", Deseasonalizer()),
("mstl_forecaster", mstl_forecaster)
])

# Set up the OptunaSearchCV

optuna_search = ForecastingOptunaSearchCV(
forecaster=forecaster,
cv=cv,
param_distributions=param_distributions,
scoring=MeanAbsolutePercentageError(symmetric=True),
n_trials=100,
random_state=42
)

# Fit the model

optuna_search.fit(y_train)

# Predict

y_pred = optuna_search.predict(fh)

# Evaluate

mape = MeanAbsolutePercentageError(symmetric=True)
final_mape = mape(y_test, y_pred)
print(f"Final sMAPE: {final_mape:.2f}")

# Plot results

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 7))
plt.plot(y_train.index, y_train.values, label='Training Data', color='blue')
plt.plot(y_test.index, y_test.values, label='Test Data', color='green')
plt.plot(y_pred.index, y_pred.values, label='Predictions', color='red', linestyle='--')
plt.title('MSTL Forecast Results with Optuna Optimization')
plt.legend()
plt.grid(True)
plt.show()

# Save the best model

from joblib import dump
dump(optuna*search.best_forecaster*, 'best_mstl_model_optuna.joblib')
print("\nBest model saved as 'best_mstl_model_optuna.joblib'")

# Print additional optimization results

print("\nOptimization Results:")
print("="\*50)
print(f"Number of completed trials: {len(optuna*search.cv_results*)}")
print(f"Best trial number: {optuna*search.best_index*}")
print(f"Best sMAPE achieved during optimization: {optuna*search.best_score*:.2f}")

# Print best parameters

print("\nBest Parameters Found:")
print("="\*50)
for param, value in optuna*search.best_params*.items():
print(f"{param}: {value}")
  </pre>" </details> </li> </ul> </li> </ul> <h2 id="how-does-it-work">How Does It Work?</h2> <p>Each generated answer is the collaborative effort between two separate LLM models. The <strong>retriever model</strong> extracts the relevant files from the github repository according to the given user input. The extracted file contents are then concatenated together with the user query and used as a prompt for the <strong>answer model</strong>, which then generates the response. <img src="/assets/img/blog/repochat_arena/PIC_complete_pipeline.png" alt="RepoChat full pipeline" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 60%"/></p> <p style="color:gray; text-align: center;">Figure 6. A graphical representation of the full RepoChat retriever-&gt;answer pipeline.</p> <h3 id="file-retrieval">File Retrieval</h3> <p>We select two models as retrievers: <code class="language-plaintext highlighter-rouge">gemini-1.5-flash-002</code> and <code class="language-plaintext highlighter-rouge">gpt-4o-mini</code>.</p> <p>When a valid GitHub link is provided, the repository is cloned, and its file structure is extracted and formatted in an indented layout. The retriever model is then prompted to strategically identify files from the file structure that are potentially useful for answering the query. It then outputs these files in decreasing order of relevance and a specified markdown format.</p> <details> <summary>Click to view retriever prompt</summary> The following files are found in the repository:<br/> {file structure}<br/> Please provide a list of files that you would like to search for answering the user query. <br/> Enclose the file paths in a list in a markdown code block as shown below:<br/> ```<br/> 1. [[ filepath_1 ]]\n<br/> 2. [[ filepath_2 ]]\n<br/> 3. [[ filepath_3 ]]\n<br/> ...<br/> ```<br/> Think step-by-step and strategically reason about the files you choose to maximize the chances of finding the answer to the query. Only pick the files that are most likely to contain the information you are looking for in decreasing order of relevance. Once you have selected the files, please submit your response in the appropriate format mentioned above (markdown numbered list in a markdown code block). The filepath within [[ and ]] should contain the complete path of the file in the repository.<br/> {query} </details> <h3 id="model-response-generation">Model Response Generation</h3> <p>The contents of relevant files are extracted, concatenated with the user query, and provided to the responding LLM as a prompt in a specified format. If the provided link is not a direct repository link but instead links to issues or pull requests, a query context section containing the content of the issue or PR thread is also included.</p> <details> <summary>Click to view answer model prompt format</summary> Here is a list of files in the repository that may help you answer the query:<br/> {pairs of (file_name, file_content)}<br/> ___<br/><br/> [INSTRUCTION]<br/><br/> You are an expert software engineer. Answer the following user query using provided context retrieved from the {repository_name} repository.<br/><br/> [QUERT CONTEXT]<br/><br/> {issue/PR content}<br/><br/> [USER QUERY]<br/><br/> {user query}<br/><br/> </details> <h2 id="further-analysis-and-results">Further Analysis and Results</h2> <h3 id="style-control">Style Control</h3> <p>Style (length and Markdown formatting) significantly impacts model ranking. A better formatted or visually appealing answer, though not necessarily better, is more likely to win. The style-controlled leaderboard isolates writing style from content quality. For details on our methodology, refer to <a href="https://blog.lmarena.ai/blog/2024/style-control/">this blog post</a>.</p> <div style="margin-left: auto; margin-right: auto; width: fit-content;"> <table style="width:100%; border-collapse: collapse; border: 1px solid lightgray; background-color: white;"> <thead style="background-color: #cbe8f1; font-weight: bold; border-bottom: 1px solid lightgray;"> <tr> <th style="padding: 8px; border: 1px solid lightgray;">Rank</th> <th style="padding: 8px; border: 1px solid lightgray;">Model</th> <th style="padding: 8px; border: 1px solid lightgray;">Arena Score</th> <th style="padding: 8px; border: 1px solid lightgray;">95% CI</th> <th style="padding: 8px; border: 1px solid lightgray;">Org.</th> </tr> </thead> <tbody> <tr style="background-color: white;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">1</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Claude 3.5 Sonnet (20241022)</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">1031</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+28 | -17</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Anthropic</td> </tr> <tr style="background-color: #f9f9f9;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">2</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">GPT 4o (20241120)</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">1028</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+23 | -24</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">OpenAI</td> </tr> <tr style="background-color: white;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">3</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">DeepSeek V3</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">1016</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+27 | -25</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">DeepSeek</td> </tr> <tr style="background-color: #f9f9f9;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">4</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Gemini 1.5 Pro</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">1007</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+21 | -25</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Google</td> </tr> <tr style="background-color: white;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">5</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Gemini 2.0 Flash Exp</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">997</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+20 | -23</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Google</td> </tr> <tr style="background-color: #f9f9f9;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">6</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">GPT 4o mini (20240718)</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">963</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+26 | -24</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">OpenAI</td> </tr> <tr style="background-color: white;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">7</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">LLaMA 3.1 405B Instruct FP8</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">956</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+23 | -23</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Meta</td> </tr> </tbody> </table> </div> <p style="color:gray; text-align: center;">Table 4. Style-controlled arena ratings of the answer models. </p> <p>Claude 3.5 Sonnet’s score and ranking has improved significantly with style control, claiming the top spot. GPT-4o’s score has decreased, greatly narrowing its lead over other models. Gemini 1.5 Pro has also seen a notable boost in score, climbing one rank higher.</p> <p><img src="/assets/img/blog/repochat_arena/PIC_ldb_vs_sc.png" alt="Original vs. Style-Controlled Leaderboard" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 60%"/></p> <p style="color:gray; text-align: center;">Figure 7. A comparison of the original and the style-controlled leaderboards of the answer models.</p> <p><strong>Why does style control affect models like Claude and GPT 4o so much?</strong></p> <p>This is likely because a significant portion of user queries are not focused on strictly code-writing tasks. Instead, many focus on code understanding—such as how to use the repository, its purpose, or other high-level questions—rather than specific code implementations. These types of queries shift the focus away from pure coding abilities, and instead place greater emphasis on organization and presentation, and overall style.</p> <p>Below is an example battle between Claude 3.5 Sonnet (20241022) and GPT 4o (20241120) with identical retrievals.</p> <p>The GitHub link is <strong>https://github.com/xtekky/gpt4free</strong>, and the user query is <strong>“Explain why in the new 0.4.0.0 version Blackbox provider is several times slower than in version 0.3.9.7”</strong></p> <details> <summary> Click to expand the example battle of Claude 3.5 Sonnet (20241022) vs. GPT 4o (20241120)</summary> <div style="margin-left: auto; margin-right: auto; width: fit-content;"> <table style="width:100%; table-layout: fixed; border-collapse: collapse; border: 1px solid lightgray; background-color: white;"> <thead style="background-color: #cbe8f1; font-weight: bold; border-bottom: 1px solid lightgray;"> <tr> <th style="padding: 8px; border: 1px solid lightgray; width: 50%;">Claude 3.5 Sonnet (20241022) Response</th> <th style="padding: 8px; border: 1px solid lightgray; width: 50%;">GPT 4o (20241120) Response</th> </tr> </thead> <tbody> <tr style="background-color: white;"> <td style="padding: 8px; border: 1px solid lightgray; vertical-align: top;"> <p>Let me analyze the code from the provided context to explain the potential reasons for Blackbox provider's slower performance in version 0.4.0.0.</p> <p>Looking at the <code>Blackbox.py</code> implementation, there are several key changes that could contribute to slower performance:</p> <ol> <li> <strong>Validation Process:</strong> The provider now implements a more complex validation process that requires fetching and parsing JavaScript files to obtain a validation token: <pre style="background-color: #f9f9f9; padding: 8px; border: 1px solid lightgray; font-family: monospace;">
            @classmethod
            async def fetch_validated(cls):
                cached_value = cls._load_cached_value()

                async with aiohttp.ClientSession() as session:
                    # Tries multiple URLs
                    urls_to_try = [
                        "https://www.blackbox.ai",
                        "https://api.blackbox.ai"
                    ]

                    for base_url in urls_to_try:
                        try:
                            async with session.get(base_url) as response:
                                if response.status != 200:
                                    continue
                                page_content = await response.text()
                                js_files = re.findall(r'static/chunks/\d{4}-[a-fA-F0-9]+\.js', page_content)
                                # Additional JS file parsing...
              </pre> </li> <li> <strong>Multiple URL Attempts:</strong> The code now tries multiple URLs and has to process multiple JavaScript files, which adds overhead: <ul> <li>It first tries <code>blackbox.ai</code>.</li> <li>Then falls back to <code>api.blackbox.ai</code>.</li> <li>For each URL, it has to download and parse multiple JS files.</li> </ul> </li> <li> <strong>Caching Implementation:</strong> While there is a caching mechanism implemented: <pre style="background-color: #f9f9f9; padding: 8px; border: 1px solid lightgray; font-family: monospace;">
            @classmethod
            def _load_cached_value(cls) -&gt; str | None:
                cache_file = cls._get_cache_file()
                if cache_file.exists():
                    try:
                        with open(cache_file, 'r') as f:
                            data = json.load(f)
                            return data.get('validated_value')
            </pre> <p>The cache might not be as effective as in previous versions, requiring more frequent validation token refreshes.</p> </li> <li> <strong>Additional Model Support:</strong> The provider now supports many more models and features: <pre style="background-color: #f9f9f9; padding: 8px; border: 1px solid lightgray; font-family: monospace;">
            vision_models = [default_vision_model, 'gpt-4o', 'gemini-pro', 'gemini-1.5-flash', 'llama-3.1-8b', 'llama-3.1-70b', 'llama-3.1-405b']
            web_search_models = ['blackboxai', 'meta-llama/Llama-3.3-70B-Instruct-Turbo', 'meta-llama/Meta-Llama-3.1-405B-Instruct-Lite-Pro']
            </pre> <p>This expanded model support likely requires additional validation and processing logic.</p> </li> </ol> <p>To improve performance, you could:</p> <ul> <li>Optimize the validation process to reduce unnecessary requests.</li> <li>Implement more aggressive caching of the validation token.</li> <li>Only load model information for the specific model being used.</li> <li>Consider using connection pooling to reduce connection overhead.</li> </ul> <p>The tradeoff seems to be between security/functionality and speed - the new version implements more robust validation and supports more features, but at the cost of performance.</p> <p>Would you like me to elaborate on any of these points or provide specific suggestions for optimizing the provider's performance?</p> </td> <td style="padding: 8px; border: 1px solid lightgray; vertical-align: top;"> <p>In version <strong>0.4.0.0</strong> of the <code>gpt4free</code> repository, the <strong>Blackbox provider</strong> is slower compared to version <strong>0.3.9.7</strong> likely due to several changes introduced in the newer implementation. Here's an analysis based on the provided context:</p> <hr/> <h3>1. Enhanced <code>fetch_validated</code> Method in <code>Blackbox</code></h3> <p>In <strong>version 0.4.0.0</strong>, the <code>Blackbox</code> provider introduces a more complex and robust mechanism for fetching and validating the <code>validated_value</code>. This involves:</p> <ul> <li><strong>Fetching multiple JavaScript files</strong> from the <code>https://www.blackbox.ai</code> or <code>https://api.blackbox.ai</code> endpoints.</li> <li><strong>Regex-based parsing of UUIDs</strong> in JavaScript files to extract the <code>validated_value</code>.</li> <li><strong>Checking the context around the UUID</strong> to ensure it is valid.</li> <li><strong>Retries and handling multiple URLs</strong>, which adds overhead.</li> </ul> <p>This validation mechanism includes additional asynchronous calls and error handling. While it improves reliability, it significantly increases the response time.</p> <p><strong>Comparison to version 0.3.9.7:</strong> The earlier version likely used a simpler or cached approach for this process, avoiding the need to repeatedly process JavaScript files and perform context validation.</p> <hr/> <h3>2. Increased Error Handling and Retrying</h3> <p>The new implementation introduces more robust error-handling mechanisms, including:</p> <ul> <li><strong>Retry loops</strong> for fetching JavaScript files and API calls.</li> <li><strong>Exponential backoff with random delays</strong> when processing requests.</li> <li><strong>Fallbacks for cached values</strong> if validation fails.</li> </ul> <p>While this improves reliability and stability, each retry and delay adds to the total processing time.</p> <hr/> <h3>3. Web Search Integration</h3> <p>The <code>create_async_generator</code> function now has an optional <strong>web search</strong> feature:</p> <ul> <li>If <code>web_search</code> is enabled, the provider calls an external search engine (via <code>get_search_message</code>) to refine the user's query.</li> <li>Although <code>web_search</code> is optional, if enabled, it introduces significant latency due to network calls and extra processing.</li> </ul> <hr/> <h3>4. Increased Complexity in Request Data Preparation</h3> <p>The new implementation constructs a more detailed payload for the API request:</p> <ul> <li><strong>Agent modes</strong> (e.g., <code>agentMode</code>, <code>trendingAgentMode</code>) are retrieved and included in the payload.</li> <li><strong>Model-specific settings</strong>, including image generation and other configurations, are passed in the data.</li> <li><strong>Image processing</strong> (if images are included) involves converting to data URIs.</li> </ul> <p>This additional processing step before sending the request increases the latency.</p> <hr/> <h3>5. More Advanced Delay and Retry Mechanisms</h3> <p>When an API request fails, the system uses:</p> <ul> <li><strong>Exponential backoff delays:</strong> <code>delay * (2 ** attempt) + random.uniform(0, 1)</code></li> <li><strong>Retries (default <code>max_retries=3</code>):</strong> Every failed request results in subsequent retries.</li> </ul> <p>These features, while improving resilience, can slow down the overall response time if the API is unreliable or under heavy load.</p> <hr/> <h3>6. Additional Debugging and Logging</h3> <p>The new version includes enhanced debugging and logging mechanisms (<code>debug.log</code>):</p> <ul> <li>Each step, including validation, retries, and errors, is logged.</li> <li>While useful for debugging, this can also slightly slow down execution, especially in development or verbose logging environments.</li> </ul> <hr/> <h3>7. Potential API Changes on Blackbox's Side</h3> <p>If the Blackbox API itself has undergone changes between the two versions, such as stricter validation or slower endpoints, this could also contribute to the slowdown. The need to fetch and validate <code>validated_value</code> repeatedly suggests that the newer version is designed to handle a more restrictive or rate-limited API.</p> <hr/> <h3>8. Cache Mechanism vs. Real-Time Validation</h3> <p>In version <strong>0.3.9.7</strong>, the <code>validated_value</code> may have been cached and reused across requests, reducing overhead. In version <strong>0.4.0.0</strong>, the <code>_load_cached_value</code> and <code>_save_cached_value</code> methods are still present, but more emphasis is placed on real-time validation through <code>fetch_validated</code>, which is inherently slower.</p> <hr/> <h3>Summary of Reasons for Slowdown</h3> <ol> <li><strong>More Robust <code>fetch_validated</code> Method:</strong> Repeatedly fetching and parsing JavaScript files for UUID validation adds significant overhead.</li> <li><strong>Increased Error Handling and Retries:</strong> New retry mechanisms and delays increase resilience but add latency.</li> <li><strong>Optional Web Search:</strong> If enabled, it introduces additional network calls and processing time.</li> <li><strong>Enhanced Payload Preparation:</strong> More detailed request data adds preprocessing time.</li> <li><strong>Reliance on Real-Time Validation:</strong> Real-time validation of the <code>validated_value</code> is slower than relying on cached values.</li> <li><strong>API Changes on Blackbox's Side:</strong> Stricter validation or rate-limiting by the Blackbox API could also be contributing.</li> </ol> <hr/> <h3>Recommendations to Improve Speed</h3> <ol> <li><strong>Cache the <code>validated_value</code>:</strong> Implement more aggressive caching and reduce reliance on real-time validation.</li> <li><strong>Optimize the <code>fetch_validated</code> process:</strong> Limit retries or check for updated values less frequently.</li> <li><strong>Disable features like <code>web_search</code>:</strong> Only enable it if explicitly needed.</li> <li><strong>Reduce retry attempts and exponential backoff delays:</strong> Optimize for faster error recovery in non-critical use cases.</li> </ol> </td> </tr> </tbody> </table> </div> </details> <h3 id="retriever-control">Retriever Control</h3> <p>Just like style, the retrievers too have an effect on model answer. A model with with a more complete or relevant retrieval will likely produce a better answer. Similar to style-control, the retriever-controlled leaderboard separates the effect of the retrievers from the answer model ranking.</p> <div style="margin-left: auto; margin-right: auto; width: fit-content;"> <table style="width:100%; border-collapse: collapse; border: 1px solid lightgray; background-color: white;"> <thead style="background-color: #cbe8f1; font-weight: bold; border-bottom: 1px solid lightgray;"> <tr> <th style="padding: 8px; border: 1px solid lightgray;">Rank</th> <th style="padding: 8px; border: 1px solid lightgray;">Model</th> <th style="padding: 8px; border: 1px solid lightgray;">Arena Score</th> <th style="padding: 8px; border: 1px solid lightgray;">95% CI</th> <th style="padding: 8px; border: 1px solid lightgray;">Org.</th> </tr> </thead> <tbody> <tr style="background-color: white;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">1</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">GPT 4o (20241120)</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">1088</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+12 | -10</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">OpenAI</td> </tr> <tr style="background-color: #f9f9f9;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">2</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">DeepSeek V3</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">1032</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+24 | -21</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">DeepSeek</td> </tr> <tr style="background-color: white;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">3</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Claude 3.5 Sonnet (20241022)</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">1015</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+16 | -10</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Anthropic</td> </tr> <tr style="background-color: #f9f9f9;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">4</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Gemini 2.0 Flash Exp</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">1005</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+15 | -13</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Google</td> </tr> <tr style="background-color: white;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">5</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Gemini 1.5 Pro</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">973</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+20 | -16</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Google</td> </tr> <tr style="background-color: #f9f9f9;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">6</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">GPT 4o mini (20240718)</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">971</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+23 | -17</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">OpenAI</td> </tr> <tr style="background-color: white;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">7</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">LLaMA 3.1 405B Instruct FP8</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">914</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+14 | -22</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Meta</td> </tr> </tbody> </table> </div> <p style="color:gray; text-align: center;">Table 5. Retriever-controlled arena ratings of the answer models. </p> <p>The retriever-controlled leaderboard shows only slight differences from the original, as the two chosen retrievers perform similarly and have little influence on the rankings</p> <h3 id="style--retriever-control">Style + Retriever Control</h3> <div style="margin-left: auto; margin-right: auto; width: fit-content;"> <table style="width:100%; border-collapse: collapse; border: 1px solid lightgray; background-color: white;"> <thead style="background-color: #cbe8f1; font-weight: bold; border-bottom: 1px solid lightgray;"> <tr> <th style="padding: 8px; border: 1px solid lightgray;">Rank</th> <th style="padding: 8px; border: 1px solid lightgray;">Model</th> <th style="padding: 8px; border: 1px solid lightgray;">Arena Score</th> <th style="padding: 8px; border: 1px solid lightgray;">95% CI</th> <th style="padding: 8px; border: 1px solid lightgray;">Org.</th> </tr> </thead> <tbody> <tr style="background-color: white;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">1</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Claude 3.5 Sonnet (20241022)</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">1031</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+28 | -17</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Anthropic</td> </tr> <tr style="background-color: #f9f9f9;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">2</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">GPT 4o (20241120)</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">1028</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+23 | -24</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">OpenAI</td> </tr> <tr style="background-color: white;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">3</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">DeepSeek V3</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">1016</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+27 | -25</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">DeepSeek</td> </tr> <tr style="background-color: #f9f9f9;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">4</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Gemini 1.5 Pro</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">1007</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+21 | -25</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Google</td> </tr> <tr style="background-color: white;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">5</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Gemini 2.0 Flash Exp</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">997</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+20 | -23</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Google</td> </tr> <tr style="background-color: #f9f9f9;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">6</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">GPT 4o mini (20240718)</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">963</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+26 | -24</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">OpenAI</td> </tr> <tr style="background-color: white;"> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">7</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">LLaMA 3.1 405B Instruct FP8</td> <td style="padding: 8px; text-align: left; border: 1px solid lightgray; font-weight: 500;">956</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">+23 | -23</td> <td style="padding: 8px; border: 1px solid lightgray; font-weight: 500;">Meta</td> </tr> </tbody> </table> </div> <p style="color:gray; text-align: center;">Table 6. Style-and-retriever-controlled arena ratings of the answer models. </p> <h3 id="leaderboard-calculation">Leaderboard Calculation</h3> <p>How do we get separate leaderboards for the retrievers and the answer models from one set of battles? Extra features!</p> <p>Chabot Arena leverages the <a href="https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model"><strong>Bradley-Terry</strong></a> model for scoring model strength using pairwise battles. We adopt its <a href="https://blog.lmarena.ai/blog/2024/extended-arena/"><strong>statistical extension</strong></a> to evaluate the additional subparts–the retrievers–by integrating them as extra features just like the answer models. Each retriever feature takes values from {-1, 0, 1}, indicating whether it was active in the file retrieval for model_a, neither/both, or model_b, respectively. By performing Logistic Regression on these additional retriever features along with the original model features, we obtain coefficients that are later scaled to become the leaderboard scores. <img src="/assets/img/blog/repochat_arena/PIC_winrates.png" alt="Model Pairwise Winrates" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 90%"/></p> <p style="color:gray; text-align: center;">Figure 8. Fraction of model A wins for all non-tied battles.</p> <h2 id="whats-next">What’s Next?</h2> <p>We are actively collecting more votes and integrating new models, with plans for more comprehensive analysis down the line. Additionally, we are exploring ways to enhance RepoChat by incorporating features such as support for private repositories, GitLab integration, and improvements to our retrieval process. Community contributions are welcome—feel free to ping us if you’d like to get involved!</p> <h2 id="appendix">Appendix</h2> <ul> <li><strong>High level questions about a repository:</strong> <ul> <li><a href="https://github.com/lllyasviel/ControlNet">https://github.com/lllyasviel/ControlNet</a><br/>“Explain”</li> <li><a href="https://github.com/remix-run/react-router">https://github.com/remix-run/react-router</a><br/>“How to use this repo?”</li> </ul> </li> <li> <p><strong>Specific how-to questions:</strong></p> <ul> <li><a href="https://github.com/leecig/OpenDevin/tree/main">https://github.com/leecig/OpenDevin/tree/main</a><br/>“How do I use a local LLM being ran by LMStudio?”</li> <li><a href="https://github.com/ohmyzsh/ohmyzsh/">https://github.com/ohmyzsh/ohmyzsh/</a><br/>“Tell me, what it means and how to use the <code class="language-plaintext highlighter-rouge">kubectx_mapping</code> in the plugins/kubectx?”</li> </ul> </li> <li> <p><strong>Implementation requests:</strong></p> <ul> <li><a href="https://github.com/varungodbole/prompt-tuning-playbook">https://github.com/varungodbole/prompt-tuning-playbook</a><br/>“Help me design a system prompt to extract metrics from financial reports.”</li> <li><a href="https://github.com/ocaml/ocaml">https://github.com/ocaml/ocaml</a><br/>“If I wanted to implement partial escape analysis, which of the OCaml Intermediate Representations would be best suited and where should I make the change?”</li> </ul> </li> <li> <p><strong>Specific explanation requests:</strong></p> <ul> <li><a href="https://github.com/ant-design/ant-design">https://github.com/ant-design/ant-design</a><br/>“Why does Ant Design 5 decide to use CSS-in-JS?”</li> <li><a href="https://github.com/celzero/rethink-app">https://github.com/celzero/rethink-app</a><br/>“How are connection/network changes handled?”</li> </ul> </li> <li> <p><strong>Requests for solving an issue:</strong></p> <ul> <li><a href="https://github.com/HALF111/calibration_CDS">https://github.com/HALF111/calibration_CDS</a><br/>“Solve this issue.”</li> <li><a href="https://github.com/pyca/cryptography/issues/7189">https://github.com/pyca/cryptography/issues/7189</a><br/>“Using asyncssh can fix the immediate loading error for the PEM file, but when accessing <code class="language-plaintext highlighter-rouge">cert.extensions</code> it still errors out with the same ordering issue.”</li> </ul> </li> <li> <p><strong>Requests for reviewing a PR:</strong> - <a href="https://github.com/fairy-stockfish/Fairy-Stockfish/pull/758">https://github.com/fairy-stockfish/Fairy-Stockfish/pull/758</a><br/>“Review this PR.” - <a href="https://github.com/ggerganov/llama.cpp/pull/10455">https://github.com/ggerganov/llama.cpp/pull/10455</a><br/>“What type of speculative decoding is supported with this pull request? “</p> </li> <li> <p><strong>Queries with code snippets, requests for modifying existing code, and debugging:</strong> (click the arrow to expand the full query)</p> <ul> <li><a href="https://github.com/julien-blanchon/arxflix/tree/feat/groq_whisper">https://github.com/julien-blanchon/arxflix/tree/feat/groq_whisper</a><br/> <details> <summary>"when using the gradio interface, the stage of generating video get me this error :</summary> <pre style="background-color: #f9f9f9; padding: 8px; border: 1px solid lightgray; font-family: monospace;">
  INFO:__main__:Generated assets successfully.
  INFO:backend.main:Generating video to output.mp4 from tmpmpc_r1vm
  Free port: 55427
  Exposed directory /var/folders/3j/jv5_hbgn59g9yxccxtfcvz4r0000gp/T/tmpmpc_r1vm
  Starting up http-server, serving /var/folders/3j/jv5_hbgn59g9yxccxtfcvz4r0000gp/T/tmpmpc_r1vm

http-server version: 14.1.1

http-server settings:
CORS: true
Cache: 3600 seconds
Connection Timeout: 120 seconds
Directory Listings: visible
AutoIndex: visible
Serve GZIP Files: false
Serve Brotli Files: false
Default File Extension: none

Available on:
http://localhost:55427
Hit CTRL-C to stop the server

INFO:backend.utils.generate_video:Exposed directory /var/folders/3j/jv5_hbgn59g9yxccxtfcvz4r0000gp/T/tmpmpc_r1vm
INFO:backend.utils.generate_video:Generating video to /var/folders/3j/jv5_hbgn59g9yxccxtfcvz4r0000gp/T/tmpmpc_r1vm/output.mp4
(node:93666) ExperimentalWarning: CommonJS module /opt/homebrew/lib/node_modules/npm/node_modules/debug/src/node.js is loading ES Module /opt/homebrew/lib/node_modules/npm/node_modules/supports-color/index.js using require().
Support for loading ES Module in require() is an experimental feature and might change at any time
(Use `node --trace-warnings ...` to show where the warning was created)
Bundled code ━━━━━━━━━━━━━━━━━━ 550ms
[Fri Dec 06 2024 00:02:39 GMT+0100 (Central European Standard Time)] "GET /audio.wav" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/131.0.6778.87 Safari/537.36"
(node:93654) [DEP0066] DeprecationWarning: OutgoingMessage.prototype.\_headers is deprecated
(Use `node --trace-deprecation ...` to show where the warning was created)
Composition Arxflix
Codec h264
Output /var/folders/3j/jv5_hbgn59g9yxccxtfcvz4r0000gp/T/tmpmpc_r1vm/output.mp4
Concurrency 1x
Rendering frames ━━━━━━━━━━━━━━━━━━ 0/2353
Encoding video ━━━━━━━━━━━━━━━━━━ 0/2353[Fri Dec 06 2024 00:02:39 GMT+0100 (Central European Standard Time)] "GET /subtitles.srt" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/131.0.6778.87 Safari/537.36"
[Fri Dec 06 2024 00:02:39 GMT+0100 (Central European Standard Time)] "GET /rich.json" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/131.0.6778.87 Safari/537.36"
node_modules/.pnpm/react-dom@18.3.1_react@18.3.1/node_modules/react-dom/cjs/react-dom.production.min.js:187 Error: No "src" prop was passed to <img />.
at ImgRefForwarding (http://localhost:3000/bundle.js:26884:15)
at Nh (http://localhost:3000/bundle.js:25838:137)
at Yi (http://localhost:3000/bundle.js:25870:57)
at Vk (http://localhost:3000/bundle.js:25971:275)
at Uk (http://localhost:3000/bundle.js:25957:389)
at Tk (http://localhost:3000/bundle.js:25957:320)
at Ik (http://localhost:3000/bundle.js:25957:180)
at Nk (http://localhost:3000/bundle.js:25948:88)
at Gk (http://localhost:3000/bundle.js:25945:429)
at J (http://localhost:3000/bundle.js:34117:203)

An error occurred while rendering frame 0:
Error No "src" prop was passed to <img />.

at node_modules/.pnpm/remotion@4.0.171_react-dom@18.3.1_react@18.3.1__react@18.3.1/node_modules/remotion/dist/cjs/Img.js:19
17 │ const { delayPlayback } = (0, use_buffer_state_js_1.useBufferState)();
18 │ const sequenceContext = (0, react_1.useContext)(SequenceContext_js_1.SequenceContext);
19 │ if (!src) {
20 │ throw new Error('No "src" prop was passed to <img />.');
21 │ }
22 │ (0, react_1.useImperativeHandle)(ref, () =&gt; {

at Nh (node_modules/.pnpm/react-dom@18.3.1_react@18.3.1/node_modules/react-dom/cjs/react-dom.production.min.js:159)
at Yi (node_modules/.pnpm/react-dom@18.3.1_react@18.3.1/node_modules/react-dom/cjs/react-dom.production.min.js:191)
at Vk (node_modules/.pnpm/react-dom@18.3.1_react@18.3.1/node_modules/react-dom/cjs/react-dom.production.min.js:292)
at Uk (node_modules/.pnpm/react-dom@18.3.1_react@18.3.1/node_modules/react-dom/cjs/react-dom.production.min.js:278)
at Tk (node_modules/.pnpm/react-dom@18.3.1_react@18.3.1/node_modules/react-dom/cjs/react-dom.production.min.js:278)
at Ik (node_modules/.pnpm/react-dom@18.3.1_react@18.3.1/node_modules/react-dom/cjs/react-dom.production.min.js:278)
at Nk (node_modules/.pnpm/react-dom@18.3.1_react@18.3.1/node_modules/react-dom/cjs/react-dom.production.min.js:269)
at Gk (node_modules/.pnpm/react-dom@18.3.1_react@18.3.1/node_modules/react-dom/cjs/react-dom.production.min.js:266)
at J (node_modules/.pnpm/scheduler@0.23.2/node_modules/scheduler/cjs/scheduler.production.min.js:12)
at R (node_modules/.pnpm/scheduler@0.23.2/node_modules/scheduler/cjs/scheduler.production.min.js:13)
INFO:backend.utils.generate_video:Generated video to /var/folders/3j/jv5_hbgn59g9yxccxtfcvz4r0000gp/T/tmpmpc_r1vm/output.mp4
http-server stopped.
INFO:**main**:Generated video successfully.
^CKeyboard interruption in main thread... closing server.
  </pre>" </details> </li> <li><a href="https://github.com/feathers-studio/telegraf-docs">https://github.com/feathers-studio/telegraf-docs</a><br/> <details> <summary>"bot.js:</summary> <pre style="background-color: #f9f9f9; padding: 8px; border: 1px solid lightgray; font-family: monospace;">
  require("dotenv").config();
    
  const { Telegraf, session } = require("telegraf");
  const { registerCommands } = require("./commands");
  const { registerHears } = require("./hears");
  const { registerScenes } = require("./scenes");
  const sequelize = require("./database");
  const User = require("./models/user");
  const { t, i18next } = require("./i18n");
  const Plans = require("./models/plans");
    
  // sync database
  (async () =&gt; {
  try {
      await sequelize.sync({ force: true });
      console.log("database synchronized successfully.");
    
      const bot = new Telegraf(process.env.BOT_TOKEN);
    
      const defaultPlan = await Plans.findOne({ where: { isDefault: true } });
      if (!defaultPlan) {
      await Plans.create({
          name: "atom",
          isDefault: true,
          price: 0,
      });
      console.log("default plan created.");
      }
    
      // telegraf.js sessions
      bot.use(
      session({
          defaultSession: () =&gt; ({ counter: 0, userName: "" }), // Initialize session variables
      })
      );
      // user check/creation middleware
      bot.use(async (ctx, next) =&gt; {
      const telegramId = ctx.from.id;
      try {
          let user = await User.findOne({ where: { telegramId } });
          if (!user) {
          user = await User.create({
              name: ctx.from.first_name || "unknown",
              telegramId: telegramId,
          });
          console.log(
              `new user created: ${user.name} (id: ${user.telegramId})`
          );
          }
    
          ctx.session.isAuthorized = true;
          ctx.session.user = user;
          ctx.session.language = user.language || "ru";
    
          i18next.changeLanguage(ctx.session.language);
          return next();
      } catch (error) {
          console.error("error during user check/creation:", error);
          return ctx.reply(t("error_user_creation"));
      }
      });
    
      // register commands, hears, and scenes
      registerCommands(bot);
      registerHears(bot);
      registerScenes(bot);
    
      // start the bot
      bot.launch();
      console.log("bot is running.");
    
      // enable graceful stop
      process.once("SIGINT", () =&gt; bot.stop("SIGINT"));
      process.once("SIGTERM", () =&gt; bot.stop("SIGTERM"));
  } catch (error) {
      console.error("error initializing bot:", error);
  }
  })();
  </pre> don't do check in database at every message, but have a kind of cache in telegraf.js session" </details> </li> </ul> </li> </ul>]]></content><author><name>Yifan Song*</name></author><summary type="html"><![CDATA[A Live Benchmark for AI Software Engineers]]></summary></entry><entry><title type="html">Arena Explorer</title><link href="https://lmarena.github.io/blog/2025/arena-explorer/" rel="alternate" type="text/html" title="Arena Explorer"/><published>2025-02-11T00:00:00+00:00</published><updated>2025-02-11T00:00:00+00:00</updated><id>https://lmarena.github.io/blog/2025/arena-explorer</id><content type="html" xml:base="https://lmarena.github.io/blog/2025/arena-explorer/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Chatbot Arena receives vast amounts of LLM conversations daily. However, understanding what people ask, how they structure prompts, and how models perform isn’t straightforward. Raw text data is messy and complex. Analyzing small samples is feasible, but identifying trends in large datasets is challenging. This isn’t just our problem. Anyone dealing with unstructured data, e.g. long texts and images, faces the same question: how do you organize it to extract meaningful insights?</p> <p>To address this, we developed a topic modeling pipeline and the <strong>Arena Explorer</strong>. This pipeline organizes user prompts into distinct topics, structuring the text data hierarchically to enable intuitive analysis. We believe this tool for hierarchical topic modeling can be valuable to anyone analyzing complex text data.</p> <div class="l-page" style="display: flex; justify-content: center; align-items: center;"> <div style="position: relative; width: 100%; max-width: 1200px; height: 0; padding-bottom: 70%; margin-bottom: 20px"> <iframe src="https://storage.googleapis.com/public-arena-no-cors/index.html" frameborder="0" scrolling="yes" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; background-color: white;" allowfullscreen=""> </iframe> </div> </div> <p>In this blog post, we will cover:</p> <ul> <li>Analysis of LLM performance insights received from Arena Explorer.</li> <li>Details of how we created the Explorer, transforming a large dataset of user conversations into an exploratory tool.</li> <li>Ways to fine-tune and improve topic models.</li> </ul> <p>Check out the pipeline in this <a href="https://colab.research.google.com/drive/1chzqjePYnpq08fA3KzyKvSkuzCjojyiE?usp=sharing">colab notebook</a>, and try it out yourself.</p> <p>We also published the <a href="https://huggingface.co/datasets/lmarena-ai/arena-explorer-preference-100k">dataset</a> we used. This dataset contains 100k leaderboard conversation data, the largest prompt dataset with human preferences we have every released!</p> <h2 id="insights">Insights</h2> <p><strong>Model Performance Comparison</strong></p> <p>In our previous <a href="https://blog.lmarena.ai/blog/2024/arena-category/">blog post</a>, we conducted an in-depth categorical analysis and discussed key insights. That analysis was based on manually defined categories in Chatbot Arena. The results showed that language models perform differently across categories. With our topic modeling pipeline, we can now analyze model performance across categories more efficiently and dive deeper into specific topics.</p> <p>Compared to <em>Tech Programming</em>, model rankings for the other two largest broad categories, <em>Creative Writing</em> and <em>Puzzles &amp; Math</em>, shifted significantly.</p> <div> <iframe src="/assets/img/blog/explorer/rank_broad_cat.html" frameborder="0" scrolling="no" height="500px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 1. Tech Programming vs. Creative Writing vs. Puzzles Chatbot Arena ranking of the top 10 ranked models in Tech Programming.</p> <p>Claude performed better than Gemini in <em>Tech Programming</em>, while Gemini outperformed Claude in <em>Creative Writing</em>. Deepseek-coder-v2 dropped in ranking for <em>Creative Writing</em> compared to its position in <em>Tech Programming</em>.</p> <div> <iframe src="/assets/img/blog/explorer/rank_tech_vs_writing.html" frameborder="0" scrolling="no" height="800px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 2. Tech Programming vs. Creative Writing Chatbot Arena Score computed using the Bradley-Terry model.</p> <p><strong>Diving into Narrow Categories</strong></p> <p>Model performance analysis can be broken down into more specific categories based on win rates. We calculated the win rates of Gemini 1.5, GPT-4o, and Claude 3.5 across the narrow categories, treating ties as 0.5 wins. Gemini 1.5 performed best in <em>Entrepreneurship and Business Strategy</em> but had a noticeably lower win rate in <em>Songwriting and Playlist Creation</em>. In contrast, GPT-4o maintained a relatively consistent win rate across most categories, except for a dip in <em>Entrepreneurship and Business Strategy</em>. Claude 3.5 excelled in <em>Web Development</em> and <em>Linux &amp; Shell Scripting</em> but had lower win rates in other, less technical categories.</p> <div> <iframe src="/assets/img/blog/explorer/winrate_narrow.html" frameborder="0" scrolling="no" height="500px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 3. Model win rates in the eight largest narrow categories.</p> <p>Even within the same broad category, model performance varies slightly. For example, within <em>Tech Programming</em>, GPT-4o showed a lower win rate in <em>GPU and CPU Performance and Comparison</em> compared to other categories. Within <em>Creative Writing</em>, Gemini had a significantly higher win rate in <em>Genshin Impact Parody Adventures</em>.</p> <div> <iframe src="/assets/img/blog/explorer/winrate_tech.html" frameborder="0" scrolling="no" height="500px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 4. Model win rates in the eight largest narrow categories within Tech Programming.</p> <div> <iframe src="/assets/img/blog/explorer/winrate_writing.html" frameborder="0" scrolling="no" height="500px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 5. Model win rates in the eight largest narrow categories within Creative Writing.</p> <p>Note: Since models compete against different sets of opponents, win rates are only meaningful when compared within the same model. Therefore, we do not directly compare win rates across different models.</p> <h2 id="topic-modeling-pipeline">Topic Modeling Pipeline</h2> <p>We used the leaderboard conversation data between June 2024 and August 2024. To facilitate clustering in later steps, we selected prompts tagged in English and removed duplicate prompts. The final dataset contains around 52k prompts.</p> <p>To group the prompts into narrow categories, we used a topic modeling pipeline with <a href="https://maartengr.github.io/BERTopic/index.html">BERTopic</a>, similar to the one presented in our paper <a href="https://arxiv.org/abs/2403.04132">(Chiang, 2024)</a>. We performed the following steps.</p> <ol> <li>We create embeddings for user prompts with SentenceTransformers’ model (all-mpnet-base-v2), transforming prompts into representation vectors.</li> <li>To reduce the dimensionality of embeddings, we use UMAP (Uniform Manifold Approximation and Projection)</li> <li>We use the density distribution-based clustering algorithm HDBSCAN to identify topic clusters with a minimum clustering size of 20.</li> <li>We select 20 example prompts per cluster. They were chosen from the ones with high HDBSCAN probability scores (top 20% within their respective clusters). For clarity, we choose those with fewer than 100 words.</li> <li>To come up with cluster names, we feed the example prompts into ChatGPT-4o to give the category a name and description.</li> <li>We reduced all outliers using probabilities obtained from HDBSCAN and then embeddings of each outlier prompt. This pipeline groups the prompts into narrow categories, each with 20 example prompts.</li> </ol> <div> <iframe src="/assets/img/blog/explorer/intertopic_distance.html" frameborder="0" scrolling="no" height="700px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 6. The intertropical distance map shows the narrow clusters identified by BERTopic. The size of the circles is proportional to the amount of prompts in the cluster.</p> <p>We consolidate the initial narrow categories into broad categories for more efficient and intuitive exploration. We perform a second round of this topic modeling pipeline on the summarized category names and descriptions generated earlier. The steps are almost identical to before. Except for steps 4 and 5, we use all category names in a cluster for summarization instead of selecting examples.</p> <h3 id="tuning-topic-clusters">Tuning Topic Clusters</h3> <p>Topic clusters are not always accurate. Some prompts may not be placed in the most suitable cluster, and the same applies to specific categories. Many factors influence the final clustering:</p> <ol> <li>Embedding models used to generate vector representations for prompts</li> <li>Sampled example prompts used to assign cluster names</li> <li>BERTopic model parameters that affect the number of clusters, such as n_neighbors in UMAP and min_cluster_size in HDBSCAN</li> <li>Outlier reduction methods</li> </ol> <p><strong>How do we improve and fine-tune the clusters?</strong> Embedding models play a major role in clustering accuracy since they are used to train the clustering model. We compared two models on a 10k sample dataset: Sentence Transformer’s all-mpnet-base-v2 and OpenAI’s text-embedding-3-large, a more recent model. According to the <a href="https://huggingface.co/spaces/mteb/leaderboard">MTEB Leaderboard</a>, text-embedding-3-large performs better on average (57.77). The clustering results are noticeably different.</p> <p>With text-embedding-3-large, the broad category distribution is more balanced. In contrast, all-mpnet-base-v2 produced a large <em>Tech Programming</em> category. Zooming in on this category, we found that AI-related clusters were merged into <em>Tech Programming</em> when using all-mpnet-base-v2, whereas text-embedding-3-large formed a separate AI-related category. Choosing which result to use depends on human preference.</p> <div class="l-page" style="display: flex; justify-content: center; align-items: center;"> <iframe src="/assets/img/blog/explorer/embedding_mpnet_broad.html" frameborder="0" scrolling="no" height="700px" width="100%"></iframe> <iframe src="/assets/img/blog/explorer/embedding_mpnet_tech.html" frameborder="0" scrolling="no" height="700px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 7 &amp; 8. Broad categories and specific categories in “Tech Programming” summarized using all-mpnet-base-v2.</p> <div class="l-page" style="display: flex; justify-content: center; align-items: center;"> <iframe src="/assets/img/blog/explorer/embedding_openai_broad.html" frameborder="0" scrolling="no" height="700px" width="100%"></iframe> <iframe src="/assets/img/blog/explorer/embedding_openai_tech.html" frameborder="0" scrolling="no" height="700px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 9 &amp; 10. Broad categories and specific categories in “Tech Programming” summarized using text-embedding-3-large.</p> <p>Beyond embedding models, adjusting parameters and outlier reduction methods helps refine the clusters. For example, we increased the min_cluster_size parameter to adjust the broad clusters. Before, several broad categories had similar meanings. By increasing this parameter, we reduced the number of clusters, resulting in more distinctive categories.</p> <h2 id="whats-next">What’s next?</h2> <p>We will add more features to our explorer that provide insights into the connection between model performance and prompt category, such as model performance per category.</p> <p>We would love to hear your feedback and how you are using the pipeline to derive insights!</p> <h2 id="citation">Citation</h2> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">tang2025explorer</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{Arena Explorer: A Topic Modeling Pipeline for LLM Evals &amp; Analytics}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Kelly Tang and Wei-Lin Chiang and Anastasios N. Angelopoulos}</span>
    <span class="nv">year</span><span class="err">={2025</span><span class="p">}</span><span class="c">,</span>
<span class="c">}</span>

<span class="nc">@misc</span><span class="p">{</span><span class="nl">chiang2024chatbot</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Wei-Lin Chiang and Lianmin Zheng and Ying Sheng and Anastasios Nikolas Angelopoulos and Tianle Li and Dacheng Li and Hao Zhang and Banghua Zhu and Michael Jordan and Joseph E. Gonzalez and Ion Stoica}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
    <span class="na">eprint</span><span class="p">=</span><span class="s">{2403.04132}</span><span class="p">,</span>
    <span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
    <span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.AI}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>Kelly Tang</name></author><summary type="html"><![CDATA[A topic modeling pipeline for LLM evals & analytics]]></summary></entry></feed>