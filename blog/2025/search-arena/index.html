<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Introducing the Search Arena: Evaluating Search-Enabled AI | LM Arena </title> <meta name="author" content="LM Arena"> <meta name="description" content="an open platform for human preference evaluations"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/vicuna.jpeg?101e2cca9da6907c55807adf8b3b38b7"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lmarena.github.io/blog/2025/search-arena/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Introducing the Search Arena: Evaluating Search-Enabled AI",
            "description": "",
            "published": "April 14, 2025",
            "authors": [
              
              {
                "author": "Mihran Miroyan*",
                "authorURL": "https://mmiroyan.github.io",
                "affiliations": [
                  {
                    "name": "UC Berkeley",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Tsung-Han Wu*",
                "authorURL": "https://tsunghan-wu.github.io",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Logan King",
                "authorURL": "https://www.linkedin.com/in/logan-king-8a4267281",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Tianle Li",
                "authorURL": "https://codingwithtim.github.io",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Anastasios N. Angelopoulos",
                "authorURL": "http://angelopoulos.ai",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Wei-Lin Chiang",
                "authorURL": "https://infwinston.github.io",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Narges Norouzi",
                "authorURL": "https://nargesnorouzi.me/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Joseph E. Gonzalez",
                "authorURL": "https://people.eecs.berkeley.edu/~jegonzal",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title" href="/"> <span class="font-weight-bold">LM</span> Arena </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/about/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Introducing the Search Arena: Evaluating Search-Enabled AI</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <h2 id="tldr">TL;DR</h2> <ol> <li>We introduce <strong>Search Arena</strong>, a crowdsourced in-the-wild evaluation platform for search-augmented LLM systems based on human preference. Unlike LM-Arena or SimpleQA, our data focuses on current events and diverse real-world use cases (see <a href="#why-search-arena">Sec. 1</a>).</li> <li>Based on 7k human votes (03/18‚Äì04/13), <strong>Gemini-2.5-Pro-Grounding</strong> and <strong>Perplexity-Sonar-Reasoning-Pro</strong> are at the top, followed by the rest of Perplexity‚Äôs Sonar models, Gemini-2.0-Flash-Grounding, and OpenAI‚Äôs web search API models. Standardizing citation styles had minimal effect on rankings (see <a href="#leaderboard">Sec. 2</a>).</li> <li>Three features show strong positive correlation with human preference: response length, citation count, and citing specific web sources like YouTube and online forum/blogs (see <a href="#analyses">Sec. 3</a>).</li> <li>We open-sourced our dataset (<a href="https://huggingface.co/datasets/lmarena-ai/search-arena-v1-7k" rel="external nofollow noopener" target="_blank">ü§ó search-arena-7k</a>) and code (<a href="https://colab.research.google.com/drive/1h7rR7rhePBPuIfaWsVNlW87kv3DLibPS?usp=sharing" rel="external nofollow noopener" target="_blank">‚öôÔ∏è Colab notebook</a>) for leaderboard analysis. Try <a href="https://lmarena.ai/?search" rel="external nofollow noopener" target="_blank">üåê Search Arena</a> and see <a href="#futurework">Sec. 4</a> for what‚Äôs next.</li> </ol> <div id="fig1"> <iframe src="/assets/img/blog/search_arena/04142025/main_bootstrap_elo_rating.html" frameborder="0" scrolling="no" height="400px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 1. Search Arena leaderboard.</p> <h2 id="why-search-arena">1. Why Search Arena?</h2> <p>Web search is undergoing a major transformation. Search-augmented LLM systems integrate dynamic real-time web data with the reasoning, problem-solving, and question-answering capabilities of LLMs. These systems go beyond traditional retrieval, enabling richer human‚Äìweb interaction. The rise of models like Perplexity‚Äôs Sonar series, OpenAI‚Äôs GPT-Search, and Google‚Äôs Gemini-Grounding highlights the growing impact of search-augmented LLM systems.</p> <p>But how should these systems be evaluated? Static benchmarks like SimpleQA focus on factual accuracy on challenging questions, but that‚Äôs only one piece. These systems are used for diverse tasks‚Äîcoding, research, recommendations‚Äîso evaluations must also consider how they retrieve, process, and present information from the web. Understanding this requires studying how humans use and evaluate these systems in the wild.</p> <p>To this end, we developed search arena, aiming to (1) enable crowd-sourced evaluation of search-augmented LLMs and (2) release a diverse, in-the-wild dataset of user‚Äìsystem interactions.</p> <p>Since our <a href="https://x.com/lmarena_ai/status/1902036561119899983" rel="external nofollow noopener" target="_blank">initial launch</a> on March 18th, we‚Äôve collected over 11k votes across 10+ models. We then filtered this data to construct 7k battles with user votes (<a href="https://huggingface.co/datasets/lmarena-ai/search-arena-v1-7k" rel="external nofollow noopener" target="_blank">ü§ó search-arena-7k</a>) and calculated the leaderboard with this <a href="https://colab.research.google.com/drive/1h7rR7rhePBPuIfaWsVNlW87kv3DLibPS?usp=sharing" rel="external nofollow noopener" target="_blank">‚öôÔ∏è Colab notebook</a>. Below, we provide details on the collected data and the supported models.</p> <h3>A. Data</h3> <p><b>Data Filtering and Citation Style Control.</b> Each model provider uses a unique inline citation style, which can potentially compromise model anonymity. However, citation formatting impacts how information is presented to and processed by the user, impacting their final votes. To balance these considerations, we introduced <em>‚Äústyle randomization‚Äù</em>: responses are displayed either in a standardized format or in the original format (i.e., the citation style agreed upon with each model provider).</p> <details> <summary>Click to view standardized and original citation styles for each provider.</summary> <div style="margin-top: 1rem;"> <img src="/assets/img/blog/search_arena/04142025/gemini_formatting_example.png" alt="Google's Gemini citation formatting comparison" style="width: 100%; max-width: 1000px; border: 1px solid #ccc; border-radius: 8px;"> </div> <p>(1) Google's Gemini Formatting: standardized (left), original (right)</p> <div style="margin-top: 1rem;"> <img src="/assets/img/blog/search_arena/04142025/ppl_formatting_example.png" alt="Perplexity's Sonar citation formatting comparison" style="width: 100%; max-width: 1000px; border: 1px solid #ccc; border-radius: 8px;"> </div> <p>(2) Perplexity‚Äôs Formatting: standardized (left), original (right)</p> <div style="margin-top: 1rem;"> <img src="/assets/img/blog/search_arena/04142025/gpt_formatting_example.png" alt="OpenAI's GPT citation formatting comparison" style="width: 100%; max-width: 1000px; border: 1px solid #ccc; border-radius: 8px;"> </div> <p>(3) OpenAI's Formatting: standardized (left), original (right)</p> </details> <p>This approach mitigates de-anonymization while allowing us to analyze how citation style impacts user votes (see the citation analyses subsection <a href="#citation_analyses">here</a>). After updating and standardizing citation styles in collaboration with providers, we filtered the dataset to include only battles with the updated styles, resulting in ~7,000 clean samples for leaderboard calculation and further analysis.</p> <p><b>Comparison to Existing Benchmarks.</b> To highlight what makes Search Arena unique, we compare our collected data to <a href="https://arxiv.org/abs/2403.04132" rel="external nofollow noopener" target="_blank">LM-Arena</a> and <a href="https://arxiv.org/abs/2411.04368" rel="external nofollow noopener" target="_blank">SimpleQA</a>. As shown in <a href="#fig2">Fig. 2</a>, Search Arena prompts focus more on current events, while LM-Arena emphasizes coding/writing, and SimpleQA targets narrow factual questions (e.g., dates, names, specific domains). <a href="#tab1">Tab. 1</a> shows that Search Arena features longer prompts, longer responses, more turns, and more languages compared to SimpleQA‚Äîcloser to natural user interactions seen in LM-Arena.</p> <div id="fig2"> <iframe src="/assets/img/blog/search_arena/04142025/topical_distribution_plot.html" frameborder="0" scrolling="no" height="450px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 2. Top-5 topic distributions across Search Arena, LM Arena, and SimpleQA. We use <a href="https://blog.lmarena.ai/blog/2025/arena-explorer/" rel="external nofollow noopener" target="_blank">Arena Explorer (Tang et al., 2025)</a> to extract topic clusters from the three datasets.</p> <table id="tab1" style="margin: 0 auto; border-collapse: collapse; text-align: center;"> <thead> <tr> <th style="padding: 8px; border-bottom: 1px solid #ccc;"></th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Search Arena</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">LM Arena</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">SimpleQA</th> </tr> </thead> <tbody> <tr> <td style="padding: 8px;">Languages</td> <td style="padding: 8px;">10+ (EN, RU, CN, ‚Ä¶)</td> <td style="padding: 8px;">10+ (EN, RU, CN, ‚Ä¶)</td> <td style="padding: 8px;">English Only</td> </tr> <tr> <td style="padding: 8px;">Avg. Prompt Length (#words)</td> <td style="padding: 8px;">88.08</td> <td style="padding: 8px;">102.12</td> <td style="padding: 8px;">16.32</td> </tr> <tr> <td style="padding: 8px;">Avg. Response Length (#words)</td> <td style="padding: 8px;">344.10</td> <td style="padding: 8px;">290.87</td> <td style="padding: 8px;">2.24</td> </tr> <tr> <td style="padding: 8px;">Avg. #Conversation Turns</td> <td style="padding: 8px;">1.46</td> <td style="padding: 8px;">1.37</td> <td style="padding: 8px;">N/A</td> </tr> </tbody> </table> <p style="color:gray; text-align: center;"> Table 1. Prompt language distribution, average prompt length, average response length, and average number of turns in Search Arena, LM Arena, and SimpleQA datasets.</p> <h3>B. Models</h3> <p>Search Arena currently supports 11 models from three providers: Perplexity, Gemini, and OpenAI. Unless specified otherwise, we treat the same model with different citation styles (original vs. standardized) as a single model. <a href="#fig3">Fig. 3</a> shows the number of battles collected per model used in this iteration of the leaderboard.</p> <p>By default, we use each provider‚Äôs standard API settings. For Perplexity and OpenAI, this includes setting the <code class="language-plaintext highlighter-rouge">search_context_size</code> parameter to <code class="language-plaintext highlighter-rouge">medium</code>, which controls how much web content is retrieved and passed to the model. We also explore specific features by changing the default settings: (1) For OpenAI, we test their geolocation feature in one model variant by passing a country code extracted from the user‚Äôs IP address. (2) For Perplexity and OpenAI, we include variants with <code class="language-plaintext highlighter-rouge">search_context_size</code> set to <code class="language-plaintext highlighter-rouge">high</code>. Below is the list of models currently supported in Search Arena:</p> <table style="width:100%; border-collapse: collapse; text-align: left;"> <thead> <tr> <th style="border-bottom: 1px solid #ccc; padding: 8px;">Provider</th> <th style="border-bottom: 1px solid #ccc; padding: 8px;">Model</th> <th style="border-bottom: 1px solid #ccc; padding: 8px;">Base model</th> <th style="border-bottom: 1px solid #ccc; padding: 8px;">Details</th> </tr> </thead> <tbody> <tr> <td rowspan="5" style="padding: 8px;">Perplexity</td> <td style="padding: 8px;"><code>ppl-sonar</code></td> <td style="padding: 8px;"><a href="https://docs.perplexity.ai/guides/models/sonar" target="_blank" rel="external nofollow noopener">sonar</a></td> <td style="padding: 8px;">Default config</td> </tr> <tr> <td style="padding: 8px;"><code>ppl-sonar-pro</code></td> <td style="padding: 8px;"><a href="https://docs.perplexity.ai/guides/models/sonar-pro" target="_blank" rel="external nofollow noopener">sonar-pro</a></td> <td style="padding: 8px;">Default config</td> </tr> <tr> <td style="padding: 8px;"><code>ppl-sonar-pro-high</code></td> <td style="padding: 8px;"><a href="https://docs.perplexity.ai/guides/models/sonar-pro" target="_blank" rel="external nofollow noopener">sonar-pro</a></td> <td style="padding: 8px;"> <code>search_context_size</code> set to <code>high</code> </td> </tr> <tr> <td style="padding: 8px;"><code>ppl-sonar-reasoning</code></td> <td style="padding: 8px;"><a href="https://docs.perplexity.ai/guides/models/sonar-reasoning" target="_blank" rel="external nofollow noopener">sonar-reasoning</a></td> <td style="padding: 8px;">Default config</td> </tr> <tr> <td style="padding: 8px;"><code>ppl-sonar-reasoning-pro-high</code></td> <td style="padding: 8px;"><a href="https://docs.perplexity.ai/guides/models/sonar-reasoning-pro" target="_blank" rel="external nofollow noopener">sonar-reasoning-pro</a></td> <td style="padding: 8px;"> <code>search_context_size</code> set to <code>high</code> </td> </tr> <tr> <td rowspan="2" style="padding: 8px;">Gemini</td> <td style="padding: 8px;"><code>gemini-2.0-flash-grounding</code></td> <td style="padding: 8px;"><a href="https://ai.google.dev/gemini-api/docs/models#gemini-2.0-flash" target="_blank" rel="external nofollow noopener">gemini-2.0-flash</a></td> <td style="padding: 8px;">With <code>Google Search</code> tool enabled</td> </tr> <tr> <td style="padding: 8px;"><code>gemini-2.5-pro-grounding</code></td> <td style="padding: 8px;"><a href="https://ai.google.dev/gemini-api/docs/models#gemini-2.5-pro-preview-03-25" target="_blank" rel="external nofollow noopener">gemini-2.5-pro-exp-03-25</a></td> <td style="padding: 8px;">With <code>Google Search</code> tool enabled</td> </tr> <tr> <td rowspan="4" style="padding: 8px;">OpenAI<sup>‚Ä†</sup> </td> <td style="padding: 8px;"><code>api-gpt-4o-mini-search-preview</code></td> <td style="padding: 8px;"><a href="https://platform.openai.com/docs/models/gpt-4o-mini-search-preview" target="_blank" rel="external nofollow noopener">gpt-4o-mini-search-preview</a></td> <td style="padding: 8px;">Default config</td> </tr> <tr> <td style="padding: 8px;"><code>api-gpt-4o-search-preview</code></td> <td style="padding: 8px;"><a href="https://platform.openai.com/docs/models/gpt-4o-search-preview" target="_blank" rel="external nofollow noopener">gpt-4o-search-preview</a></td> <td style="padding: 8px;">Default config</td> </tr> <tr> <td style="padding: 8px;"><code>api-gpt-4o-search-preview-high</code></td> <td style="padding: 8px;"><a href="https://platform.openai.com/docs/models/gpt-4o-search-preview" target="_blank" rel="external nofollow noopener">gpt-4o-search-preview</a></td> <td style="padding: 8px;"> <code>search_context_size</code> set to <code>high</code> </td> </tr> <tr> <td style="padding: 8px;"><code>api-gpt-4o-search-preview-high-loc</code></td> <td style="padding: 8px;"><a href="https://platform.openai.com/docs/models/gpt-4o-search-preview" target="_blank" rel="external nofollow noopener">gpt-4o-search-preview</a></td> <td style="padding: 8px;"> <code>user_location</code> feature enabled</td> </tr> </tbody> </table> <p style="color:gray; font-size: 14px; text-align: center; margin-top: 8px;"> Table 2. Models currently supported in Search Arena. </p> <p style="color:gray; font-size: 13px; max-width: 700px; margin: 0 auto;"> <sup>‚Ä†</sup>We evaluate OpenAI‚Äôs <a href="https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat" target="_blank" rel="external nofollow noopener">web search API</a>, which is different from the search feature in the ChatGPT product. </p> <div id="fig3"> <iframe src="/assets/img/blog/search_arena/04142025/main_battle_count.html" frameborder="0" scrolling="no" height="400px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 3. Battle counts across 11 models. The distribution is not even as (1) we released models into the arena in batches and (2) filtered votes (described above).</p> <h2 id="leaderboard">2. Leaderboard</h2> <p>We begin by analyzing pairwise win rates‚Äîi.e., the proportion of wins of model A over model B in head-to-head battles. This provides a direct view of model performance differences without aggregating scores. The results are shown in <a href="#fig4">Fig. 4</a>, along with the following observations:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">gemini-2.5-pro-grounding</code> and <code class="language-plaintext highlighter-rouge">ppl-sonar-reasoning-pro-high</code> outperform all other models by a large margin. In direct head-to-head battles <code class="language-plaintext highlighter-rouge">ppl-sonar-reasoning-pro-high</code> has a slight advantage (53% win rate).</li> <li> <code>ppl-sonar-reasoning</code> outperforms the rest of Perplexity‚Äôs models. There‚Äôs no clear difference between <code>ppl-sonar-pro</code> and <code>ppl-sonar-pro-high</code> (52%/48% win rate), and even <code>ppl-sonar</code> beats <code>ppl-sonar-pro-high</code> (60% win rate). This suggests that increasing search context does not necessarily improve performance and may even degrade it.</li> <li>Within OpenAI‚Äôs models, larger search context does not significantly improve performance (<code class="language-plaintext highlighter-rouge">api-gpt-4o-search</code> vs <code class="language-plaintext highlighter-rouge">api-gpt-4o-search-high</code>). While adding user location improves performance in head-to-head battles (58% win rate of <code class="language-plaintext highlighter-rouge">api-gpt-4o-search-high-loc</code> over <code class="language-plaintext highlighter-rouge">api-gpt-4o-search-high</code>), location-enabled version ranks lower in the leaderboard.</li> </ul> <div id="fig4"> <iframe src="/assets/img/blog/search_arena/04142025/main_pairwise_average_win_rate.html" frameborder="0" scrolling="no" height="500px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 4. Pairwise win rates (Model A wins Model B), excluding <code>tie</code> and <code>tie (bothbad)</code> votes.</p> <p>Now we build the leaderboard! Consistent with <a href="https://lmarena.ai/" rel="external nofollow noopener" target="_blank">LM Arena</a>, we apply the Bradley-Terry (BT) model to compute model scores. The resulting BT coefficients are then translated to Elo scale, with the final model scores and rankings displayed in <a href="#fig1">Fig. 1</a> and <a href="#tab3">Tab. 3</a>. The confidence intervals are still wide, which means the leaderboard hasn‚Äôt fully settled and there‚Äôs still some uncertainty. But clear performance trends are already starting to emerge. Consistent with the pairwise win rate analysis in the previous section, <code class="language-plaintext highlighter-rouge">gemini-2.5-pro-grounding</code> and <code class="language-plaintext highlighter-rouge">ppl-sonar-reasoning-pro-high</code> top the leaderboard by a substantial margin. They are followed by models from the <code class="language-plaintext highlighter-rouge">ppl-sonar</code> family, with <code class="language-plaintext highlighter-rouge">ppl-sonar-reasoning</code> leading the group. Then comes <code class="language-plaintext highlighter-rouge">gemini-2.0-flash-grounding</code>, and finally OpenAI models with <code class="language-plaintext highlighter-rouge">api-gpt-4o-search</code> based models outperforming <code class="language-plaintext highlighter-rouge">api-gpt-4o-mini-search</code>. Generally, users prefer responses from reasoning models (top 3 on the leaderboard).</p> <table id="tab2" style="width: 100%; border-collapse: collapse; text-align: center;"> <thead> <tr> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Rank</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Model</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Arena Score</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">95% CI</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Votes</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Organization</th> </tr> </thead> <tbody> <tr> <td>1</td> <td><code>gemini-2.5-pro-grounding</code></td> <td>1142</td> <td>+14/-17</td> <td>1,215</td> <td>Google</td> </tr> <tr> <td>1</td> <td><code>ppl-sonar-reasoning-pro-high</code></td> <td>1136</td> <td>+21/-19</td> <td>861</td> <td>Perplexity</td> </tr> <tr> <td>3</td> <td><code>ppl-sonar-reasoning</code></td> <td>1097</td> <td>+11/-17</td> <td>1,644</td> <td>Perplexity</td> </tr> <tr> <td>3</td> <td><code>ppl-sonar</code></td> <td>1072</td> <td>+15/-17</td> <td>1,208</td> <td>Perplexity</td> </tr> <tr> <td>3</td> <td><code>ppl-sonar-pro-high</code></td> <td>1071</td> <td>+15/-10</td> <td>1,364</td> <td>Perplexity</td> </tr> <tr> <td>4</td> <td><code>ppl-sonar-pro</code></td> <td>1066</td> <td>+12/-13</td> <td>1,214</td> <td>Perplexity</td> </tr> <tr> <td>7</td> <td><code>gemini-2.0-flash-grounding</code></td> <td>1028</td> <td>+16/-16</td> <td>1,193</td> <td>Google</td> </tr> <tr> <td>7</td> <td><code>api-gpt-4o-search</code></td> <td>1000</td> <td>+13/-19</td> <td>1,196</td> <td>OpenAI</td> </tr> <tr> <td>7</td> <td><code>api-gpt-4o-search-high</code></td> <td>999</td> <td>+13/-14</td> <td>1,707</td> <td>OpenAI</td> </tr> <tr> <td>8</td> <td><code>api-gpt-4o-search-high-loc</code></td> <td>994</td> <td>+14/-14</td> <td>1,226</td> <td>OpenAI</td> </tr> <tr> <td>11</td> <td><code>api-gpt-4o-mini-search</code></td> <td>961</td> <td>+16/-15</td> <td>1,172</td> <td>OpenAI</td> </tr> </tbody> </table> <p style="color:gray; text-align: center;"> Table 3. Search Arena leaderboard. </p> <h3 id="citation_analyses">Citation Style Analysis</h3> <p>Having calculated the main leaderboard, we can now analyze the effect of citation style on user votes and model rankings. For each battle, we record model A‚Äôs and B‚Äôs citation style ‚Äî original (agreed upon with the providers) vs standardized.</p> <p>First, following the method in <a href="https://blog.lmarena.ai/blog/2024/style-control/" rel="external nofollow noopener" target="_blank">(Li et al., 2024)</a>, we apply style control and use the citation style indicator variable (1 if standardized, 0 otherwise) as an additional feature in the BT model. The resulting model scores and rankings do not change significantly from <a href="#fig1">the main leaderboard</a>. Although the leaderboard does not change, the corresponding coefficient is positive (0.044) and statistically significant (p&lt;0.05), implying that standardization of citation style has a positive impact on model score.</p> <p>We further investigate the effect of citation style on model performance, by treating each combination of model and citation style as a distinct model (e.g., <code class="language-plaintext highlighter-rouge">api-gpt-4o-search</code> with original style will be different from <code class="language-plaintext highlighter-rouge">api-gpt-4o-search</code> with standardized citation style). <a href="#fig5">Fig. 5</a> shows the change in the arena score between the two styles of each model. Overall, we observe increase or no change in score with standardized citations across all models except <code class="language-plaintext highlighter-rouge">gemini-2.0-flash</code>. However, the differences remain within the confidence intervals (CI), and we will continue collecting data to assess whether the trend converges toward statistical significance.</p> <div id="fig5"> <iframe src="/assets/img/blog/search_arena/04142025/og_vs_st.html" frameborder="0" scrolling="no" height="400px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 5. Change in arena score for original vs standardized citation style for each model.</p> <h2 id="analyses">3. Three Secrets Behind a WIN</h2> <p>After reviewing the leaderboard‚Äîand showing that the citation style doesn‚Äôt impact results all that much‚Äîyou might be wondering: <em>What features contribute to the model‚Äôs win rate?</em></p> <p>To answer this, we used the framework in¬†<a href="https://arxiv.org/abs/2201.12323" rel="external nofollow noopener" target="_blank">(Zhong et al., 2022)</a>, a method that automatically proposes and tests hypotheses to identify key differences between two groups of natural language texts‚Äîin this case, human-preferred and rejected model outputs. In our implementation, we asked the model to generate 25 hypotheses and evaluate them, leading to the discovery of <em>three distinguishing factors</em> with statistically significant p-values, shown in <a href="#tab4">Tab. 4</a>.</p> <table id="tab4" style="width: 100%; border-collapse: collapse; text-align: left;"> <thead> <tr> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Feature</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">p-value</th> </tr> </thead> <tbody> <tr> <td style="padding: 8px;">References to specific known entities or platforms</td> <td style="padding: 8px;">0.0000114</td> </tr> <tr> <td style="padding: 8px;">Frequent use of external citations and hyperlinks</td> <td style="padding: 8px;">0.01036</td> </tr> <tr> <td style="padding: 8px;">Longer, more in-depth answers</td> <td style="padding: 8px;">0.04761</td> </tr> </tbody> </table> <p style="color:gray; text-align: center;"> Table 4. Candidate key factors between the winning and losing model outputs. </p> <h3 id="model-characteristics">Model Characteristics</h3> <p>Guided by the above findings, we analyze how these features vary across models and model families.</p> <p><a href="#fig6">Fig. 6 (left)</a> shows the distribution of average response length across models. Gemini models are generally the most verbose‚Äî<code>gemini-2.5-pro-grounding</code>, in particular, produces responses nearly twice as long as most Perplexity or OpenAI models. Within the Perplexity and OpenAI families, response length is relatively consistent, with the exception of <code class="language-plaintext highlighter-rouge">ppl-sonar-reasoning-pro-high</code>. <a href="#fig6">Fig. 6 (right)</a> shows the average number of citations per response. Sonar models cite the most, with <code>ppl-sonar-pro-high</code> citing 2-3x more than Gemini models. OpenAI models cite the fewest sources (2-2.5) with little variation within the group.</p> <div id="fig6"> <iframe src="/assets/img/blog/search_arena/04142025/lenght_cit_features.html" frameborder="0" scrolling="no" height="400px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 6. Average response length (left) and number of citations (right) per model.</p> <p>In addition to number of citations and response length, we also study the common <em>source domains</em> cited by each model. We categorize retrieved URLs into ten types: YouTube, News (U.S. and foreign), Community &amp; Blogs (e.g., Reddit, Medium), Wikipedia, Tech &amp; Coding (e.g., Stack Overflow, GitHub), Government &amp; Education, Social Media, Maps, and Academic Journals. <a href="#fig7">Fig. 7</a> shows the domain distribution across providers in two settings: (1) all conversations, and (2) a filtered subset focused on Trump-related prompts. The case study helps examine how models behave when responding to queries on current events. Here are three interesting findings:</p> <ol> <li>All models favor authoritative sources (e.g., Wikipedia, <code>.edu</code>, <code>.gov</code> domains).</li> <li>OpenAI models heavily cite news sources‚Äî51.3% overall and 87.3% for Trump-related prompts.</li> <li>Gemini prefers community/blog content, whereas Perplexity frequently cites YouTube. Perplexity also strongly favors U.S. news sources over foreign ones (3x more often).</li> </ol> <div id="fig7"> <iframe src="/assets/img/blog/search_arena/04142025/domain_citations.html" frameborder="0" scrolling="no" height="400px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 7. Distribution of cited domain categories across models. Use the dropdown to switch between all prompts and a filtered Trump-related subset.</p> <h3 id="control-experiments">Control Experiments</h3> <p>After analyzing model characteristics such as response length, citation count, and citation sources, we revisited the Bradley-Terry model with these features as additional control variables <a href="https://blog.lmarena.ai/blog/2024/style-control/" rel="external nofollow noopener" target="_blank">(Li et al., 2024)</a>. Below are some findings when controlling for different subsets of control features:</p> <ul> <li> <strong>Response length</strong>: Controlling for response length yields a positive and statistically significant coefficient (0.255, <em>p</em> &lt; 0.05), indicating that users prefer more verbose responses.</li> <li> <strong>Number of citations</strong>: Controlling for citation count also results in a positive and significant coefficient (0.234, <em>p</em> &lt; 0.05), suggesting a preference for responses with more cited sources.</li> <li> <strong>Citation source categories</strong>: As shown in <a href="#fig8">Fig. 8</a>, citations from <strong>community platforms</strong> (e.g., Reddit, Quora) and <strong>YouTube</strong> have statistically significant positive effects on user votes. The remaining categories have insignificant coefficients.</li> <li> <strong>Joint controls</strong>: When controlling for all features, only <strong>response length</strong> and <strong>citation count</strong> remain statistically significant.</li> </ul> <div id="fig8" style="display: flex; justify-content: center;"> <iframe src="/assets/img/blog/search_arena/04142025/domain_citations_style_control_bootstrap_style_coefs.html" frameborder="0" scrolling="no" height="400px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 8. Estimates (with 95% CIs) of style coefficients.</p> <p>Finally, we used all previously described features to construct a controlled leaderboard. <a href="#fig9">Fig. 9</a> compares the original and adjusted arena scores after controlling for response length, citation count, and cited sources. Interestingly, when using all these features as control variables, the top six models all show a reduction in score, while the remaining models are largely unaffected. This narrows the gap between <code>gemini-2.0-flash-grounding</code> and non-reasoning Perplexity models. <a href="#tab5">Tab. 5</a> shows model rankings when controlling for different subsets of these features:</p> <ul> <li>Controlling for response length, <code class="language-plaintext highlighter-rouge">ppl-sonar-reasoning</code> shares the first rank with <code class="language-plaintext highlighter-rouge">gemini-2.5-pro-grounding</code> and <code class="language-plaintext highlighter-rouge">ppl-sonar-reasoning-pro-high</code>. The difference between (1) <code class="language-plaintext highlighter-rouge">sonar-pro</code> and other non-reasoning sonar models as well (2) <code class="language-plaintext highlighter-rouge">api-gpt-4o-search-high</code> and <code class="language-plaintext highlighter-rouge">api-gpt-4o-search-high-loc</code>, disappear.</li> <li>When controlling for the number of citations, model rankings converge (i.e., multiple models share the same rank), suggesting that the number of citations is a significant factor impacting differences across models and the resulting rankings.</li> <li>Controlling for cited domains has minimal effect on model rankings.</li> </ul> <div id="fig9"> <iframe src="/assets/img/blog/search_arena/04142025/style_control.html" frameborder="0" scrolling="no" height="400px" width="100%"></iframe> </div> <p style="color:gray; text-align: center;">Figure 9. Arena scores before and after a controlled setting.</p> <table id="tab5" style="width: 100%; border-collapse: collapse; text-align: center;"> <thead> <tr> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Model</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Rank Diff (Length)</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Rank Diff (# Citations)</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Rank Diff (Domain Sources)</th> <th style="padding: 8px; border-bottom: 1px solid #ccc;">Rank Diff (All)</th> </tr> </thead> <tbody> <tr> <td><code>gemini-2.5-pro-grounding</code></td> <td>1‚Üí1</td> <td>1‚Üí1</td> <td>1‚Üí1</td> <td>1‚Üí1</td> </tr> <tr> <td><code>ppl-sonar-reasoning-pro-high</code></td> <td>1‚Üí1</td> <td>1‚Üí1</td> <td>1‚Üí1</td> <td>1‚Üí1</td> </tr> <tr> <td><code>ppl-sonar-reasoning</code></td> <td>3‚Üí1</td> <td>3‚Üí3</td> <td>3‚Üí3</td> <td>3‚Üí2</td> </tr> <tr> <td><code>ppl-sonar</code></td> <td>3‚Üí3</td> <td>3‚Üí3</td> <td>3‚Üí3</td> <td>3‚Üí3</td> </tr> <tr> <td><code>ppl-sonar-pro-high</code></td> <td>3‚Üí3</td> <td>3‚Üí4</td> <td>3‚Üí4</td> <td>3‚Üí3</td> </tr> <tr> <td><code>ppl-sonar-pro</code></td> <td>4‚Üí3</td> <td>4‚Üí4</td> <td>4‚Üí4</td> <td>4‚Üí3</td> </tr> <tr> <td><code>gemini-2.0-flash-grounding</code></td> <td>7‚Üí7</td> <td>7‚Üí4</td> <td>7‚Üí5</td> <td>7‚Üí4</td> </tr> <tr> <td><code>api-gpt-4o-search</code></td> <td>7‚Üí7</td> <td>7‚Üí4</td> <td>7‚Üí7</td> <td>7‚Üí6</td> </tr> <tr> <td><code>api-gpt-4o-search-high</code></td> <td>7‚Üí8</td> <td>7‚Üí4</td> <td>7‚Üí7</td> <td>7‚Üí7</td> </tr> <tr> <td><code>api-gpt-4o-search-high-loc</code></td> <td>8‚Üí8</td> <td>8‚Üí5</td> <td>8‚Üí7</td> <td>8‚Üí7</td> </tr> <tr> <td><code>api-gpt-4o-mini-search</code></td> <td>11‚Üí11</td> <td>11‚Üí11</td> <td>11‚Üí11</td> <td>11‚Üí11</td> </tr> </tbody> </table> <p style="color:gray; text-align: center;"> Table 5. Model rankings change when controlling for different subsets of features. </p> <h2 id="futurework">4. Conclusion &amp; What‚Äôs Next</h2> <p>As search-augmented LLMs become increasingly popular, <strong>Search Arena</strong> provides a real-time, in-the-wild evaluation platform driven by crowdsourced human feedback. Unlike static QA benchmarks, our dataset emphasizes current events and diverse real-world queries, offering a more realistic view of how users interact with these systems. Using 7k human votes, we found that <strong>Gemini-2.5-Pro-Grounding</strong> and <strong>Perplexity-Sonar-Reasoning-Pro-High</strong> share the first rank in the leaderboard. User preferences are positively correlated with <strong>response length</strong>, <strong>number of citations</strong>, and <strong>citation sources</strong>. Citation formatting, surprisingly, had minimal impact.</p> <p>We have open-sourced our data (<a href="https://huggingface.co/datasets/lmarena-ai/search-arena-v1-7k" rel="external nofollow noopener" target="_blank">ü§ó search-arena-7k</a>) and analysis code (<a href="https://colab.research.google.com/drive/1h7rR7rhePBPuIfaWsVNlW87kv3DLibPS?usp=sharing" rel="external nofollow noopener" target="_blank">‚öôÔ∏è Colab notebook</a>). Try <a href="https://lmarena.ai/?search" rel="external nofollow noopener" target="_blank">üåê Search Arena</a> now and see what‚Äôs next:</p> <ul> <li> <strong>Open participation</strong>: We are inviting model submissions from researchers and industry, and encouraging public voting.</li> <li> <strong>Cross-task evaluation</strong>: How well do search models handle general questions? Can LLMs manage search-intensive tasks?</li> <li> <strong>Raise the bar for open models:</strong> Can simple wrappers with search engine/scraping + tools like ReAct and function calling make open models competitive?</li> </ul> <h2 id="citation">Citation</h2> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">searcharena2025</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">{Introducing the Search Arena: Evaluating Search-Enabled AI}</span><span class="p">,</span>
    <span class="na">url</span> <span class="p">=</span> <span class="s">{https://blog.lmarena.ai/blog/2025/search-arena/}</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">{Mihran Miroyan*, Tsung-Han Wu*, Logan Kenneth King, Tianle Li, Anastasios N. Angelopoulos, Wei-Lin Chiang, Narges Norouzi, Joseph E. Gonzalez}</span><span class="p">,</span>
    <span class="na">month</span> <span class="p">=</span> <span class="s">{April}</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span>

<span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chiang2024chatbot</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Chatbot arena: An open platform for evaluating llms by human preference}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhu, Banghua and Zhang, Hao and Jordan, Michael and Gonzalez, Joseph E and others}</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Forty-first International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="p">}</span>
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"lmarena/lmarena.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 LM Arena. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-",title:"",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-about",title:"about",description:"",section:"Navigation",handler:()=>{window.location.href="/about/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-our-response-to-39-the-leaderboard-illusion-39-writeup",title:"Our Response to &#39;The Leaderboard Illusion&#39; Writeup",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/our-response/"}},{id:"post-celebrating-community-impact-3m-votes-400-models-and-300-pre-release-tests",title:"Celebrating Community Impact: 3M+ votes, 400+ models, and 300+ pre-release tests",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/two-year-celebration/"}},{id:"post-does-sentiment-matter-too",title:"Does Sentiment Matter Too?",description:"Introducing Sentiment Control: Disentagling Sentiment and Substance",section:"Posts",handler:()=>{window.location.href="/blog/2025/sentiment-control/"}},{id:"post-how-many-user-prompts-are-new",title:"How Many User Prompts are New?",description:"Analysis of prompt freshness and benchmark contamination",section:"Posts",handler:()=>{window.location.href="/blog/2025/freshness/"}},{id:"post-lmarena-is-growing-to-support-our-community-platform",title:"LMArena is Growing to Support our Community Platform",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/new-beta/"}},{id:"post-introducing-the-search-arena-evaluating-search-enabled-ai",title:"Introducing the Search Arena: Evaluating Search-Enabled AI",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/search-arena/"}},{id:"post-lmarena-community-updates-looking-ahead",title:"LMArena Community Updates: Looking Ahead",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/new-discord-server/"}},{id:"post-webdev-arena",title:"WebDev Arena",description:"A Live LLM Leaderboard for Web App Development",section:"Posts",handler:()=>{window.location.href="/blog/2025/webdev-arena/"}},{id:"post-repochat-arena",title:"RepoChat Arena",description:"A Live Benchmark for AI Software Engineers",section:"Posts",handler:()=>{window.location.href="/blog/2025/repochat-arena/"}},{id:"post-arena-explorer",title:"Arena Explorer",description:"A topic modeling pipeline for LLM evals &amp; analytics",section:"Posts",handler:()=>{window.location.href="/blog/2025/arena-explorer/"}},{id:"post-code-editing-in-copilot-arena",title:"Code Editing in Copilot Arena",description:"Copilot Arena&#39;s Code Editing Leaderboard and Insights",section:"Posts",handler:()=>{window.location.href="/blog/2025/copilot-arena-edits/"}},{id:"post-copilot-arena",title:"Copilot Arena",description:"Copilot Arena&#39;s Initial Leaderboard, Insights, and a New Prompting Method for Code Completions",section:"Posts",handler:()=>{window.location.href="/blog/2024/copilot-arena/"}},{id:"post-chatbot-arena-categories",title:"Chatbot Arena Categories",description:"Definitions, Methods, and Insights",section:"Posts",handler:()=>{window.location.href="/blog/2024/arena-category/"}},{id:"post-preference-proxy-evaluations",title:"Preference Proxy Evaluations",description:"A New Benchmark for Evaluating Reward Models and LLM Judges",section:"Posts",handler:()=>{window.location.href="/blog/2024/preference-proxy-evaluations/"}},{id:"post-agent-arena",title:"Agent Arena",description:"A Platform for Evaluating and Comparing LLM Agents Across Models, Tools, and Frameworks",section:"Posts",handler:()=>{window.location.href="/blog/2024/agent-arena/"}},{id:"post-statistical-extensions-of-the-bradley-terry-and-elo-models",title:"Statistical Extensions of the Bradley-Terry and Elo Models",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/extended-arena/"}},{id:"post-chatbot-arena-new-blog",title:"Chatbot Arena New Blog",description:"A new chapter for Chatbot Arena!",section:"Posts",handler:()=>{window.location.href="/blog/2024/new-site/"}},{id:"post-redteam-arena",title:"RedTeam Arena",description:"An Open-Source, Community-driven Jailbreaking Platform",section:"Posts",handler:()=>{window.location.href="/blog/2024/redteam-arena/"}},{id:"post-does-style-matter",title:"Does Style Matter?",description:"Disentangling style and substance in Chatbot Arena",section:"Posts",handler:()=>{window.location.href="/blog/2024/style-control/"}},{id:"post-the-multimodal-arena-is-here",title:"The Multimodal Arena is Here!",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/multimodal/"}},{id:"post-introducing-hard-prompts-category-in-chatbot-arena",title:"Introducing Hard Prompts Category in Chatbot Arena",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/hard-prompts/"}},{id:"post-what-39-s-up-with-llama-3-arena-data-analysis",title:"What&#39;s up with Llama 3? Arena data analysis",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/llama3/"}},{id:"post-lmsys-chatbot-arena-kaggle-competition",title:"LMSYS Chatbot Arena Kaggle Competition",description:"Predicting Human Preference with $100,000 in Prizes",section:"Posts",handler:()=>{window.location.href="/blog/2024/kaggle-competition/"}},{id:"post-from-live-data-to-high-quality-benchmarks-the-arena-hard-pipeline",title:"From Live Data to High-Quality Benchmarks - The Arena-Hard Pipeline",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/arena-hard/"}},{id:"post-chatbot-arena-policy",title:"Chatbot Arena Policy",description:"Live and Community-Driven LLM Evaluation",section:"Posts",handler:()=>{window.location.href="/blog/2024/policy/"}},{id:"post-chatbot-arena-new-models-amp-elo-system-update",title:"Chatbot Arena - New models &amp; Elo system update",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-elo-update/"}},{id:"post-catch-me-if-you-can-how-to-beat-gpt-4-with-a-13b-model",title:"Catch me if you can! How to beat GPT-4 with a 13B model...",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/llm-decontaminator/"}},{id:"post-chatbot-arena-conversation-dataset-release",title:"Chatbot Arena Conversation Dataset Release",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/dataset/"}},{id:"post-chatbot-arena-leaderboard-updates-week-8",title:"Chatbot Arena Leaderboard Updates (Week 8)",description:"Introducing MT-Bench and Vicuna-33B",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-week8/"}},{id:"post-chatbot-arena-leaderboard-updates-week-4",title:"Chatbot Arena Leaderboard Updates (Week 4)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-week4/"}},{id:"post-chatbot-arena-leaderboard-updates-week-2",title:"Chatbot Arena Leaderboard Updates (Week 2)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-week2/"}},{id:"post-chatbot-arena",title:"Chatbot Arena",description:"Benchmarking LLMs in the Wild with Elo Ratings",section:"Posts",handler:()=>{window.location.href="/blog/2023/arena/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6C%6D%61%72%65%6E%61.%61%69@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>