<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Chatbot Arena Leaderboard Updates (Week 8) | Chatbot Arena </title> <meta name="author" content="Chatbot Arena"> <meta name="description" content="Introducing MT-Bench and Vicuna-33B"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/bear.png?23fade998cf650fe43c0a84f33581251"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lmarena.github.io/blog/2023/leaderboard-week8/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Chatbot Arena Leaderboard Updates (Week 8)",
            "description": "Introducing MT-Bench and Vicuna-33B",
            "published": "June 22, 2023",
            "authors": [
              
              {
                "author": "Lianmin Zheng",
                "authorURL": "https://lmzheng.net/",
                "affiliations": [
                  {
                    "name": "UC Berkeley, LMSys",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Wei-Lin Chiang",
                "authorURL": "https://infwinston.github.io/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Ying Sheng",
                "authorURL": "https://sites.google.com/view/yingsheng/home",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Hao Zhang",
                "authorURL": "https://cseweb.ucsd.edu/~haozhang/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title" href="/"> <span class="font-weight-bold">Chatbot</span> Arena </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/about/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Chatbot Arena Leaderboard Updates (Week 8)</h1> <p>Introducing MT-Bench and Vicuna-33B</p> </d-title> <d-byline></d-byline> <d-article> <p>In this blog post, we share the latest update on Chatbot Arena leaderboard, which now includes more open models and three metrics:</p> <ol> <li> <strong>Chatbot Arena Elo</strong>, based on 42K anonymous votes from <a href="https://blog.lmarena.ai/blog/2023/arena/" rel="external nofollow noopener" target="_blank">Chatbot Arena</a> using the Elo rating system.</li> <li> <strong>MT-Bench score</strong>, based on a challenging multi-turn benchmark and GPT-4 grading, proposed and validated in our <a href="https://arxiv.org/abs/2306.05685" rel="external nofollow noopener" target="_blank">Judging LLM-as-a-judge paper</a>.</li> <li> <strong>MMLU</strong>, a widely adopted <a href="https://arxiv.org/abs/2009.03300" rel="external nofollow noopener" target="_blank">benchmark</a>.</li> </ol> <p>Furthermore, we’re excited to introduce our <strong>new series of Vicuna-v1.3 models</strong>, ranging from 7B to 33B parameters, trained on an extended set of user-shared conversations. Their weights are now <a href="https://github.com/lm-sys/FastChat/tree/main#vicuna-weights" rel="external nofollow noopener" target="_blank">available</a>.</p> <h2 id="updated-leaderboard-and-new-models">Updated Leaderboard and New Models</h2> <style>th{text-align:left}td{text-align:left}table{border-collapse:collapse;width:100%}th{cursor:pointer}th:hover{background-color:#ddd}.arrow{display:inline-block;width:0;height:0;vertical-align:middle;margin-left:5px;border-left:5px solid transparent;border-right:5px solid transparent}.arrow-up{border-bottom:5px solid #000}.arrow-down{border-top:5px solid #000}th:nth-child(1) .arrow-down{border-top:5px solid #000}</style> <script>function sortTable(e,a){let r,t,l,s,n,o,_;r=document.getElementById(a),l=!0;let m="asc"===sortOrder[e];for(;l;){for(l=!1,t=r.getElementsByTagName("tr"),s=1;s<t.length-1;s++)if(_=!1,n=t[s].getElementsByTagName("td")[e],o=t[s+1].getElementsByTagName("td")[e],x_char=n.innerHTML.toLowerCase(),y_char=o.innerHTML.toLowerCase(),m){if("-"===x_char?x_val=9999:x_val=Number(x_char),"-"===y_char?y_val=9999:y_val=Number(y_char),x_val>y_val){_=!0;break}}else if("-"===x_char?x_val=0:x_val=Number(x_char),"-"===y_char?y_val=0:y_val=Number(y_char),x_val<y_val){_=!0;break}_&&(t[s].parentNode.insertBefore(t[s+1],t[s]),l=!0)}let c=document.getElementsByClassName("arrow");for(let e=0;e<c.length;e++)c[e].classList.remove("arrow-up","arrow-down");document.getElementsByTagName("th")[e].getElementsByClassName("arrow")[0].classList.add(m?"arrow-up":"arrow-down"),sortOrder[e]=m?"desc":"asc"}let sortOrder=["desc",undefined,undefined];</script> <p><br></p> <p style="color:gray; text-align: center;">Table 1. LLM Leaderboard (April 24 - June 19, 2023). The latest and detailed version <a href="https://lmarena.ai/?leaderboard" target="_blank" rel="external nofollow noopener">here</a>.</p> <div style="display: flex; justify-content: center;"> <table id="Table1"> <tbody> <tr> <th>Model</th> <th onclick="sortTable(1, 'Table1')">MT-bench (score) <span class="arrow arrow-down"></span> </th> <th onclick="sortTable(2, 'Table1')">Arena Elo Rating <span class="arrow"></span> </th> <th onclick="sortTable(3, 'Table1')">MMLU <span class="arrow"></span> </th> <th>License</th> </tr> <tr> <td><a target="_blank" href="https://openai.com/research/gpt-4" rel="external nofollow noopener"> GPT-4 </a></td> <td>8.99</td> <td>1227</td> <td>86.4</td> <td>Proprietary</td> </tr> <tr> <td><a target="_blank" href="https://openai.com/blog/chatgpt" rel="external nofollow noopener"> GPT-3.5-turbo </a></td> <td>7.94</td> <td>1130</td> <td>70.0</td> <td>Proprietary</td> </tr> <tr> <td><a target="_blank" href="https://www.anthropic.com/index/introducing-claude" rel="external nofollow noopener"> Claude-v1 </a></td> <td>7.90</td> <td>1178</td> <td>75.6</td> <td>Proprietary</td> </tr> <tr> <td><a target="_blank" href="https://www.anthropic.com/index/introducing-claude" rel="external nofollow noopener"> Claude-instant-v1 </a></td> <td>7.85</td> <td>1156</td> <td>61.3</td> <td>Proprietary</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/lmsys/vicuna-33b-v1.3" rel="external nofollow noopener"> Vicuna-33B </a></td> <td>7.12</td> <td>-</td> <td>59.2</td> <td>Non-commercial</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/WizardLM" rel="external nofollow noopener"> WizardLM-30B </a></td> <td>7.01</td> <td>-</td> <td>58.7</td> <td>Non-commercial</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/timdettmers/guanaco-33b-merged" rel="external nofollow noopener"> Guanaco-33B </a></td> <td>6.53</td> <td>1065</td> <td>57.6</td> <td>Non-commercial</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/allenai/tulu-30b" rel="external nofollow noopener"> Tulu-30B </a></td> <td>6.43</td> <td>-</td> <td>58.1</td> <td>Non-commercial</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/timdettmers/guanaco-65b-merged" rel="external nofollow noopener"> Guanaco-65B </a></td> <td>6.41</td> <td>-</td> <td>62.1</td> <td>Non-commercial</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor" rel="external nofollow noopener"> OpenAssistant-LLaMA-30B </a></td> <td>6.41</td> <td>-</td> <td>56.0</td> <td>Non-commercial</td> </tr> <tr> <td><a target="_blank" href="https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#foundation_models" rel="external nofollow noopener"> PaLM-Chat-Bison-001 </a></td> <td>6.40</td> <td>1038</td> <td>-</td> <td>Proprietary</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/lmsys/vicuna-13b-v1.3" rel="external nofollow noopener"> Vicuna-13B </a></td> <td>6.39</td> <td>1061</td> <td>52.1</td> <td>Non-commercial</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/mosaicml/mpt-30b-chat" rel="external nofollow noopener"> MPT-30B-chat </a></td> <td>6.39</td> <td>-</td> <td>50.4</td> <td>CC-BY-NC-SA-4.0</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/WizardLM/WizardLM-13B-V1.0" rel="external nofollow noopener"> WizardLM-13B </a></td> <td>6.35</td> <td>1048</td> <td>52.3</td> <td>Non-commercial</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/lmsys/vicuna-7b-v1.3" rel="external nofollow noopener"> Vicuna-7B </a></td> <td>6.00</td> <td>1008</td> <td>47.1</td> <td>Non-commercial</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/project-baize/baize-v2-13b" rel="external nofollow noopener"> Baize-v2-13B </a></td> <td>5.75</td> <td>-</td> <td>48.9</td> <td>Non-commercial</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/NousResearch/Nous-Hermes-13b" rel="external nofollow noopener"> Nous-Hermes-13B </a></td> <td>5.51</td> <td>-</td> <td>49.3</td> <td>Non-commercial</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/mosaicml/mpt-7b-chat" rel="external nofollow noopener"> MPT-7B-Chat </a></td> <td>5.42</td> <td>956</td> <td>32.0</td> <td>CC-BY-NC-SA-4.0</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/nomic-ai/gpt4all-13b-snoozy" rel="external nofollow noopener"> GPT4All-13B-Snoozy </a></td> <td>5.41</td> <td>986</td> <td>43.0</td> <td>Non-commercial</td> </tr> <tr> <td><a target="_blank" href="https://bair.berkeley.edu/blog/2023/04/03/koala/" rel="external nofollow noopener"> Koala-13B </a></td> <td>5.35</td> <td>992</td> <td>44.7</td> <td>Non-commercial</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/mosaicml/mpt-30b-instruct" rel="external nofollow noopener"> MPT-30B-Instruct </a></td> <td>5.22</td> <td>-</td> <td>47.8</td> <td>CC-BY-SA 3.0</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/tiiuae/falcon-40b-instruct" rel="external nofollow noopener"> Falcon-40B-Instruct </a></td> <td>5.17</td> <td>-</td> <td>54.7</td> <td>Apache 2.0</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-13b" rel="external nofollow noopener"> H2O-Oasst-OpenLLaMA-13B </a></td> <td>4.63</td> <td>-</td> <td>42.8</td> <td>Apache 2.0</td> </tr> <tr> <td><a target="_blank" href="https://crfm.stanford.edu/2023/03/13/alpaca.html" rel="external nofollow noopener"> Alpaca-13B </a></td> <td>4.53</td> <td>930</td> <td>48.1</td> <td>Non-commercial</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/THUDM/chatglm-6b" rel="external nofollow noopener"> ChatGLM-6B </a></td> <td>4.50</td> <td>905</td> <td>36.1</td> <td>Non-commercial</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5" rel="external nofollow noopener"> OpenAssistant-Pythia-12B </a></td> <td>4.32</td> <td>924</td> <td>27.0</td> <td>Apache 2.0</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/BlinkDL/rwkv-4-raven" rel="external nofollow noopener"> RWKV-4-Raven-14B </a></td> <td>3.98</td> <td>950</td> <td>25.6</td> <td>Apache 2.0</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/databricks/dolly-v2-12b" rel="external nofollow noopener"> Dolly-V2-12B </a></td> <td>3.28</td> <td>850</td> <td>25.7</td> <td>MIT</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/lmsys/fastchat-t5-3b-v1.0" rel="external nofollow noopener"> FastChat-T5-3B </a></td> <td>3.04</td> <td>897</td> <td>47.7</td> <td>Apache 2.0</td> </tr> <tr> <td><a target="_blank" href="https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b" rel="external nofollow noopener"> StableLM-Tuned-Alpha-7B </a></td> <td>2.75</td> <td>871</td> <td>24.4</td> <td>CC-BY-NC-SA-4.0</td> </tr> <tr> <td><a target="_blank" href="https://arxiv.org/abs/2302.13971" rel="external nofollow noopener"> LLaMA-13B </a></td> <td>2.61</td> <td>826</td> <td>47.0</td> <td>Non-commercial</td> </tr> </tbody> </table> </div> <p>Welcome to try the Chatbot Arena voting <a href="https://lmarena.ai" rel="external nofollow noopener" target="_blank">demo</a>. Keep in mind that each benchmark has its limitations. Please consider the results as guiding references. See our discussion below for more technical details.</p> <h2 id="evaluating-chatbots-with-mt-bench-and-arena">Evaluating Chatbots with MT-bench and Arena</h2> <h3 id="motivation">Motivation</h3> <p>While several benchmarks exist for evaluating Large Language Model’s (LLM) performance, such as <a href="https://arxiv.org/abs/2009.03300" rel="external nofollow noopener" target="_blank">MMLU</a>, <a href="https://arxiv.org/abs/1905.07830" rel="external nofollow noopener" target="_blank">HellaSwag</a>, and <a href="https://github.com/openai/human-eval" rel="external nofollow noopener" target="_blank">HumanEval</a>, we noticed that these benchmarks might fall short when assessing LLMs’ human preferences. Traditional benchmarks often test LLMs on close-ended questions with concise outputs (e.g., multiple choices), which do not reflect the typical use cases of LLM-based chat assistants.</p> <p>To fill this gap, in this leaderboard update, in addition to the Chatbot Arena Elo system, we add a new benchmark: MT-Bench.</p> <ul> <li> <a href="https://arxiv.org/abs/2306.05685" rel="external nofollow noopener" target="_blank">MT-bench</a> is a challenging multi-turn question set designed to evaluate the conversational and instruction-following ability of models. You can view sample questions and answers of MT-bench <a href="https://huggingface.co/spaces/lmsys/mt-bench" rel="external nofollow noopener" target="_blank">here</a>.</li> <li> <a href="https://lmarena.ai" rel="external nofollow noopener" target="_blank">Chatbot Arena</a> is a crowd-sourced battle platform, where users ask chatbots any question and vote for their preferred answer.</li> </ul> <p>Both benchmarks are designed to use human preferences as the primary metric.</p> <h3 id="why-mt-bench">Why MT-Bench?</h3> <p>MT-Bench is a carefully curated benchmark that includes 80 high-quality, multi-turn questions. These questions are tailored to assess the conversation flow and instruction-following capabilities of models in multi-turn dialogues. They include both common use cases and challenging instructions meant to distinguish between chatbots. MT-Bench serves as a <strong>quality-controlled complement</strong> to our crowd-sourced based evaluation – Chatbot Arena.</p> <p>Through running the Chatbot Arena for 2 months and analyzing our users’ prompts, we’ve identified 8 primary categories of user prompts: Writing, Roleplay, Extraction, Reasoning, Math, Coding, Knowledge I (STEM), and Knowledge II (humanities/social science). We crafted 10 multi-turn questions per category, yielding a set of 160 questions in total. We display some sample questions below in Figure 1. You can find more <a href="https://huggingface.co/spaces/lmsys/mt-bench" rel="external nofollow noopener" target="_blank">here</a>.</p> <p><img src="/assets/img/blog/leaderboard_week8/sample_question.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;" width="90%"></p> <p style="color:gray; text-align: center;">Figure 1: Sample questions from the MT-Bench.</p> <h3 id="but-still-how-to-grade-chatbots-answers">But Still, How to Grade Chatbots’ Answers?</h3> <p>Though we believe human preference is the gold standard, it is notoriously slow and expensive to collect. In our first <a href="https://lmsys.org/blog/2023-03-30-vicuna/" rel="external nofollow noopener" target="_blank">Vicuna blogpost</a>, we explored an automated evaluation pipeline based on GPT-4. This approach has since got popular and adopted in several <a href="#related-work">concurrent and follow-up works</a>.</p> <p>In our latest paper, <a href="https://arxiv.org/abs/2306.05685" rel="external nofollow noopener" target="_blank">“Judging LLM-as-a-judge”</a>, we conducted a systematic study to answer how reliable those LLM judges are. We provide a brief overview of conclusions here but recommend reading the paper for more details.</p> <p>We begin by acknowledging potential limitations of LLM-as-a-judge:</p> <ul> <li> <strong>Position bias</strong> where LLM judges may favor the first answer in a pairwise comparison.</li> <li> <strong>Verbosity bias</strong> where LLM judges may favor lengthier answers, regardless of their quality.</li> <li> <strong>Self-enhancement bias</strong> where LLM judges may favor their own responses.</li> <li> <strong>Limited reasoning ability</strong> referring to LLM judges’ possible shortcomings in grading math and reasoning questions.</li> </ul> <p>Our study then explores how few-shot judge, chain-of-thought judge, reference-based judge, and fine-tuned judge can help to mitigate these limitations.</p> <p>Upon implementing some of these solutions, we discovered that despite limitations, strong LLM judges like GPT-4 can align impressively well with both controlled and crowdsourced human preferences, achieving over 80% agreement. This level of agreement is comparable to the agreement between two different human judges. Therefore, if used carefully, LLM-as-a-judge can act as a <em>scalable</em> and <em>explainable</em> approximation of human preferences.</p> <p>We also found that single-answer grading based on GPT-4, without pairwise comparison, can also rank models effectively and match human preferences well. In Table 1, we present the MT-Bench as a column on the leaderboard based on single-answer grading with GPT-4.</p> <h2 id="results-and-analysis">Results and Analysis</h2> <h3 id="mt-bench-effectively-distinguishes-among-chatbots">MT-Bench Effectively Distinguishes Among Chatbots</h3> <p>Table 1 provides a detailed rundown of the MT-bench-enhanced leaderboard, where we conduct an exhaustive evaluation of 28 popular instruction-tuned models. We observe a clear distinction among chatbots of varying abilities, with scores showing a high correlation with the Chatbot Arena Elo rating. In particular, MT-Bench reveals noticeable performance gaps between GPT-4 and GPT-3.5/Claude, and between open and proprietary models.</p> <p>To delve deeper into the distinguishing factors among chatbots, we select a few representative chatbots and break down their performance per category in Figure 2. GPT-4 shows superior performance in Coding and Reasoning compared to GPT-3.5/Claude, while Vicuna-13B lags significantly behind in several specific categories: Extraction, Coding, and Math. This suggests there is still ample room for improvement for open-source models.</p> <p><img src="/assets/img/blog/leaderboard_week8/ability_breakdown.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;" width="90%"></p> <p style="color:gray; text-align: center;">Figure 2: The comparison of 6 representative LLMs regarding their abilities in 8 categories: Writing, Roleplay, Reasoning, Math, Coding, Extraction, STEM, Humanities.</p> <h3 id="multi-turn-conversation-capabilities">Multi-turn Conversation Capabilities</h3> <p>We next analyze the multi-turn scores of selected models, presented in Table 2.</p> <p style="color:gray; text-align: center;">Table 2. The breakdown of LLMs' MT-bench scores in the 1st and 2nd turn of a dialogue. Full score is 10.</p> <div style="display: flex; justify-content: center;"> <table id="Table2"> <tbody> <tr> <th>Model</th> <th>Average 1st Turn Score</th> <th>Average 2nd Turn Score</th> <th>Score Difference</th> </tr> <tr> <td><a href="https://chat.openai.com/" target="_blank" rel="external nofollow noopener">GPT-4</a></td> <td>8.96</td> <td>9.03</td> <td>0.07</td> </tr> <tr> <td><a href="https://www.anthropic.com/index/introducing-claude" target="_blank" rel="external nofollow noopener">Claude-v1</a></td> <td>8.15</td> <td>7.65</td> <td>-0.50</td> </tr> <tr> <td><a href="https://chat.openai.com/" target="_blank" rel="external nofollow noopener">GPT-3.5-turbo</a></td> <td>8.08</td> <td>7.81</td> <td>-0.26</td> </tr> <tr> <td><a href="https://github.com/lm-sys/FastChat#vicuna-weights" target="_blank" rel="external nofollow noopener">Vicuna-33B</a></td> <td>7.46</td> <td>6.79</td> <td>-0.67</td> </tr> <tr> <td><a href="https://huggingface.co/WizardLM/" target="_blank" rel="external nofollow noopener">WizardLM-30B</a></td> <td>7.13</td> <td>6.89</td> <td>-0.24</td> </tr> <tr> <td><a href="https://huggingface.co/WizardLM/" target="_blank" rel="external nofollow noopener">WizardLM-13B</a></td> <td>7.12</td> <td>5.59</td> <td>-1.53</td> </tr> <tr> <td><a href="https://huggingface.co/timdettmers/guanaco-33b-merged" target="_blank" rel="external nofollow noopener">Guanaco-33B</a></td> <td>6.88</td> <td>6.18</td> <td>-0.71</td> </tr> <tr> <td><a href="https://github.com/lm-sys/FastChat#vicuna-weights" target="_blank" rel="external nofollow noopener">Vicuna-13B</a></td> <td>6.81</td> <td>5.96</td> <td>-0.85</td> </tr> <tr> <td><a href="https://cloud.google.com/vertex-ai/docs/release-notes#May_10_2023" target="_blank" rel="external nofollow noopener">PaLM2-Chat-Bison</a></td> <td>6.71</td> <td>6.09</td> <td>-0.63</td> </tr> <tr> <td><a href="https://github.com/lm-sys/FastChat#vicuna-weights" target="_blank" rel="external nofollow noopener">Vicuna-7B</a></td> <td>6.69</td> <td>5.30</td> <td>-1.39</td> </tr> <tr> <td><a href="https://huggingface.co/young-geng/koala" target="_blank" rel="external nofollow noopener">Koala-13B</a></td> <td>6.08</td> <td>4.63</td> <td>-1.45</td> </tr> <tr> <td><a href="https://huggingface.co/mosaicml/mpt-7b-chat" target="_blank" rel="external nofollow noopener">MPT-7B-Chat</a></td> <td>5.85</td> <td>4.99</td> <td>-0.86</td> </tr> <tr> <td><a href="https://huggingface.co/tiiuae/falcon-40b-instruct" target="_blank" rel="external nofollow noopener">Falcon-40B-instruct</a></td> <td>5.81</td> <td>4.53</td> <td>-1.29</td> </tr> <tr> <td><a href="https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-13b" target="_blank" rel="external nofollow noopener">H2OGPT-Oasst-Open-LLaMA-13B</a></td> <td>5.51</td> <td>3.74</td> <td>-1.78</td> </tr> </tbody> </table> </div> <p>­</p> <p>The MT-bench incorporates challenging follow-up questions as part of its design. For open models, The performance drops significantly from the first to the second turn (e.g., Vicuna-7B, WizardLM-13B), while strong proprietary models maintain consistency. We also notice a considerable performance gap between LLaMA-based models and those with permissive licenses (MPT-7B, Falcon-40B, and instruction-tuned Open-LLaMA).</p> <h3 id="explainability-in-llm-judges">Explainability in LLM judges</h3> <p>Another advantage of LLM judges is their ability to provide explainable evaluations. Figure 3 presents an instance of GPT-4’s judgment on an MT-bench question, with answers from alpaca-13b and gpt-3.5-turbo. GPT-4 provides thorough and logical feedback to support its judgment. Our <a href="https://arxiv.org/abs/2306.05685" rel="external nofollow noopener" target="_blank">study</a> found that such reviews are beneficial in guiding humans to make better-informed decisions (refer to Section 4.2 for more details). All the GPT-4 judgments can be found on our <a href="https://huggingface.co/spaces/lmsys/mt-bench" rel="external nofollow noopener" target="_blank">demo site</a>.</p> <p><img src="/assets/img/blog/leaderboard_week8/explainability_sample.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;" width="90%"></p> <p style="color:gray; text-align: center;">Figure 3: MT-bench provides more explainability in evaluating LLMs' human preferences.</p> <p>In conclusion, we have shown that MT-Bench effectively differentiates between chatbots of varying capabilities. It’s scalable, offers valuable insights with category breakdowns, and provides explainability for human judges to verify. However, LLM judges should be used carefully. It can still make errors, especially when grading math/reasoning questions.</p> <h2 id="how-to-evaluate-new-models-on-mt-bench">How to Evaluate New Models on MT-Bench?</h2> <p>Evaluating models on MT-bench is simple and fast. Our script supports all huggingface models, and we’ve provided <a href="https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge#mt-bench" rel="external nofollow noopener" target="_blank">detailed instructions</a>, in which you can generate model’s answers to the MT-bench questions and their GPT-4 judgments. You can also examine the answers and reviews on our gradio browsing demo.</p> <h2 id="next-steps">Next steps</h2> <p><strong>Release of Conversations Data</strong></p> <p>We’re in the process of releasing Chatbot Arena conversations data to the broader research community. Stay tuned for updates!</p> <p><strong>MT-bench-1K</strong></p> <p>MT-Bench currently consists of a concise set of 80 carefully curated questions, ensuring the highest quality. We’re actively expanding the question set to MT-Bench-1K by integrating high-quality prompts from the Chatbot Arena and generating new ones automatically using LLMs. If you have any good ideas, we’d be delighted to hear from you.</p> <p><strong>Invitation for collaborations</strong></p> <p>We’re engaging with various organizations to explore possibilities for standardizing the evaluation of human preferences for LLMs at scale. If this interests you, please feel free to reach out to us.</p> <h2 id="related-work">Related work</h2> <p>There has been a great amount of interesting work studying how to evaluate human preferences and how to use strong LLM as judges for evaluation. You are welcome to check them out and see more opinions on this topic:</p> <ul> <li><a href="https://arxiv.org/abs/2306.05685" rel="external nofollow noopener" target="_blank">Judging LLM-as-a-judge with MT-Bench and Chatbot Arena</a></li> <li><a href="https://huggingface.co/blog/llm-leaderboard" rel="external nofollow noopener" target="_blank">Can foundation models label data like humans?</a></li> <li><a href="https://arxiv.org/abs/2306.04751" rel="external nofollow noopener" target="_blank">How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources</a></li> <li><a href="https://arxiv.org/abs/2305.15717" rel="external nofollow noopener" target="_blank">The False Promise of Imitating Proprietary LLMs</a></li> <li><a href="https://github.com/tatsu-lab/alpaca_eval" rel="external nofollow noopener" target="_blank">AlpacaEval and AlpacaFarm</a></li> <li><a href="https://arxiv.org/abs/2305.17926" rel="external nofollow noopener" target="_blank">Large Language Models are not Fair Evaluators</a></li> </ul> <h2 id="links">Links</h2> <p>Below are readily available tools and code to run MT-bench and other metrics used in this blogpost:</p> <ul> <li>The MT-bench uses <a href="https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge" rel="external nofollow noopener" target="_blank">fastchat.llm_judge</a>,</li> <li>The <a href="https://colab.research.google.com/drive/1RAWb22-PFNI-X1gPVzc927SGUdfr6nsR?usp=sharing" rel="external nofollow noopener" target="_blank">Arena Elo calculator</a>.</li> <li>The MMLU is based on <a href="https://github.com/declare-lab/instruct-eval/blob/main/mmlu.py" rel="external nofollow noopener" target="_blank">InstructEval</a> and <a href="https://github.com/FranxYao/chain-of-thought-hub/tree/main/MMLU" rel="external nofollow noopener" target="_blank">Chain-of-Thought Hub</a>.</li> </ul> <p>If you wish to see more models on leaderboard, we invite you to <a href="https://github.com/lm-sys/FastChat/blob/main/docs/arena.md#how-to-add-a-new-model" rel="external nofollow noopener" target="_blank">contribute to FastChat</a> or <a href="mailto:lmsysorg@gmail.com">contact us</a> to provide us with API access.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"lmarena/lmarena.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Chatbot Arena. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-",title:"",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-about",title:"about",description:"",section:"Navigation",handler:()=>{window.location.href="/about/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-preference-proxy-evaluations",title:"Preference Proxy Evaluations",description:"A New Benchmark for Evaluating Reward Models and LLM Judges",section:"Posts",handler:()=>{window.location.href="/blog/2024/preference-proxy-evaluations/"}},{id:"post-agent-arena",title:"Agent Arena",description:"A Platform for Evaluating and Comparing LLM Agents Across Models, Tools, and Frameworks",section:"Posts",handler:()=>{window.location.href="/blog/2024/agent-arena/"}},{id:"post-statistical-extensions-of-the-bradley-terry-and-elo-models",title:"Statistical Extensions of the Bradley-Terry and Elo Models",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/extended-arena/"}},{id:"post-chatbot-arena-new-blog",title:"Chatbot Arena New Blog",description:"A new chapter for Chatbot Arena!",section:"Posts",handler:()=>{window.location.href="/blog/2024/new-site/"}},{id:"post-redteam-arena",title:"RedTeam Arena",description:"An Open-Source, Community-driven Jailbreaking Platform",section:"Posts",handler:()=>{window.location.href="/blog/2024/redteam-arena/"}},{id:"post-does-style-matter",title:"Does Style Matter?",description:"Disentangling style and substance in Chatbot Arena",section:"Posts",handler:()=>{window.location.href="/blog/2024/style-control/"}},{id:"post-the-multimodal-arena-is-here",title:"The Multimodal Arena is Here!",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/multimodal/"}},{id:"post-introducing-hard-prompts-category-in-chatbot-arena",title:"Introducing Hard Prompts Category in Chatbot Arena",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/hard-prompts/"}},{id:"post-what-39-s-up-with-llama-3-arena-data-analysis",title:"What&#39;s up with Llama 3? Arena data analysis",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/llama3/"}},{id:"post-lmsys-chatbot-arena-kaggle-competition",title:"LMSYS Chatbot Arena Kaggle Competition",description:"Predicting Human Preference with $100,000 in Prizes",section:"Posts",handler:()=>{window.location.href="/blog/2024/kaggle-competition/"}},{id:"post-from-live-data-to-high-quality-benchmarks-the-arena-hard-pipeline",title:"From Live Data to High-Quality Benchmarks - The Arena-Hard Pipeline",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/arena-hard/"}},{id:"post-chatbot-arena-policy-update",title:"Chatbot Arena Policy Update",description:"Live and Community-Driven LLM Evaluation",section:"Posts",handler:()=>{window.location.href="/blog/2024/policy/"}},{id:"post-chatbot-arena-new-models-amp-elo-system-update",title:"Chatbot Arena - New models &amp; Elo system update",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-elo-update/"}},{id:"post-catch-me-if-you-can-how-to-beat-gpt-4-with-a-13b-model",title:"Catch me if you can! How to beat GPT-4 with a 13B model...",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/llm-decontaminator/"}},{id:"post-chatbot-arena-conversation-dataset-release",title:"Chatbot Arena Conversation Dataset Release",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/dataset/"}},{id:"post-chatbot-arena-leaderboard-updates-week-8",title:"Chatbot Arena Leaderboard Updates (Week 8)",description:"Introducing MT-Bench and Vicuna-33B",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-week8/"}},{id:"post-chatbot-arena-leaderboard-updates-week-4",title:"Chatbot Arena Leaderboard Updates (Week 4)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-week4/"}},{id:"post-chatbot-arena-leaderboard-updates-week-2",title:"Chatbot Arena Leaderboard Updates (Week 2)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-week2/"}},{id:"post-chatbot-arena",title:"Chatbot Arena",description:"Benchmarking LLMs in the Wild with Elo Ratings",section:"Posts",handler:()=>{window.location.href="/blog/2023/arena/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6C%6D%61%72%65%6E%61.%61%69@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>