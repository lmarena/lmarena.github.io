<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Chatbot Arena | Chatbot Arena </title> <meta name="author" content="Chatbot Arena"> <meta name="description" content="Benchmarking LLMs in the Wild with Elo Ratings"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/bear.png?23fade998cf650fe43c0a84f33581251"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lmarena.github.io/blog/2023/arena/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Chatbot Arena",
            "description": "Benchmarking LLMs in the Wild with Elo Ratings",
            "published": "May 03, 2023",
            "authors": [
              
              {
                "author": "Lianmin Zheng*",
                "authorURL": "https://lmzheng.net/",
                "affiliations": [
                  {
                    "name": "UC Berkeley, LMSys",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Ying Sheng*",
                "authorURL": "https://sites.google.com/view/yingsheng/home",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Wei-Lin Chiang",
                "authorURL": "https://infwinston.github.io/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Hao Zhang",
                "authorURL": "https://cseweb.ucsd.edu/~haozhang/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Joseph E. Gonzalez",
                "authorURL": "https://people.eecs.berkeley.edu/~jegonzal/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Ion Stoica",
                "authorURL": "https://people.eecs.berkeley.edu/~istoica/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title" href="/"> <span class="font-weight-bold">Chatbot</span> Arena </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/about/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Chatbot Arena</h1> <p>Benchmarking LLMs in the Wild with Elo Ratings</p> </d-title> <d-byline></d-byline> <d-article> <p>We present Chatbot Arena, a benchmark platform for large language models (LLMs) that features anonymous, randomized battles in a crowdsourced manner. In this blog post, we are releasing our initial results and a leaderboard based on the Elo rating system, which is a widely-used rating system in chess and other competitive games. We invite the entire community to join this effort by contributing new models and evaluating them by asking questions and voting for your favorite answer.</p> <style>th{text-align:left}td{text-align:left}</style> <p style="color:gray; text-align: center;">Table 1. LLM Leaderboard (April 24 - May 1, 2023). The latest and detailed version <a href="https://lmarena.ai/?leaderboard" target="_blank" rel="external nofollow noopener">here</a>.</p> <table style="display: flex; justify-content: center;" align="left"> <tbody> <tr> <th>Rank</th> <th>Model</th> <th>Elo Rating</th> <th>Description</th> </tr> <tr> <td>1</td> <td>ðŸ¥‡ <a href="https://lmsys.org/blog/2023-03-30-vicuna/" target="_blank" rel="external nofollow noopener">vicuna-13b</a> </td> <td>1169</td> <td>a chat assistant fine-tuned from LLaMA on user-shared conversations by LMSYS</td> </tr> <tr> <td>2</td> <td>ðŸ¥ˆ <a href="https://bair.berkeley.edu/blog/2023/04/03/koala" target="_blank" rel="external nofollow noopener">koala-13b</a> </td> <td>1082</td> <td>a dialogue model for academic research by BAIR</td> </tr> <tr> <td>3</td> <td>ðŸ¥‰ <a href="https://open-assistant.io" target="_blank" rel="external nofollow noopener">oasst-pythia-12b</a> </td> <td>1065</td> <td>an Open Assistant for everyone by LAION</td> </tr> <tr> <td>4</td> <td><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html" target="_blank" rel="external nofollow noopener">alpaca-13b</a></td> <td>1008</td> <td>a model fine-tuned from LLaMA on instruction-following demonstrations by Stanford</td> </tr> <tr> <td>5</td> <td><a href="https://chatglm.cn/blog" target="_blank" rel="external nofollow noopener">chatglm-6b</a></td> <td>985</td> <td>an open bilingual dialogue language model by Tsinghua University</td> </tr> <tr> <td>6</td> <td><a href="https://huggingface.co/lmsys/fastchat-t5-3b-v1.0" target="_blank" rel="external nofollow noopener">fastchat-t5-3b</a></td> <td>951</td> <td>a chat assistant fine-tuned from FLAN-T5 by LMSYS</td> </tr> <tr> <td>7</td> <td><a href="https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm" target="_blank" rel="external nofollow noopener">dolly-v2-12b</a></td> <td>944</td> <td>an instruction-tuned open large language model by Databricks</td> </tr> <tr> <td>8</td> <td><a href="https://arxiv.org/abs/2302.13971" target="_blank" rel="external nofollow noopener">llama-13b</a></td> <td>932</td> <td>open and efficient foundation language models by Meta</td> </tr> <tr> <td>9</td> <td><a href="https://github.com/stability-AI/stableLM" target="_blank" rel="external nofollow noopener">stablelm-tuned-alpha-7b</a></td> <td>858</td> <td>Stability AI language models</td> </tr> </tbody> </table> <p>Â­</p> <p>Table 1 displays the Elo ratings of nine popular models, which are based on the 4.7K voting data and calculations shared in this <a href="https://colab.research.google.com/drive/1RAWb22-PFNI-X1gPVzc927SGUdfr6nsR?usp=sharing" rel="external nofollow noopener" target="_blank">notebook</a>. You can also try the voting <a href="https://lmarena.ai" rel="external nofollow noopener" target="_blank">demo</a>.</p> <p><img src="/assets/img/blog/arena/chat_demo.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;" width="100%"></p> <p style="color:gray; text-align: center;">Figure 1. The side-by-side chatting and voting interface.</p> <p>Please note that we periodically release blog posts to update the leaderboard. Feel free to check the following updates:</p> <ul> <li><a href="https://blog.lmarena.ai/blog/2023/leaderboard-week2/" rel="external nofollow noopener" target="_blank">May 10 Updates</a></li> <li><a href="https://blog.lmarena.ai/blog/2023/leaderboard-week4/" rel="external nofollow noopener" target="_blank">May 25 Updates</a></li> <li><a href="https://blog.lmarena.ai/blog/2023/leaderboard-week8/" rel="external nofollow noopener" target="_blank">June 22 Updates</a></li> <li><a href="https://blog.lmarena.ai/blog/2023/dataset/" rel="external nofollow noopener" target="_blank">Dataset Release (July 20)</a></li> <li><a href="https://blog.lmarena.ai/blog/2023/leaderboard-elo-update/" rel="external nofollow noopener" target="_blank">Dec. 7 Updates</a></li> <li><a href="https://blog.lmarena.ai/blog/2024/policy/" rel="external nofollow noopener" target="_blank">Policy Updates (March 1, 2024)</a></li> </ul> <h2 id="introduction">Introduction</h2> <p>Following the great success of ChatGPT, there has been a proliferation of open-source large language models that are finetuned to follow instructions. These models are capable of providing valuable assistance in response to usersâ€™ questions/prompts. Notable examples include Alpaca and Vicuna, based on LLaMA, and OpenAssistant and Dolly, based on Pythia.</p> <p>Despite the constant release of new models every week, the community faces a challenge in benchmarking these models effectively. Benchmarking LLM assistants is extremely challenging because the problems can be open-ended, and it is very difficult to write a program to automatically evaluate the response quality. In this case, we typically have to resort to human evaluation based on pairwise comparison.</p> <p>There are some desired properties for a good benchmark system based on pairwise comparison.</p> <ul> <li> <strong>Scalability</strong>. The system should scale to a large number of models when it is not feasible to collect sufficient data for all possible model pairs.</li> <li> <strong>Incrementality</strong>. The system should be able to evaluate a new model using a relatively small number of trials.</li> <li> <strong>Unique order</strong>. The system should provide a unique order for all models. Given any two models, we should be able to tell which ranks higher or whether they are tied.</li> </ul> <p>Existing LLM benchmark systems rarely satisfy all of these properties. Classical LLM benchmark frameworks, such as <a href="https://crfm.stanford.edu/helm/latest/" rel="external nofollow noopener" target="_blank">HELM</a> and <a href="https://github.com/EleutherAI/lm-evaluation-harness" rel="external nofollow noopener" target="_blank">lm-evaluation-harness</a>, provide multi-metric measurements for tasks commonly used in academic research. However, they are not based on pairwise comparison and are not effective at evaluating open-ended questions. OpenAI also launched the <a href="https://github.com/openai/evals" rel="external nofollow noopener" target="_blank">evals</a> project to collect better questions, but this project does not provide ranking mechanisms for all participating models. When we launched our <a href="https://lmsys.org/blog/2023-03-30-vicuna/" rel="external nofollow noopener" target="_blank">Vicuna</a> model, we utilized a GPT-4-based evaluation pipeline, but it does not provide a solution for scalable and incremental ratings.</p> <p>In this blog post, we introduce Chatbot Arena, an LLM benchmark platform featuring anonymous randomized battles in a crowdsourced manner. Chatbot Arena adopts the <a href="https://en.wikipedia.org/wiki/Elo_rating_system" rel="external nofollow noopener" target="_blank">Elo rating system</a>, which is a widely-used rating system in chess and other competitive games. The Elo rating system is promising to provide the desired property mentioned above. We noticed that the <a href="https://arxiv.org/pdf/2204.05862.pdf" rel="external nofollow noopener" target="_blank">Anthropic LLM paper</a> also adopted the Elo rating system.</p> <p>To collect data, we launched the arena with several popular open-source LLMs one week ago. In the arena, a user can chat with two anonymous models side-by-side and vote for which one is better. This crowdsourcing way of data collection represents some use cases of LLMs in the wild. A comparison between several evaluation methods is shown in Table 2.</p> <p><br></p> <p style="color:gray; text-align: center;">Table 2: Comparison between different evaluation methods.</p> <div style="display: flex; justify-content: center; min-width: 700px;"> <table> <tbody> <tr> <th></th> <th>HELM / lm-evaluation-harness</th> <th>OpenAI/eval</th> <th>Alpaca Evaluation</th> <th>Vicuna Evaluation</th> <th>Chatbot Arena</th> </tr> <tr> <td><strong>Question Source</strong></td> <td>Academic datasets</td> <td>Mixed</td> <td>Self-instruct evaluation set</td> <td>GPT-4 generated</td> <td>User prompts</td> </tr> <tr> <td><strong>Evaluator</strong></td> <td>Program</td> <td>Program/Model</td> <td>Human</td> <td>GPT-4</td> <td>User</td> </tr> <tr> <td><strong>Metrics</strong></td> <td>Basic metrics </td> <td>Basic metrics</td> <td>Win rate</td> <td>Win rate</td> <td>Elo ratings</td> </tr> </tbody> </table> </div> <h2 id="data-collection">Data Collection</h2> <p>We hosted the arena at <a href="https://lmarena.ai" rel="external nofollow noopener" target="_blank">https://lmarena.ai</a> with our multi-model serving system, <a href="https://github.com/lm-sys/FastChat" rel="external nofollow noopener" target="_blank">FastChat</a>. When a user enters the arena, they can chat with two anonymous models side-by-side, as shown in Figure 1. After getting responses from the two models, users can continue chatting or vote for the model they think is better. Once a vote is submitted, the model names will be revealed. Users can continue chatting or restart a new battle with two new randomly chosen anonymous models. The platform logs all user interactions. In our analysis, we only use the votes when the model names are hidden.</p> <p>The arena was launched about one week ago and we have collected 4.7k valid anonymous votes since then. We share some exploratory analysis in this <a href="https://colab.research.google.com/drive/1RAWb22-PFNI-X1gPVzc927SGUdfr6nsR?usp=sharing" rel="external nofollow noopener" target="_blank">notebook</a> and present a short summary here.</p> <p><img src="/assets/img/blog/arena/battle_counts.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 90%"></p> <p style="color:gray; text-align: center;">Figure 2: Battle count of each combination of models</p> <p>Figure 2 shows the battles count of each combination of models. When we initially launched the tournament, we had prior information on the likely ranking based on our benchmarks and chose to pair models according to this ranking. We gave preference to what we believed would be strong pairings based on this ranking. However, we later switched to uniform sampling to get better overall coverage of the rankings. Towards the end of the tournament, we also introduced a new model <code class="language-plaintext highlighter-rouge">fastchat-t5-3b</code>. All of these result in non-uniform model frequency.</p> <p><img src="/assets/img/blog/arena/lang_counts.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 90%"></p> <p style="color:gray; text-align: center;">Figure 3: Battle counts for the top-15 languages.</p> <p>Figure 3 plots the language distribution and shows most user prompts are in English.</p> <h2 id="elo-rating-system">Elo Rating System</h2> <p>The <a href="https://en.wikipedia.org/wiki/Elo_rating_system" rel="external nofollow noopener" target="_blank">Elo rating system</a> is a method for calculating the relative skill levels of players, which has been widely adopted in competitive games and sports. The difference in the ratings between two players serves as a predictor of the outcome of a match. The Elo rating system works well for our case because we have multiple models and we run pairwise battles between them.</p> <p>If player A has a rating of <code class="language-plaintext highlighter-rouge">Ra</code> and player B a rating of <code class="language-plaintext highlighter-rouge">Rb</code>, the exact formula (using the logistic curve with base 10) for the probability of player A winning is</p> <p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7c80282e9c95e92d6b210467aab48a8c4c81ef10" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;"></p> <p>The ratings of players can be linearly updated after each battle. Suppose player A (with Rating <code class="language-plaintext highlighter-rouge">Ra</code>) was expected to score <code class="language-plaintext highlighter-rouge">Ea</code> points but actucally scored <code class="language-plaintext highlighter-rouge">Sa</code> points. The formula for updating that playerâ€™s rating is</p> <p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1cad9fb1cfc6a8e845493ac9a40eb98541a4641a" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;"></p> <p>Using the collected data, we compute the Elo ratings of the models in this <a href="https://colab.research.google.com/drive/1RAWb22-PFNI-X1gPVzc927SGUdfr6nsR?usp=sharing" rel="external nofollow noopener" target="_blank">notebook</a> and put the main results in Table 1. You are welcome to try the notebook and play with the voting data by yourself. The data only contains voting results without conversation histories because releasing the conversation history will raise concerns such as privacy and toxicity.</p> <h2 id="pairwise-win-rates">Pairwise Win Rates</h2> <p>As a basis for calibration, we also present here the pairwise win rates for each model in the tournament (Figure 4) as well as the predicted pairwise win rate estimated using Elo ratings (Figure 5). By comparing the figures, we find the elo ratings can predict win rates relatively well.</p> <p><img src="/assets/img/blog/arena/win_fraction.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;" width="90%"></p> <p style="color:gray; text-align: center;">Figure 4: Fraction of Model A wins for all non-tied A vs. B battles.</p> <p><img src="/assets/img/blog/arena/predicted_win_fraction.png" style="display:block; margin-left: auto; margin-right: auto; margin-bottom: auto;" width="90%"></p> <p style="color:gray; text-align: center;">Figure 5: Predicted win rate using Elo ratings for Model A in an A vs. B battle</p> <h2 id="future-plans">Future Plans</h2> <p>We plan to work on the following items:</p> <ul> <li>Add more closed-source models (ChatGPT-3.5, ChatGPT-4, and Claude-v1 are avaiable now in the anonymous Arena)</li> <li>Add more open-source models</li> <li>Release periodically updated leaderboards (e.g., monthly)</li> <li>Implement better sampling algorithms, tournament mechanisms, and serving systems to support a much larger number of models</li> <li>Provide fine-grained rankings on different task types.</li> </ul> <p>We appreciate any feedback from you to make the arena better.</p> <h2 id="join-us">Join Us</h2> <p>We invite the entire community to join this benchmarking effort by contributing your models and votes for the anonymous models you think provide better answers. You can visit <a href="https://lmarena.ai" rel="external nofollow noopener" target="_blank">https://lmarena.ai</a> to vote for better models. If you want to see a specific model in the arena, you can follow this <a href="https://github.com/lm-sys/FastChat/blob/main/docs/arena.md#how-to-add-a-new-model" rel="external nofollow noopener" target="_blank">guide</a> to help us add it.</p> <h2 id="acknowledgment">Acknowledgment</h2> <p>We thank other members of the Vicuna team for valuable feedback and MBZUAI for donating compute resources. Additionally, we extend our thanks to Tianjun Zhang and Eric Wallace for their insightful discussions.</p> <h2 id="links">Links</h2> <ul> <li>Demo: <a href="https://lmarena.ai" rel="external nofollow noopener" target="_blank">https://lmarena.ai</a> </li> <li>Leaderboard: <a href="https://lmarena.ai/?leaderboard" rel="external nofollow noopener" target="_blank">https://lmarena.ai/?leaderboard</a> </li> <li>GitHub: <a href="https://github.com/lm-sys/FastChat" rel="external nofollow noopener" target="_blank">https://github.com/lm-sys/FastChat</a> </li> <li>Colab notebook: <a href="https://colab.research.google.com/drive/1RAWb22-PFNI-X1gPVzc927SGUdfr6nsR?usp=sharing" rel="external nofollow noopener" target="_blank">https://colab.research.google.com/drive/1RAWb22-PFNI-X1gPVzc927SGUdfr6nsR?usp=sharing</a> </li> </ul> <h2 id="citation">Citation</h2> <p>Please cite the following <a href="https://arxiv.org/abs/2403.04132" rel="external nofollow noopener" target="_blank">papers</a> if you find our work useful.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{chiang2024chatbot,
    title={Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference},
    author={Wei-Lin Chiang and Lianmin Zheng and Ying Sheng and Anastasios Nikolas Angelopoulos and Tianle Li and Dacheng Li and Hao Zhang and Banghua Zhu and Michael Jordan and Joseph E. Gonzalez and Ion Stoica},
    year={2024},
    eprint={2403.04132},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@inproceedings{zheng2023judging,
    title={Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena},
    author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
    year={2023},
    url={https://openreview.net/forum?id=uccHPGDlao}
}

@inproceedings{zheng2024lmsyschatm,
    title={LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset},
    author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Tianle Li and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zhuohan Li and Zi Lin and Eric Xing and Joseph E. Gonzalez and Ion Stoica and Hao Zhang},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=BOfDKxfwt0}
}
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"lmarena/lmarena.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2024 Chatbot Arena. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-",title:"",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-about",title:"about",description:"",section:"Navigation",handler:()=>{window.location.href="/about/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-preference-proxy-evaluations",title:"Preference Proxy Evaluations",description:"A New Benchmark for Evaluating Reward Models and LLM Judges",section:"Posts",handler:()=>{window.location.href="/blog/2024/preference-proxy-evaluations/"}},{id:"post-agent-arena",title:"Agent Arena",description:"A Platform for Evaluating and Comparing LLM Agents Across Models, Tools, and Frameworks",section:"Posts",handler:()=>{window.location.href="/blog/2024/agent-arena/"}},{id:"post-statistical-extensions-of-the-bradley-terry-and-elo-models",title:"Statistical Extensions of the Bradley-Terry and Elo Models",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/extended-arena/"}},{id:"post-chatbot-arena-new-blog",title:"Chatbot Arena New Blog",description:"A new chapter for Chatbot Arena!",section:"Posts",handler:()=>{window.location.href="/blog/2024/new-site/"}},{id:"post-redteam-arena",title:"RedTeam Arena",description:"An Open-Source, Community-driven Jailbreaking Platform",section:"Posts",handler:()=>{window.location.href="/blog/2024/redteam-arena/"}},{id:"post-does-style-matter",title:"Does Style Matter?",description:"Disentangling style and substance in Chatbot Arena",section:"Posts",handler:()=>{window.location.href="/blog/2024/style-control/"}},{id:"post-the-multimodal-arena-is-here",title:"The Multimodal Arena is Here!",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/multimodal/"}},{id:"post-introducing-hard-prompts-category-in-chatbot-arena",title:"Introducing Hard Prompts Category in Chatbot Arena",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/hard-prompts/"}},{id:"post-what-39-s-up-with-llama-3-arena-data-analysis",title:"What&#39;s up with Llama 3? Arena data analysis",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/llama3/"}},{id:"post-lmsys-chatbot-arena-kaggle-competition",title:"LMSYS Chatbot Arena Kaggle Competition",description:"Predicting Human Preference with $100,000 in Prizes",section:"Posts",handler:()=>{window.location.href="/blog/2024/kaggle-competition/"}},{id:"post-from-live-data-to-high-quality-benchmarks-the-arena-hard-pipeline",title:"From Live Data to High-Quality Benchmarks - The Arena-Hard Pipeline",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/arena-hard/"}},{id:"post-chatbot-arena-policy-update",title:"Chatbot Arena Policy Update",description:"Live and Community-Driven LLM Evaluation",section:"Posts",handler:()=>{window.location.href="/blog/2024/policy/"}},{id:"post-chatbot-arena-new-models-amp-elo-system-update",title:"Chatbot Arena - New models &amp; Elo system update",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-elo-update/"}},{id:"post-catch-me-if-you-can-how-to-beat-gpt-4-with-a-13b-model",title:"Catch me if you can! How to beat GPT-4 with a 13B model...",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/llm-decontaminator/"}},{id:"post-chatbot-arena-conversation-dataset-release",title:"Chatbot Arena Conversation Dataset Release",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/dataset/"}},{id:"post-chatbot-arena-leaderboard-updates-week-8",title:"Chatbot Arena Leaderboard Updates (Week 8)",description:"Introducing MT-Bench and Vicuna-33B",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-week8/"}},{id:"post-chatbot-arena-leaderboard-updates-week-4",title:"Chatbot Arena Leaderboard Updates (Week 4)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-week4/"}},{id:"post-chatbot-arena-leaderboard-updates-week-2",title:"Chatbot Arena Leaderboard Updates (Week 2)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-week2/"}},{id:"post-chatbot-arena",title:"Chatbot Arena",description:"Benchmarking LLMs in the Wild with Elo Ratings",section:"Posts",handler:()=>{window.location.href="/blog/2023/arena/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6C%6D%61%72%65%6E%61.%61%69@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>