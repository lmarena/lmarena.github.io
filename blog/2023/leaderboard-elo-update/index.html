<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Chatbot Arena - New models &amp; Elo system update | Chatbot Arena </title> <meta name="author" content="Chatbot Arena"> <meta name="description" content="an open platform for human preference evaluations"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/bear.png?23fade998cf650fe43c0a84f33581251"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lmarena.github.io/blog/2023/leaderboard-elo-update/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Chatbot Arena - New models & Elo system update",
            "description": "",
            "published": "December 07, 2023",
            "authors": [
              
              {
                "author": "Wei-Lin Chiang",
                "authorURL": "https://infwinston.github.io/",
                "affiliations": [
                  {
                    "name": "UC Berkeley",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Tianle Li",
                "authorURL": "https://codingwithtim.github.io/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Joseph E. Gonzalez",
                "authorURL": "https://people.eecs.berkeley.edu/~jegonzal/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Ion Stoica",
                "authorURL": "https://people.eecs.berkeley.edu/~istoica/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title" href="/"> <span class="font-weight-bold">Chatbot</span> Arena </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/about/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Chatbot Arena - New models &amp; Elo system update</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <p>Welcome to our latest update on the Chatbot Arena, our open evaluation platform to test the most advanced LLMs. We’re excited to share that over <strong>130,000</strong> votes that are now collected to rank the most capable 40+ models! In this blog post, we’ll cover the results of several new models:</p> <ol> <li>Tulu-2-DPO-70B and Yi-34B-Chat are the new SoTA open models</li> <li>Mistral-based 7B models (OpenChat, OpenHermes-2.5, Starling-7B) show promising performance</li> </ol> <p>We also present our findings from differentiating versions of proprietary models (e.g., GPT-4 =&gt; GPT-4-0314, GPT-4-0613), and the transition from the online Elo system to the Bradley-Terry model, which gives us significantly more stable ratings and precise confidence intervals.</p> <p>Let’s dive into it!</p> <h2 id="introducing-new-models">Introducing new models</h2> <p>LLM has become smarter than ever and it’s been a real challenge to evaluate them properly. Traditional benchmarks such as MMLU have been useful, but they may fall short in capturing the nuance of human preference and open-ended nature of real-world conversations. We believe deploying chat models in the real-world to get feedback from users produces the most direct signals. This led to the Chatbot Arena launch in May. Since then, the open-source community has taken off. Over the past few months, we have deployed more than <strong>45 models</strong> in Arena and we’ve collected over <strong>130,000</strong> valid votes from our users. We believe such a scale covers a diverse range of use cases which bring us useful insights to understand how these models work in real-world scenarios.</p> <p>In November, we added record-breaking nine new models with sizes ranging from 7B to 70B, as well as proprietary ones, and gathered over new 25,000 votes for them. Excitingly, we are now seeing the gap between proprietary and open models narrowing. New models such as <strong>Tulu-2-DPO-70B</strong> and <strong>Yi-34B-Chat</strong> have been leading the open space, delivering close to gpt-3.5 performance.</p> <table> <thead> <tr> <th style="text-align: left">Model</th> <th style="text-align: right">Arena Elo Rating</th> <th style="text-align: right">Vote count</th> <th style="text-align: right">License</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><a href="https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo" rel="external nofollow noopener" target="_blank"><strong>GPT-4-Turbo</strong></a></td> <td style="text-align: right">1217</td> <td style="text-align: right">7007</td> <td style="text-align: right">Proprietary</td> </tr> <tr> <td style="text-align: left"><a href="https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo" rel="external nofollow noopener" target="_blank">GPT-4-0613</a></td> <td style="text-align: right">1153</td> <td style="text-align: right">11944</td> <td style="text-align: right">Proprietary</td> </tr> <tr> <td style="text-align: left"><a href="https://www.anthropic.com/index/claude-2-1" rel="external nofollow noopener" target="_blank"><strong>Claude-2.1</strong></a></td> <td style="text-align: right">1118</td> <td style="text-align: right">5929</td> <td style="text-align: right">Proprietary</td> </tr> <tr> <td style="text-align: left"><a href="https://platform.openai.com/docs/models/gpt-3-5" rel="external nofollow noopener" target="_blank">GPT-3.5-Turbo-0613</a></td> <td style="text-align: right">1112</td> <td style="text-align: right">15974</td> <td style="text-align: right">Proprietary</td> </tr> <tr> <td style="text-align: left"><a href="https://www.anthropic.com/index/releasing-claude-instant-1-2" rel="external nofollow noopener" target="_blank">Claude-instant-1</a></td> <td style="text-align: right">1108</td> <td style="text-align: right">5929</td> <td style="text-align: right">Proprietary</td> </tr> <tr> <td style="text-align: left"><a href="https://huggingface.co/allenai/tulu-2-dpo-70b" rel="external nofollow noopener" target="_blank"><strong>Tulu-2-DPO-70B</strong></a></td> <td style="text-align: right">1105</td> <td style="text-align: right">2922</td> <td style="text-align: right">AI2 ImpACT Low-risk</td> </tr> <tr> <td style="text-align: left"><a href="https://huggingface.co/01-ai/Yi-34B-Chat" rel="external nofollow noopener" target="_blank"><strong>Yi-34B-Chat</strong></a></td> <td style="text-align: right">1102</td> <td style="text-align: right">3123</td> <td style="text-align: right">Yi License</td> </tr> <tr> <td style="text-align: left"><a href="https://huggingface.co/WizardLM/WizardLM-70B-V1.0" rel="external nofollow noopener" target="_blank">Wizardlm-70B</a></td> <td style="text-align: right">1096</td> <td style="text-align: right">5865</td> <td style="text-align: right">Llama 2 Community</td> </tr> <tr> <td style="text-align: left"><a href="https://huggingface.co/lmsys/vicuna-33b-v1.3" rel="external nofollow noopener" target="_blank">Vicuna-33B</a></td> <td style="text-align: right">1093</td> <td style="text-align: right">11671</td> <td style="text-align: right">Non-commercial</td> </tr> <tr> <td style="text-align: left"><a href="https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha" rel="external nofollow noopener" target="_blank"><strong>Starling-LM-7B-alpha</strong></a></td> <td style="text-align: right">1083</td> <td style="text-align: right">2250</td> <td style="text-align: right">CC-BY-NC-4.0</td> </tr> <tr> <td style="text-align: left"><a href="https://blog.perplexity.ai/blog/introducing-pplx-online-llms" rel="external nofollow noopener" target="_blank"><strong>PPLX-70B-Online</strong></a></td> <td style="text-align: right">1080</td> <td style="text-align: right">1500</td> <td style="text-align: right">Proprietary</td> </tr> <tr> <td style="text-align: left"><a href="https://huggingface.co/openchat/openchat_3.5" rel="external nofollow noopener" target="_blank"><strong>OpenChat-3.5</strong></a></td> <td style="text-align: right">1077</td> <td style="text-align: right">4662</td> <td style="text-align: right">Apache-2.0</td> </tr> <tr> <td style="text-align: left"><a href="https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B" rel="external nofollow noopener" target="_blank"><strong>Openhermes-2.5-mistral-7B</strong></a></td> <td style="text-align: right">1075</td> <td style="text-align: right">1180</td> <td style="text-align: right">Apache-2.0</td> </tr> <tr> <td style="text-align: left"><a href="https://huggingface.co/meta-llama/Llama-2-70b-chat-hf" rel="external nofollow noopener" target="_blank">Llama-2-70B-chat</a></td> <td style="text-align: right">1069</td> <td style="text-align: right">8659</td> <td style="text-align: right">Llama 2 Community</td> </tr> <tr> <td style="text-align: left"><a href="https://huggingface.co/HuggingFaceH4/zephyr-7b-beta" rel="external nofollow noopener" target="_blank">Zephyr-7B-beta</a></td> <td style="text-align: right">1045</td> <td style="text-align: right">8412</td> <td style="text-align: right">MIT</td> </tr> <tr> <td style="text-align: left"><a href="https://blog.perplexity.ai/blog/introducing-pplx-online-llms" rel="external nofollow noopener" target="_blank"><strong>PPLX-7B-Online</strong></a></td> <td style="text-align: right">1016</td> <td style="text-align: right">1041</td> <td style="text-align: right">Proprietary</td> </tr> </tbody> </table> <p>On the other hand, 7B models have also shown significant improvements. Fine-tuning the 7B Mistral model has led to Zephyr, OpenChat-3.5, Starling-lm-7b-alpha, and OpenHermes-2.5-Mistral-7b which all demonstrate impressive performance despite smaller scale. Shoutout to the open-source community pushing limits! On the other hand, to understand how freshness and grounded information help LLMs in answering user queries, we also bring Perplexity AI’s online LLMs to Arena. We have collected over 1500 votes for PPLX-70B-Online and the preliminary results show great potential. Congrats to all the teams and we look forward to seeing more models in the future!</p> <p>Please find the latest leaderboard <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard" rel="external nofollow noopener" target="_blank">here</a> or try <a href="https://lmarena.ai" rel="external nofollow noopener" target="_blank">Arena demo</a> to chat with 20+ models! We also prepare a <a href="https://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH" rel="external nofollow noopener" target="_blank">notebook</a> to reproduce all the calculation of Elo ratings and confidence intervals.</p> <p><img src="/assets/img/blog/leaderboard_202312/mle_elo.png" style="display:block; margin:auto; max-width:100%; height:auto;"></p> <h2 id="tracking-performance-of-proprietary-apis---gpt-4-0314-vs-0613">Tracking Performance of Proprietary APIs - GPT-4-0314 vs 0613?</h2> <p>Since OpenAI’s GPT-4 update in June, the community has been wondering whether there’s a performance change on the newer version of GPT-4. Some people find performance drop in certain domains (<a href="https://x.com/matei_zaharia/status/1681467961905926144?s=20" rel="external nofollow noopener" target="_blank">reference</a>), but it’s still unclear what’s really going on. Previously we combined votes of the two versions into just GPT-4. As we transition from online Elo to the BT model (explained later in the post), we decide to separate out different versions of proprietary model APIs to better satisfy its assumptions on model staying static.</p> <p><img src="/assets/img/blog/leaderboard_202312/gpt_version.png" style="display:block; margin:auto; max-width:100%; height:auto;"></p> <p>Surprisingly, we observe a significant difference between <code class="language-plaintext highlighter-rouge">gpt-4-0314</code> and <code class="language-plaintext highlighter-rouge">gpt-4-0613</code> (Rating 1201 vs 1152) based on Arena user preference. The GPT-4 API was automatically updated from 0314 to 0613 on June 27 and the 0314 version has since then been retired from Arena. Potential hypotheses:</p> <ol> <li>Arena user distribution has shifted before/after July (e.g., prompt distribution, voting behaviors etc)</li> <li>No comparison data for 0314 against newly added models after July may be unfair.</li> <li>Arena users indeed prefer the 0314 version of GPT-4 than 0613.</li> </ol> <p>To address this problem, we have brought up <code class="language-plaintext highlighter-rouge">gpt-4-0314</code> online again to collect new votes, also directly comparing it against its newer 0613 version. At the time of writing we have collected 1,000 new votes for <code class="language-plaintext highlighter-rouge">gpt-4-0314</code> and its performance is still robust from winrate over other models shown below. We’ll give more updates on this in the future.</p> <p><img src="/assets/img/blog/leaderboard_202312/gpt4_winrate.png" style="display:block; margin:auto; max-width:80%; height:auto;"></p> <p>Interestingly, gpt-3.5-turbo, which has been through a similar version change (0314 -&gt; 0613), seems to be normal. As you can see, <code class="language-plaintext highlighter-rouge">gpt-3.5-turbo-0613</code> has slightly higher rating than <code class="language-plaintext highlighter-rouge">gpt-3.5-turbo-0314</code> (1112 vs 1106). However, we again observe a strange performance drop of the latest version <code class="language-plaintext highlighter-rouge">gpt-3.5-turbo-1106</code> which has obtained over 5,000 votes. We hope to investigate this deeper by developing new tools to analyze user prompts and identify model strengths and weaknesses in different areas.</p> <h2 id="transition-from-online-elo-rating-system-to-bradley-terry-model">Transition from online Elo rating system to Bradley-Terry model</h2> <p>We adopted the Elo rating system for ranking models since the launch of the Arena. It has been useful to transform pairwise human preference to Elo ratings that serve as a predictor of winrate between models. Specifically, if player A has a rating of $R_A$ and player B a rating of $R_B$, the probability of player A winning is</p> <p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7c80282e9c95e92d6b210467aab48a8c4c81ef10" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;"></p> <p>ELO rating has been used to rank chess players by the international community for over 60 years. Standard Elo rating systems assume a player’s performance changes overtime. So an online algorithm is needed to capture such dynamics, meaning recent games should weigh more than older games. Specifically, after each game, a player’s rating is updated according to the difference between predicted outcome and actual outcome.</p> <p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1cad9fb1cfc6a8e845493ac9a40eb98541a4641a" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;"></p> <p>This algorithm has two distinct features:</p> <ol> <li>It can be computed asynchronously by players around the world.</li> <li>It allows for players performance to change dynamically – it does not assume a fixed unknown value for the players rating.</li> </ol> <p>This ability to adapt is determined by the parameter K which controls the magnitude of rating changes that can affect the overall result. A larger K essentially put more weight on the recent games, which may make sense for new players whose performance improves quickly. However as players become more senior and their performance “converges” then a smaller value of K is more appropriate. As a result, USCF adopted K based on the number of games and tournaments completed by the player (<a href="https://new.uschess.org/sites/default/files/media/documents/the-us-chess-rating-system-revised-september-2020.pdf" rel="external nofollow noopener" target="_blank">reference</a>). That is, the Elo rating of a senior player changes slower than a new player.</p> <p>When we launched the Arena, we noticed considerable variability in the ratings using the classic online algorithm. We tried to tune the K to be sufficiently stable while also allowing new models to move up quickly in the leaderboard. We ultimately decided to adopt a bootstrap-like technique to shuffle the data and sample Elo scores from 1000 permutations of the online plays. You can find the details in this <a href="https://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH" rel="external nofollow noopener" target="_blank">notebook</a>. This provided consistent stable scores and allowed us to incorporate new models quickly. This is also observed in a recent <a href="https://arxiv.org/abs/2311.17295" rel="external nofollow noopener" target="_blank">work</a> by Cohere. However, we used the same samples to estimate confidence intervals which were therefore too wide (effectively CI’s for the original online Elo estimates).</p> <p>In the context of LLM ranking, there are two important differences from the classic Elo chess ranking system. First, we have access to the entire history of all games for all models and so we don’t need a decentralized algorithm. Second, most models are static (we have access to the weights) and so we don’t expect their performance to change. However, it is worth noting that the hosted proprietary models may not be static and their behavior can change without notice. We try our best to pin specific model API versions if possible.</p> <p>To improve the quality of our rankings and their confidence estimates, we are adopting another widely used rating system called the <a href="https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model" rel="external nofollow noopener" target="_blank">Bradley–Terry</a> (BT) model. This model actually is the maximum likelihood (MLE) estimate of the underlying Elo model assuming a fixed but unknown pairwise win-rate. Similar to Elo rating, BT model is also based on pairwise comparison to derive ratings of players to estimate win rate between each other. The core difference between BT model vs the online Elo system is the assumption that player’s performance does not change (i.e., game order does not matter) and the computation takes place in a centralized fashion.</p> <p>With the static performance assumption, the model ratings can be obtained by maximum likelihood estimation (MLE), i.e. maximizing the likelihood of the observed game outcomes given the model ratings. Code snippet below shows how to use MLE to compute the model ratings.</p> <p><img src="/assets/img/blog/leaderboard_202312/mle_code.png" style="display:block; margin:auto; max-width:100%; height:auto;"></p> <p>Similarly, we can also bootstrap the MLE Bradley-Terry scores to obtain the confidence intervals of model ratings. We observe that the mean rating by both methods are very similar and the rankings are almost the same.</p> <p><img src="/assets/img/blog/leaderboard_202312/elo_vs_bt.png" style="display:block; margin:auto; max-width:100%; height:auto;"></p> <p>More importantly, with the BT model, the bootstrap confidence intervals now better capture the variance of the model performance estimates. We observe clear improvement in the below figures. Newly added models with fewer votes have a wider range of confidence intervals than others.</p> <table> <thead> <tr> <th>Bootstraping Online Elo</th> </tr> </thead> <tbody> <tr> <td><img src="/assets/img/blog/leaderboard_202312/online_elo.png" style="display:block; max-width:100%;"></td> </tr> <tr> <td><strong>Bootstraping MLE Elo (BT model)</strong></td> </tr> <tr> <td><img src="/assets/img/blog/leaderboard_202312/mle_elo.png" style="display:block; max-width:100%;"></td> </tr> </tbody> </table> <p>Note that we extend BT model to consider ties by counting a tie as half a win and half a loss. Code to reproduce the calculation can be found at this <a href="https://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH" rel="external nofollow noopener" target="_blank">notebook</a>.</p> <h3 id="bonus-topic-modeling-on-user-prompts">Bonus: Topic modeling on user prompts</h3> <p>We’ve also conducted topic modeling on 50,000 user prompts to better understand how users interact with these models. Our approach utilized OpenAI embeddings <code class="language-plaintext highlighter-rouge">text-embedding-ada-002</code> and K-means clustering, followed by GPT-4 to summarize the topics for each cluster, provided with the prompts close to the center. This analysis revealed a wide range of topics, from role-playing, story writing to programming advice. We show the topic distribution and a few examples below.</p> <p><img src="/assets/img/blog/leaderboard_202312/topic_distribution_bar.png" style="display:block; margin:auto; max-width:90%; height:auto;"></p> <style>.foo table th:first-of-type{width:10%}.foo table th:nth-of-type(2){width:90%}</style> <div class="foo"> | Cluster ID | Arena User Prompt | | ---------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | 1 | You are a Chief information Officer for a Biotechnology Manufacturing company and will act like one. Write a business need and objectives for a case study to Engage Info-Tech technical consulting services to conduct a comprehensive assessment of our current application development practices, including analyzing our development methodologies, tools, and frameworks. | | 2 | Write a short scene from a novel where a beautiful, wicked lamia coils around an unfortunate, quippy human adventurer. | | 3 | How should the balance be struck between freedom of speech and the ability to function in a world without continual distractions and distortions from misinformation? | | 4 | Can you give me a list of 5 suggestions on how to write software with fewer bugs? | </div> <p>Moving forward, we aim to refine our methods to filter out low-quality prompts and improve categorization for a clearer understanding of model strengths and weaknesses in different areas.</p> <h2 id="next-steps">Next steps</h2> <p>We plan to ship real-time leaderboard update, diving deeper into user prompt analysis, and enhancing prompt moderation and categorization. Stay tuned for more insights as we continue to refine our approach to evaluating the evolving landscape of LLMs. Thanks for supporting us on this journey, and we look forward to sharing more updates soon!</p> <h2 id="links">Links</h2> <ul> <li><a href="https://lmarena.ai/" rel="external nofollow noopener" target="_blank">Chatbot Arena Demo</a></li> <li><a href="https://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH#scrollTo=mukqgshMarFi" rel="external nofollow noopener" target="_blank">Arena Elo Colab</a></li> <li><a href="https://arxiv.org/abs/2307.09009" rel="external nofollow noopener" target="_blank">How Is ChatGPT’s Behavior Changing over Time?</a></li> <li>Bradley-Terry model <a href="https://web.stanford.edu/class/archive/stats/stats200/stats200.1172/Lecture24.pdf" rel="external nofollow noopener" target="_blank">lecture note</a>, <a href="https://www.jstor.org/stable/2334029" rel="external nofollow noopener" target="_blank">paper</a> </li> <li><a href="https://arxiv.org/abs/2311.17295" rel="external nofollow noopener" target="_blank">Elo Uncovered: Robustness and Best Practices in Language Model Evaluation</a></li> </ul> <p>If you wish to see more models on Arena leaderboard, we invite you to <a href="https://github.com/lm-sys/FastChat/blob/main/docs/arena.md#how-to-add-a-new-model" rel="external nofollow noopener" target="_blank">contribute to FastChat</a> or <a href="mailto:lmsysorg@gmail.com">contact us</a> to provide us with API access.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"lmarena/lmarena.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Chatbot Arena. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-",title:"",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-about",title:"about",description:"",section:"Navigation",handler:()=>{window.location.href="/about/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-statistical-extensions-of-the-bradley-terry-and-elo-models",title:"Statistical Extensions of the Bradley-Terry and Elo Models",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/extended-arena/"}},{id:"post-chatbot-arena-new-blog",title:"Chatbot Arena New Blog",description:"A new chapter for Chatbot Arena!",section:"Posts",handler:()=>{window.location.href="/blog/2024/new-site/"}},{id:"post-redteam-arena",title:"RedTeam Arena",description:"An Open-Source, Community-driven Jailbreaking Platform",section:"Posts",handler:()=>{window.location.href="/blog/2024/redteam-arena/"}},{id:"post-does-style-matter",title:"Does Style Matter?",description:"Disentangling style and substance in Chatbot Arena",section:"Posts",handler:()=>{window.location.href="/blog/2024/style-control/"}},{id:"post-the-multimodal-arena-is-here",title:"The Multimodal Arena is Here!",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/multimodal/"}},{id:"post-introducing-hard-prompts-category-in-chatbot-arena",title:"Introducing Hard Prompts Category in Chatbot Arena",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/hard-prompts/"}},{id:"post-what-39-s-up-with-llama-3-arena-data-analysis",title:"What&#39;s up with Llama 3? Arena data analysis",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/llama3/"}},{id:"post-lmsys-chatbot-arena-kaggle-competition",title:"LMSYS Chatbot Arena Kaggle Competition",description:"Predicting Human Preference with $100,000 in Prizes",section:"Posts",handler:()=>{window.location.href="/blog/2024/kaggle-competition/"}},{id:"post-from-live-data-to-high-quality-benchmarks-the-arena-hard-pipeline",title:"From Live Data to High-Quality Benchmarks - The Arena-Hard Pipeline",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/arena-hard/"}},{id:"post-chatbot-arena-policy-update",title:"Chatbot Arena Policy Update",description:"Live and Community-Driven LLM Evaluation",section:"Posts",handler:()=>{window.location.href="/blog/2024/policy/"}},{id:"post-chatbot-arena-new-models-amp-elo-system-update",title:"Chatbot Arena - New models &amp; Elo system update",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-elo-update/"}},{id:"post-catch-me-if-you-can-how-to-beat-gpt-4-with-a-13b-model",title:"Catch me if you can! How to beat GPT-4 with a 13B model...",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/llm-decontaminator/"}},{id:"post-chatbot-arena-conversation-dataset-release",title:"Chatbot Arena Conversation Dataset Release",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/dataset/"}},{id:"post-chatbot-arena-leaderboard-updates-week-8",title:"Chatbot Arena Leaderboard Updates (Week 8)",description:"Introducing MT-Bench and Vicuna-33B",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-week8/"}},{id:"post-chatbot-arena-leaderboard-updates-week-4",title:"Chatbot Arena Leaderboard Updates (Week 4)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-week4/"}},{id:"post-chatbot-arena-leaderboard-updates-week-2",title:"Chatbot Arena Leaderboard Updates (Week 2)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-week2/"}},{id:"post-chatbot-arena",title:"Chatbot Arena",description:"Benchmarking LLMs in the Wild with Elo Ratings",section:"Posts",handler:()=>{window.location.href="/blog/2023/arena/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6C%6D%61%72%65%6E%61.%61%69@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>