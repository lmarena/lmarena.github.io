<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Copilot Arena | LM Arena </title> <meta name="author" content="LM Arena"> <meta name="description" content="Copilot Arena's Initial Leaderboard, Insights, and a New Prompting Method for Code Completions"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/bear.png?23fade998cf650fe43c0a84f33581251"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lmarena.github.io/blog/2024/copilot-arena/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Copilot Arena",
            "description": "Copilot Arena's Initial Leaderboard, Insights, and a New Prompting Method for Code Completions",
            "published": "November 12, 2024",
            "authors": [
              
              {
                "author": "Wayne Chi",
                "authorURL": "https://waynchi.github.io",
                "affiliations": [
                  {
                    "name": "CMU, UC Berkeley",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Valerie Chen",
                "authorURL": "https://valeriechen.github.io/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Anastasios N. Angelopoulos",
                "authorURL": "http://angelopoulos.ai",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Wei-Lin Chiang",
                "authorURL": "https://infwinston.github.io/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Naman Jain",
                "authorURL": "https://naman-ntc.github.io/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Tianjun Zhang",
                "authorURL": "https://tianjunz.github.io/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Ion Stoica",
                "authorURL": "https://people.eecs.berkeley.edu/~istoica/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Chris Donahue",
                "authorURL": "https://chrisdonahue.com/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Ameet Talwalkar",
                "authorURL": "https://www.cs.cmu.edu/~atalwalk/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title" href="/"> <span class="font-weight-bold">LM</span> Arena </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/about/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Copilot Arena</h1> <p>Copilot Arena's Initial Leaderboard, Insights, and a New Prompting Method for Code Completions</p> </d-title> <d-byline></d-byline> <d-article> <h2 id="introduction">Introduction</h2> <p>As LLMs are embedded more and more within production workflows, it’s time to rethink how we measure LLM capabilities to better reflect real-world usage. A few weeks ago, we launched <a href="https://marketplace.visualstudio.com/items?itemName=copilot-arena.copilot-arena&amp;ssr=false#overview" rel="external nofollow noopener" target="_blank">Copilot Arena</a>, a <strong>free</strong> AI coding assistant that provides paired responses from different state-of-the-art LLMs. We first introduced paired code completions and more recently rolled out inline editing-a feature where users can highlight code segments, write a prompt, and receive two diff-based suggestions for modifying that code.</p> <p>Thus far, Copilot Arena has been downloaded 2.5K times on the VSCode Marketplace, served over 100K completions, and accumulated over 10K code completion battles. In this blog post, we’ll cover:</p> <ul> <li> <a href="#initial-leaderboard-and-results">Initial Leaderboard and Results</a>. Our preliminary results for the code completions leaderboard and analysis of model tiers.</li> <li> <a href="#how-do-people-use-copilot-arena">Copilot Arena Usage</a>. Analysis on Copilot Arena usage, including the distribution of coding languages, context lengths, and an initial inspection into position bias.</li> <li> <a href="#how-do-we-prompt-chat-models-to-perform-code-completions">Prompting</a>. Details of how we use chat models like Claude Sonnet 3.5 and GPT-4o to perform code completions (spoiler alert, we generate code snippets and post-process!).</li> </ul> <h2 id="initial-leaderboard-and-results">Initial Leaderboard and Results</h2> <p>As an initial set of models, we selected 9 of the best models across multiple model providers that include both open, code-specific, and commercial models. To ensure a fair comparison between models, we do the following…</p> <ul> <li>We randomize whether models appear at the top or bottom for each completion along with which models are paired for each battle.</li> <li>We show both completions at the same time. This means that a faster model completion needs to wait for the slower model.</li> <li>Many of the models with superior coding capabilities are chat models. As discussed later in the post, we optimize the prompts so they can perform code completions. We manually verified there were no significant formatting issues in the data collected.</li> <li>We set the same max number of output tokens, input tokens, top-p, and temperature (unless specified by the model provider).</li> </ul> <div style="margin-left: auto; margin-right: auto; width: fit-content;"> <table class="tg"> <thead> <tr> <th>Model</th> <th style="text-align: center;">Arena Score</th> <th style="text-align: center;">Confidence Intervals</th> <th style="text-align: center;">Median Latency (s)</th> </tr> </thead> <tbody> <tr style="background-color: #EFBF04; color: black"> <td>Deepseek V2.5</td> <td style="text-align: center;">1074</td> <td style="text-align: center;">+16/-11</td> <td style="text-align: center;">2.13</td> </tr> <tr style="background-color: #EFBF04; color: black"> <td>Claude Sonnet 3.5 (06/20)</td> <td style="text-align: center;">1053</td> <td style="text-align: center;">+18/-17</td> <td style="text-align: center;">2.29</td> </tr> <tr style="background-color: #C0C0C0; color: black"> <td>Codestral (05/24)</td> <td style="text-align: center;">1046</td> <td style="text-align: center;">+12/-10</td> <td style="text-align: center;">1.01</td> </tr> <tr style="background-color: #C0C0C0; color: black"> <td>Meta-Llama-3.1-405B-Instruct</td> <td style="text-align: center;">1024</td> <td style="text-align: center;">+17/-15</td> <td style="text-align: center;">1.12</td> </tr> <tr style="background-color: #CD7F32; color: black"> <td>GPT-4o (08/06)</td> <td style="text-align: center;">1016</td> <td style="text-align: center;">+17/-20</td> <td style="text-align: center;">0.75</td> </tr> <tr style="background-color: #CD7F32; color: black"> <td>Gemini-1.5-Pro-002</td> <td style="text-align: center;">1014</td> <td style="text-align: center;">+19/-18</td> <td style="text-align: center;">1.44</td> </tr> <tr style="background-color: #CD7F32; color: black"> <td>Meta-Llama-3.1-70B-Instruct</td> <td style="text-align: center;">1013</td> <td style="text-align: center;">+14/-15</td> <td style="text-align: center;">0.88</td> </tr> <tr style="background-color: #CD7F32; color: black"> <td>Gemini-1.5-Flash-002</td> <td style="text-align: center;">1005</td> <td style="text-align: center;">+16/-22</td> <td style="text-align: center;">0.55</td> </tr> <tr style="background-color: #E8E8E8; color: black"> <td>GPT-4o-mini (07/18)</td> <td style="text-align: center;">962</td> <td style="text-align: center;">+17/-15</td> <td style="text-align: center;">0.74</td> </tr> </tbody> </table> </div> <p style="color:gray; text-align: center;">Table 1. Elo ratings and median latency of nine popular models based on over 10K votes collected between October 16-November 11, 2024. We color rows based on tiers determined by confidence intervals. Each model has at least 1K votes.</p> <p>Table 1 presents the current code completion leaderboard and stratifies them into tiers. Here are our main takeaways:</p> <ul> <li>With a minor prompt tweak, Claude is able to compete with code-specific models (e.g., Deepseek V2.5) on code completion tasks, including ones that require “fill-in-the-middle”. From the beginning, we observed that these Claude and DeepSeek models have emerged as top contenders and separated themselves from the rest of the pack.</li> <li>Within a tier, we still observe slight fluctuations as we obtain more votes. Check out Figure 2 for a breakdown of the win rate percentage for each pair of models.</li> <li>We find that GPT-4o-mini is much worse than all other models.</li> </ul> <p>We follow the same leaderboard computation as the latest version of Chatbot Arena, which is based on learning Bradley-Terry coefficients that minimize loss when predicting whether one model will beat the other. Please check out <a href="https://blog.lmarena.ai/blog/2024/extended-arena/" rel="external nofollow noopener" target="_blank">this blog post</a> for a more in-depth description.</p> <p><img src="/assets/img/blog/copilot_arena/winrate_matrix.png" alt="Model win rate matrix" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 90%"></p> <p style="color:gray; text-align: center;">Figure 2. Fraction of model A wins for all battles</p> <h3 id="the-effect-of-latency">The Effect of Latency</h3> <p>While the Arena scores (Table 1) do not explicitly factor in model latency since both completions are shown simultaneously, we explore whether Arena Scores correlate with latency. We include median latency as a separate column in the results. In general, we find that people don’t necessarily prefer faster models. However, this may be partially because code completions are only generated in Copilot Arena after a user pauses.</p> <h2 id="how-do-people-use-copilot-arena">How do people use Copilot Arena?</h2> <p><strong>What kind of languages do people code in?</strong><br> Most current Copilot Arena users code in Python, followed by javascript/typescript, html/markdown, and C++. This statistic is determined based on the file extension.</p> <p><img src="/assets/img/blog/copilot_arena/filetype_dist.png" alt="Copilot Arena filetype distribution" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 90%"></p> <p style="color:gray; text-align: center;">Figure 3. Filetypes requested in Copilot Arena. Filetypes are determined based on file extension.</p> <p><strong>What kind of context lengths are we looking at?</strong><br> The mean context length is 1002 tokens and the median is 560 tokens. This is much longer than tasks considered in existing static benchmarks. For example, human eval has a median length of ~100 tokens.</p> <p><img src="/assets/img/blog/copilot_arena/context_length_dist.png" alt="Copilot Arena context length distribution" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 90%"></p> <p style="color:gray; text-align: center;">Figure 4. Context length of files requested in Copilot Arena.</p> <p><strong>Are people biased towards the top completion?</strong> Yes. In fact, 82% of accepted completions were the top completion. We are still analyzing our data, but here are some of our insights.</p> <ul> <li> <em>Are people even reading the completions? Or are they just instinctively pressing Tab?</em> Users take a median of 7 seconds to view the response and then select a response. As such, we believe that people are indeed reading the completions.</li> </ul> <p><img src="/assets/img/blog/copilot_arena/vote_times.png" alt="Copilot Arena Vote time distribution" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 90%"></p> <p style="color:gray; text-align: center;">Figure 5. Distribution of user response times. Most users are taking a few seconds to read the responses.</p> <ul> <li> <em>Does position bias affect models equally?</em> Surprisingly, no! For example, when shown as the bottom completion, Sonnet-3.5 is accepted 23.4% of the time compared to only 12.8% of the time for Gemini Flash. On the other hand, they are accepted at roughly the same rate when shown as the top completion (86.7% vs 85% respectively). We are still exploring the reasons behind this phenomenon and will continue our analysis in a future post.</li> </ul> <p><strong>How many people are regular users?</strong> In total, we have had votes from 833 unique users and between 200-250 daily active users.</p> <p><strong>How do you handle ties in Arena?</strong> We do not currently have an option for people to select that both responses are equally good (or bad).</p> <p><strong>How do you handle models pre-trained on FiM?</strong> For Deepseek V2.5 and Codestral, we use their API which directly allows for FiM capabilities.</p> <h2 id="how-do-we-prompt-chat-models-to-perform-code-completions">How Do We Prompt Chat Models to Perform Code Completions?</h2> <p><img src="/assets/img/blog/copilot_arena/instruct_error.png" alt="Copilot Arena filetype distribution" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 90%"></p> <p style="color:gray; text-align: center;">Figure 6. (Top) Example of code completion that requires infilling capabilities. (Bottom) Example of formatting issue that chat models encounters when prompted to complete code given the prefix and suffix.</p> <p>During real development processes, developers frequently modify or expand on existing code, rather than only write code in a left-to-right manner. As such, <a href="https://arxiv.org/abs/2204.05999" rel="external nofollow noopener" target="_blank">“fill in the middle”</a> (FiM) capabilities when generating code completions are critical for any models to be used in Copilot Arena. Many code-specific models, including DeepSeek and Codestral, are specifically trained to perform FiM. However, most models in Copilot Arena are not because they are chat models, and thus, they struggle to appropriately format a completion when provided with the prefix and suffix. We explore a simple prompting trick that allows chat models to perform code completions with high success.</p> <div style="margin-left: auto; margin-right: auto; width: fit-content;"> <table class="tg"> <thead> <tr> <th>Model</th> <th style="text-align: center;">PSM</th> <th style="text-align: center;">SPM</th> <th style="text-align: center;">Mask</th> </tr> </thead> <tbody> <tr> <td>Claude-3.5-sonnet</td> <td style="text-align: center;">0.67 (+0.16)</td> <td style="text-align: center;">0.66 (+0.15)</td> <td style="text-align: center;">0.66 (+0.14)</td> </tr> <tr> <td>GPT-4o-2024-08-06</td> <td style="text-align: center;">0.71 (+0.02)</td> <td style="text-align: center;">0.55 (+0.19)</td> <td style="text-align: center;">0.62 (+0.12)</td> </tr> <tr> <td>GPT-4o-mini-2024-07-18</td> <td style="text-align: center;">0.18 (+0.39)</td> <td style="text-align: center;">0.12 (+0.54)</td> <td style="text-align: center;">0.15 (+0.36)</td> </tr> <tr> <td>Gemini-1.5-pro-001</td> <td style="text-align: center;">0.38 (+0.28)</td> <td style="text-align: center;">0.34 (+0.36)</td> <td style="text-align: center;">0.43 (-0.04)</td> </tr> <tr> <td>Gemini-1.5-flash-001</td> <td style="text-align: center;">0.34 (+0.24)</td> <td style="text-align: center;">0.27 (+0.37)</td> <td style="text-align: center;">0.36 (+0.19)</td> </tr> <tr> <td>Llama-3.1-70B-Instruct</td> <td style="text-align: center;">0.14 (+0.46)</td> <td style="text-align: center;">0.15 (+0.48)</td> <td style="text-align: center;">0.12 (+0.27)</td> </tr> </tbody> </table> </div> <p style="color:gray; text-align: center;">Table 2: Percentage of well-formatted code completions with different prompt templates (PSM, SPM, Mask). We denote the gain by our prompting method in parentheses.</p> <p><strong>Evaluation Set-up.</strong> To verify that chat models would indeed struggle to perform FiM, we use the <a href="https://github.com/openai/human-eval-infilling" rel="external nofollow noopener" target="_blank">HumanEval-infilling</a> dataset as an imperfect proxy to benchmark chat models’ FiM capabilities. We adopt three prompt templates considered in prior work (e.g., <a href="https://arxiv.org/abs/2403.04814" rel="external nofollow noopener" target="_blank">Gong et al.</a>) like Prefix-suffix-middle (PSM), Suffix-prefix-middle (SPM), and Mask. Instead of measuring pass@1, we only consider whether the returned infilled code is formatted correctly.</p> <p><strong>Chat models can’t naively FiM.</strong> Table 2 shows that standard prompt templates are insufficient for chat models to complete FiM tasks. This is not necessarily an indication that models cannot code as clearly many SOTA chat models are proficient coders. Instead, the vast majority of the errors resulted from issues in formatting or duplicate code segments rather than logical errors, indicating that these models cannot generalize their code outputs to FiM tasks. While it is not feasible to retrain these models, we explore alternative approaches via prompting to enable models to complete FiM tasks.</p> <p><strong>Our solution significantly reduces formatting errors.</strong> Instead of forcing chat models to output code in a format unaligned with its training (e.g. FiM), we allow the model to generate code snippets, which is a more natural format, and then post-process them into a FiM completion. Our approach is as follows: in addition to the same prompt templates above, the models are provided with instructions to begin by re-outputting a portion of the prefix and similarly end with a portion of the suffix. We then match portions of the output code in the input and delete the repeated code. As you can see in Table 2, the models make much fewer formatting issues. These benefits hold regardless of the prompt template.</p> <h3 id="whats-next">What’s next?</h3> <ul> <li> <strong>More results and analyses:</strong> We’re working on adding new metrics that will provide more insight into how useful models are (e.g., code persistence), and more fine-grain analyses to understand why some models are preferred by users over others. We will also release a leaderboard for inline editing once we have collected enough votes!</li> <li> <strong>Opportunities to contribute to Copilot Arena:</strong> While our initial prompting efforts are proving to be usable in practice, we welcome suggestions for prompt improvements. In general, we are always looking to improve Copilot Arena. Ping us to get involved!</li> </ul> <h2 id="citation">Citation</h2> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">chi2024copilot</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{Copilot Arena}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Wayne Chi and Valerie Chen and Wei-Lin Chiang and Anastasios N. Angelopoulos and Naman Jain and Tianjun Zhang and Ion Stoica and Chris Donahue and Ameet Talwalkar}</span>
    <span class="nv">year</span><span class="err">={2024</span><span class="p">}</span><span class="c">,</span>
<span class="c">}</span>
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"lmarena/lmarena.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 LM Arena. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-",title:"",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-about",title:"about",description:"",section:"Navigation",handler:()=>{window.location.href="/about/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-webdev-arena",title:"WebDev Arena",description:"A Live LLM Leaderboard for Web App Development",section:"Posts",handler:()=>{window.location.href="/blog/2025/webdev-arena/"}},{id:"post-repochat-arena",title:"RepoChat Arena",description:"A Live Benchmark for AI Software Engineers",section:"Posts",handler:()=>{window.location.href="/blog/2025/repochat-arena/"}},{id:"post-arena-explorer",title:"Arena Explorer",description:"A topic modeling pipeline for LLM evals &amp; analytics",section:"Posts",handler:()=>{window.location.href="/blog/2025/arena-explorer/"}},{id:"post-code-editing-in-copilot-arena",title:"Code Editing in Copilot Arena",description:"Copilot Arena&#39;s Code Editing Leaderboard and Insights",section:"Posts",handler:()=>{window.location.href="/blog/2025/copilot-arena-edits/"}},{id:"post-copilot-arena",title:"Copilot Arena",description:"Copilot Arena&#39;s Initial Leaderboard, Insights, and a New Prompting Method for Code Completions",section:"Posts",handler:()=>{window.location.href="/blog/2024/copilot-arena/"}},{id:"post-chatbot-arena-categories",title:"Chatbot Arena Categories",description:"Definitions, Methods, and Insights",section:"Posts",handler:()=>{window.location.href="/blog/2024/arena-category/"}},{id:"post-preference-proxy-evaluations",title:"Preference Proxy Evaluations",description:"A New Benchmark for Evaluating Reward Models and LLM Judges",section:"Posts",handler:()=>{window.location.href="/blog/2024/preference-proxy-evaluations/"}},{id:"post-agent-arena",title:"Agent Arena",description:"A Platform for Evaluating and Comparing LLM Agents Across Models, Tools, and Frameworks",section:"Posts",handler:()=>{window.location.href="/blog/2024/agent-arena/"}},{id:"post-statistical-extensions-of-the-bradley-terry-and-elo-models",title:"Statistical Extensions of the Bradley-Terry and Elo Models",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/extended-arena/"}},{id:"post-chatbot-arena-new-blog",title:"Chatbot Arena New Blog",description:"A new chapter for Chatbot Arena!",section:"Posts",handler:()=>{window.location.href="/blog/2024/new-site/"}},{id:"post-redteam-arena",title:"RedTeam Arena",description:"An Open-Source, Community-driven Jailbreaking Platform",section:"Posts",handler:()=>{window.location.href="/blog/2024/redteam-arena/"}},{id:"post-does-style-matter",title:"Does Style Matter?",description:"Disentangling style and substance in Chatbot Arena",section:"Posts",handler:()=>{window.location.href="/blog/2024/style-control/"}},{id:"post-the-multimodal-arena-is-here",title:"The Multimodal Arena is Here!",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/multimodal/"}},{id:"post-introducing-hard-prompts-category-in-chatbot-arena",title:"Introducing Hard Prompts Category in Chatbot Arena",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/hard-prompts/"}},{id:"post-what-39-s-up-with-llama-3-arena-data-analysis",title:"What&#39;s up with Llama 3? Arena data analysis",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/llama3/"}},{id:"post-lmsys-chatbot-arena-kaggle-competition",title:"LMSYS Chatbot Arena Kaggle Competition",description:"Predicting Human Preference with $100,000 in Prizes",section:"Posts",handler:()=>{window.location.href="/blog/2024/kaggle-competition/"}},{id:"post-from-live-data-to-high-quality-benchmarks-the-arena-hard-pipeline",title:"From Live Data to High-Quality Benchmarks - The Arena-Hard Pipeline",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/arena-hard/"}},{id:"post-chatbot-arena-policy",title:"Chatbot Arena Policy",description:"Live and Community-Driven LLM Evaluation",section:"Posts",handler:()=>{window.location.href="/blog/2024/policy/"}},{id:"post-chatbot-arena-new-models-amp-elo-system-update",title:"Chatbot Arena - New models &amp; Elo system update",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-elo-update/"}},{id:"post-catch-me-if-you-can-how-to-beat-gpt-4-with-a-13b-model",title:"Catch me if you can! How to beat GPT-4 with a 13B model...",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/llm-decontaminator/"}},{id:"post-chatbot-arena-conversation-dataset-release",title:"Chatbot Arena Conversation Dataset Release",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/dataset/"}},{id:"post-chatbot-arena-leaderboard-updates-week-8",title:"Chatbot Arena Leaderboard Updates (Week 8)",description:"Introducing MT-Bench and Vicuna-33B",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-week8/"}},{id:"post-chatbot-arena-leaderboard-updates-week-4",title:"Chatbot Arena Leaderboard Updates (Week 4)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-week4/"}},{id:"post-chatbot-arena-leaderboard-updates-week-2",title:"Chatbot Arena Leaderboard Updates (Week 2)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-week2/"}},{id:"post-chatbot-arena",title:"Chatbot Arena",description:"Benchmarking LLMs in the Wild with Elo Ratings",section:"Posts",handler:()=>{window.location.href="/blog/2023/arena/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6C%6D%61%72%65%6E%61.%61%69@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>