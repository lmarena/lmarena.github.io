<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Chatbot Arena Policy | LM Arena </title> <meta name="author" content="LM Arena"> <meta name="description" content="Live and Community-Driven LLM Evaluation"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/vicuna.jpeg?101e2cca9da6907c55807adf8b3b38b7"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lmarena.github.io/blog/2024/policy/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Chatbot Arena Policy",
            "description": "Live and Community-Driven LLM Evaluation",
            "published": "March 01, 2024",
            "authors": [
              
              {
                "author": "Chatbot Arena Team",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "LMArena",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title" href="/"> <span class="font-weight-bold">LM</span> Arena </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/about/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Chatbot Arena Policy</h1> <p>Live and Community-Driven LLM Evaluation</p> </d-title> <d-byline></d-byline> <d-article> <h2 id="our-mission">Our Mission</h2> <p>Chatbot Arena (<a href="https://lmarena.ai" rel="external nofollow noopener" target="_blank">lmarena.ai</a>) is an open-source project created by members from <a href="https://lmarena.ai/?about" rel="external nofollow noopener" target="_blank">LMSYS</a> and UC Berkeley SkyLab. Our mission is to advance LLM development and understanding through live, open, and community-driven evaluations. We maintain the open evaluation platform for any user to rate LLMs via pairwise comparisons under real-world use cases and publish <a href="https://lmarena.ai/?leaderboard" rel="external nofollow noopener" target="_blank">leaderboard</a> periodically.</p> <p><img src="/assets/img/blog/arena_policy/arena_logo_v0_4x3.png" style="margin-left: auto; margin-right: auto; margin-bottom: auto; text-align: center;" width="70%"></p> <h2 id="our-progress">Our Progress</h2> <p>Chatbot Arena was first launched in <a href="https://blog.lmarena.ai/blog/2023/arena/" rel="external nofollow noopener" target="_blank">May 2023</a> and has emerged as a critical platform for live, community-driven LLM evaluation, attracting millions of participants and collecting over 800,000 votes. This extensive engagement has enabled the evaluation of more than 90 LLMs, including both commercial GPT-4, Gemini/Bard and open-weight Llama and Mistral models, significantly enhancing our understanding of their capabilities and limitations.</p> <p>Our periodic <a href="https://lmarena.ai/?leaderboard" rel="external nofollow noopener" target="_blank">leaderboard</a> and blog post updates have become a valuable resource for the community, offering critical insights into model performance that guide the ongoing development of LLMs. Our commitment to open science is further demonstrated through the sharing of <a href="https://huggingface.co/datasets/lmsys/chatbot_arena_conversations" rel="external nofollow noopener" target="_blank">user preference data</a> and <a href="https://huggingface.co/datasets/lmsys/lmsys-chat-1m" rel="external nofollow noopener" target="_blank">one million user prompts</a>, supporting research and model improvement.</p> <p>We also collaborate with open-source and commercial model providers to bring their latest models to community for preview testing. We believe this initiative helps advancing the field and encourages user engagement to collect crucial votes for evaluating all the models in the Arena. Moreover, it provides an opportunity for the community to test and provide anonymized feedback before the models are officially released.</p> <p>The platform’s infrastructure (<a href="https://github.com/lm-sys/FastChat" rel="external nofollow noopener" target="_blank">FastChat</a>) and evaluation tools, available on GitHub, emphasize our dedication to transparency and community engagement in the evaluation process. This approach not only enhances the reliability of our findings but also fosters a collaborative environment for advancing LLMs.</p> <p>In our ongoing efforts, we feel obligated to establish policies that guarantee evaluation transparency and trustworthiness. Moreover, we actively involve the community in shaping any modifications to the evaluation process, reinforcing our commitment to openness and collaborative progress.</p> <h2 id="our-policy">Our Policy</h2> <div style="text-align: right">Last Updated: May 27, 2025</div> <p><strong>Transparent</strong>: The model evaluation and ranking pipelines have been open-sourced in the <a href="https://github.com/lm-sys/FastChat" rel="external nofollow noopener" target="_blank">FastChat</a> repository. We release data collected from the platform as well. Together, this means anyone can audit our leaderboard using publicly released data. The methodology and technical details behind LMArena have been published in a sequence of academic papers (<a href="https://arxiv.org/abs/2403.04132" rel="external nofollow noopener" target="_blank">1</a>, <a href="https://arxiv.org/abs/2306.05685" rel="external nofollow noopener" target="_blank">2</a>, <a href="https://arxiv.org/abs/2502.14855" rel="external nofollow noopener" target="_blank">3</a>). Furthermore, we will involve the community in deciding any changes in the evaluation process.</p> <p><strong>Listing models on the leaderboard</strong>: The public leaderboard will only include models that are generally available to the public. Specifically, models must meet at least one of the following criteria to qualify as <strong>publicly released models</strong>:</p> <ol> <li> <strong>Open Weights</strong>: the model’s weights are publicly accessible.</li> <li> <strong>Public APIs</strong>: The model is accessible via an API (e.g., OpenAI’s GPT-4o, Anthropic’s Claude) with transparent pricing and documentation.</li> <li> <strong>Public Services</strong>: The model is available through a widely accessible public-facing service (e.g., Gemini App, ChatGPT).</li> </ol> <p>Once a publicly released model is listed on the leaderboard, the model will remain accessible at <a href="https://lmarena.ai" rel="external nofollow noopener" target="_blank">lmarena.ai</a> for at least <strong>two weeks</strong> for the community to evaluate it.</p> <p>The leaderboard distinguishes between first-party endpoints and third-party endpoints:</p> <ul> <li> <strong>First-party endpoints</strong>: Endpoints hosted by the model’s original creator (e.g., GPT-4 by OpenAI).</li> <li> <strong>Third-party endpoints</strong>: Endpoints provided by a different entity using models developed by another creator (e.g., a company, other than Meta, offering an endpoint based on Llama).</li> </ul> <p>We prioritize listing first-party endpoints by default but may include third-party endpoints under the following conditions:</p> <ol> <li>The provider must disclose all first-party open-source and proprietary models used in the endpoint (e.g., Llama, GPT-4o).</li> <li>The endpoint must be version-controlled and remain static throughout the period of leaderboard listing.</li> </ol> <p>Third-party endpoints will be explicitly labeled as “third-party” on the leaderboard.</p> <p><strong>Evaluating publicly released models</strong>. Evaluating such a model consists of the following steps:</p> <ol> <li>Add the model to Arena for blind testing and let the community know it was added.</li> <li>Accumulate enough votes until the model’s rating stabilizes.</li> <li>Once the model’s rating stabilizes, we list the model on the public leaderboard.</li> <li>Testing timeline</li> </ol> <ul> <li>Public models will be tested until their leaderboard ranking converges: after 3,000 votes, or earlier if the confidence interval is small enough to distinguish it from surrounding models.</li> <li>If more than 10 models were pre-release tested in parallel, we will mark model scores as “provisional” until 2,000 fresh votes have been collected after the model’s public release.</li> <li>We will release model results to the community on the public leaderboard immediately once the ranking converges. There is one exception: the model provider can reach out before its listing and ask for an one-day heads up. In this case, we will share the rating with the model provider and wait for an additional day before listing the model on the public leaderboard.</li> <li>The sampling weight of a model is set to 5 until 3,000 votes are collected. Then after release we assign the sampling weight to 1. Models may be retired after 3,000 votes if there are two more recent models in the same series (e.g. <code class="language-plaintext highlighter-rouge">gpt-4o-0513</code> and <code class="language-plaintext highlighter-rouge">gpt-4o-0806</code>) and/or if there are more than 3 providers that offer models cheaper or same price and strictly better (according to overall Arena Score) than this model.</li> <li>The top-10 models according to the overall Arena Score will be given a sampling weight of 3. This is to ensure the best community experience when visiting our site.</li> <li>The best model from the top-10 providers according to the overall Arena Score will be given a sampling weight of 1 to ensure diversity of battles.</li> </ul> <p>This policy may be modified moving forward; visit this website for the most recent version.</p> <p><strong>Evaluating unreleased models</strong>: We collaborate with open-source and commercial model providers to bring their unreleased models to community for preview testing.</p> <p>Model providers can test their unreleased models anonymously, meaning the models’ names will be anonymized. A model is considered unreleased if its weights are neither open, nor available via a public API or service. Evaluating an unreleased model consists of the following steps:</p> <ol> <li>Add the model to Arena with an anonymous label. i.e., its identity will not be shown to users.</li> <li>Keep it until we accumulate enough votes for its rating to stabilize or until the model provider withdraws it.</li> <li>Once we accumulate enough votes, we will share the result privately with the model provider. These include the rating, as well as release samples of up to 20% of the votes. (See Sharing data with the model providers for further details).</li> <li>Remove the model from Arena.</li> </ol> <p>If while we test an unreleased model, that model is publicly released, we immediately switch to the publicly released model evaluation process. Model providers are all allowed to test multiple variants of their models pre-release, subject to our system’s constraints.</p> <p>To ensure the leaderboard accurately reflects model rankings, we rely on live comparisons between models. Hence, we may deprecate models from the leaderboard one month after they are no longer available online or publicly accessible. As of June 15, 2025, models that have been retired from battle mode will be recorded in a public list for transparency and future reference.</p> <p><strong>Sharing data with the community</strong>: We will periodically share data with the community. Specifically, as of July 1, 2025, we will share 100% of the arena vote data used for the public leaderboard (model identities and votes), so the leaderboard can be reproduced. We may also share a portion of the prompts and responses, so long as users have consented to the inclusion of this data in a public dataset. For models that have not appeared on the public leaderboard, we may still release data, but the model will be labeled as “anonymous”.</p> <p><strong>Sharing data with the model providers</strong>: Upon request, we will offer early data access with model providers who wish to improve their models. In particular, with a model provider, we will share the data that includes their model’s answers. For battles, we may not reveal the opponent model and may use “anonymous” label. If the model is not on the leaderboard at the time of sharing, the model’s answers will also be labeled as “anonymous”. Before sharing the data, we will remove user PII (e.g., Azure PII detection for texts).</p> <h2 id="faq">FAQ</h2> <h3 id="why-another-eval">Why another eval?</h3> <p>Most LLM benchmarks are static, which makes them prone to contamination, as these LLMs are trained on most available data on the Internet. Chatbot Arena aims to alleviate this problem by providing live evaluation with a continuous stream of new prompts from real people. We also believe that the open nature of the platform will attract users that accurately reflect the broader set of LLM users and real use cases.</p> <h3 id="what-model-to-evaluate-why-not-all">What model to evaluate? Why not all?</h3> <p>We will continuously add new models and retire old ones. It is not feasible to add every possible model due to the cost and the scalability of our evaluation process, i.e., it might take too much to accumulate enough votes to accurately rate each model. Today, the decision to add new models is rather ad-hoc: we add models based on the community’s perceived interest. We intend to formalize his process in the near future.</p> <h3 id="why-should-the-community-trust-our-eval">Why should the community trust our eval?</h3> <p>We seek to provide transparency and all tools as well as the platform we are using in open-source. We invite the community to use our platform and tools to statistically reproduce our results.</p> <h3 id="how-do-you-handle-security">How do you handle security?</h3> <p>Security is always a concern, particularly against DDOS and Sybil attacks. Our safeguards currently include:</p> <ul> <li>Malicious user detection: As outlined in Section 5.1 of the original <a href="https://arxiv.org/abs/2403.04132" rel="external nofollow noopener" target="_blank">Chatbot Arena paper</a>, we identify and handle malicious behavior effectively.</li> <li>Cloudflare bot protection: Applied to all web traffic to defend against automated attacks.</li> <li>Google reCAPTCHA v3: Enabled for each chat and vote to minimize spam and bot activity.</li> <li>Category pipeline: Filters out low-quality data and organizes incoming data into relevant categories.</li> <li>Vote limitations: Limits the number of votes per IP address per day to prevent abuse.</li> <li>Prompt deduplication: Deduplicates overly frequent prompts (e.g., top 0.1%) to ensure a diverse dataset.</li> <li>Delayed leaderboard release: Allows us to manually address any issues that may arise.</li> </ul> <p>We are also planning to introduce further defenses, such as statistical methods and optional user login systems, to enhance security. If you have ideas on how to further improve our security measures, please reach out to us!</p> <h3 id="why-do-you-share-only-20-of-the-data-and-not-all-of-it">Why do you share only 20% of the data and not all of it?</h3> <p>We share 20% of the data to balance transparency with the need to prevent overfitting and benchmark leakage. Sharing the entire dataset could lead to models being overly optimized for specific distribution. By providing a representative subset, we ensure researchers and developers gain meaningful insights while maintaining the integrity of the evaluation process. This policy is regularly reviewed and may adapt based on community feedback to align with best practices.</p> <h3 id="who-will-fund-this-effort-any-conflict-of-interests">Who will fund this effort? Any conflict of interests?</h3> <p>Chatbot Arena is only funded by gifts, in money, cloud credits, or API credits. The gifts have no strings attached.</p> <h2 id="any-feedback">Any feedback?</h2> <p>Feel free to send us email or leave feedback on <a href="https://github.com/lm-sys/FastChat/issues" rel="external nofollow noopener" target="_blank">Github</a>!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"lmarena/lmarena.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 LM Arena. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-",title:"",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-about",title:"about",description:"",section:"Navigation",handler:()=>{window.location.href="/about/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-our-response-to-39-the-leaderboard-illusion-39-writeup",title:"Our Response to &#39;The Leaderboard Illusion&#39; Writeup",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/our-response/"}},{id:"post-celebrating-community-impact-3m-votes-400-models-and-300-pre-release-tests",title:"Celebrating Community Impact: 3M+ votes, 400+ models, and 300+ pre-release tests",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/two-year-celebration/"}},{id:"post-does-sentiment-matter-too",title:"Does Sentiment Matter Too?",description:"Introducing Sentiment Control: Disentagling Sentiment and Substance",section:"Posts",handler:()=>{window.location.href="/blog/2025/sentiment-control/"}},{id:"post-how-many-user-prompts-are-new",title:"How Many User Prompts are New?",description:"Analysis of prompt freshness and benchmark contamination",section:"Posts",handler:()=>{window.location.href="/blog/2025/freshness/"}},{id:"post-lmarena-is-growing-to-support-our-community-platform",title:"LMArena is Growing to Support our Community Platform",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/new-beta/"}},{id:"post-introducing-the-search-arena-evaluating-search-enabled-ai",title:"Introducing the Search Arena: Evaluating Search-Enabled AI",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/search-arena/"}},{id:"post-lmarena-community-updates-looking-ahead",title:"LMArena Community Updates: Looking Ahead",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/new-discord-server/"}},{id:"post-webdev-arena",title:"WebDev Arena",description:"A Live LLM Leaderboard for Web App Development",section:"Posts",handler:()=>{window.location.href="/blog/2025/webdev-arena/"}},{id:"post-repochat-arena",title:"RepoChat Arena",description:"A Live Benchmark for AI Software Engineers",section:"Posts",handler:()=>{window.location.href="/blog/2025/repochat-arena/"}},{id:"post-arena-explorer",title:"Arena Explorer",description:"A topic modeling pipeline for LLM evals &amp; analytics",section:"Posts",handler:()=>{window.location.href="/blog/2025/arena-explorer/"}},{id:"post-code-editing-in-copilot-arena",title:"Code Editing in Copilot Arena",description:"Copilot Arena&#39;s Code Editing Leaderboard and Insights",section:"Posts",handler:()=>{window.location.href="/blog/2025/copilot-arena-edits/"}},{id:"post-copilot-arena",title:"Copilot Arena",description:"Copilot Arena&#39;s Initial Leaderboard, Insights, and a New Prompting Method for Code Completions",section:"Posts",handler:()=>{window.location.href="/blog/2024/copilot-arena/"}},{id:"post-chatbot-arena-categories",title:"Chatbot Arena Categories",description:"Definitions, Methods, and Insights",section:"Posts",handler:()=>{window.location.href="/blog/2024/arena-category/"}},{id:"post-preference-proxy-evaluations",title:"Preference Proxy Evaluations",description:"A New Benchmark for Evaluating Reward Models and LLM Judges",section:"Posts",handler:()=>{window.location.href="/blog/2024/preference-proxy-evaluations/"}},{id:"post-agent-arena",title:"Agent Arena",description:"A Platform for Evaluating and Comparing LLM Agents Across Models, Tools, and Frameworks",section:"Posts",handler:()=>{window.location.href="/blog/2024/agent-arena/"}},{id:"post-statistical-extensions-of-the-bradley-terry-and-elo-models",title:"Statistical Extensions of the Bradley-Terry and Elo Models",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/extended-arena/"}},{id:"post-chatbot-arena-new-blog",title:"Chatbot Arena New Blog",description:"A new chapter for Chatbot Arena!",section:"Posts",handler:()=>{window.location.href="/blog/2024/new-site/"}},{id:"post-redteam-arena",title:"RedTeam Arena",description:"An Open-Source, Community-driven Jailbreaking Platform",section:"Posts",handler:()=>{window.location.href="/blog/2024/redteam-arena/"}},{id:"post-does-style-matter",title:"Does Style Matter?",description:"Disentangling style and substance in Chatbot Arena",section:"Posts",handler:()=>{window.location.href="/blog/2024/style-control/"}},{id:"post-the-multimodal-arena-is-here",title:"The Multimodal Arena is Here!",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/multimodal/"}},{id:"post-introducing-hard-prompts-category-in-chatbot-arena",title:"Introducing Hard Prompts Category in Chatbot Arena",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/hard-prompts/"}},{id:"post-what-39-s-up-with-llama-3-arena-data-analysis",title:"What&#39;s up with Llama 3? Arena data analysis",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/llama3/"}},{id:"post-lmsys-chatbot-arena-kaggle-competition",title:"LMSYS Chatbot Arena Kaggle Competition",description:"Predicting Human Preference with $100,000 in Prizes",section:"Posts",handler:()=>{window.location.href="/blog/2024/kaggle-competition/"}},{id:"post-from-live-data-to-high-quality-benchmarks-the-arena-hard-pipeline",title:"From Live Data to High-Quality Benchmarks - The Arena-Hard Pipeline",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/arena-hard/"}},{id:"post-chatbot-arena-policy",title:"Chatbot Arena Policy",description:"Live and Community-Driven LLM Evaluation",section:"Posts",handler:()=>{window.location.href="/blog/2024/policy/"}},{id:"post-chatbot-arena-new-models-amp-elo-system-update",title:"Chatbot Arena - New models &amp; Elo system update",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-elo-update/"}},{id:"post-catch-me-if-you-can-how-to-beat-gpt-4-with-a-13b-model",title:"Catch me if you can! How to beat GPT-4 with a 13B model...",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/llm-decontaminator/"}},{id:"post-chatbot-arena-conversation-dataset-release",title:"Chatbot Arena Conversation Dataset Release",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/dataset/"}},{id:"post-chatbot-arena-leaderboard-updates-week-8",title:"Chatbot Arena Leaderboard Updates (Week 8)",description:"Introducing MT-Bench and Vicuna-33B",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-week8/"}},{id:"post-chatbot-arena-leaderboard-updates-week-4",title:"Chatbot Arena Leaderboard Updates (Week 4)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-week4/"}},{id:"post-chatbot-arena-leaderboard-updates-week-2",title:"Chatbot Arena Leaderboard Updates (Week 2)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/leaderboard-week2/"}},{id:"post-chatbot-arena",title:"Chatbot Arena",description:"Benchmarking LLMs in the Wild with Elo Ratings",section:"Posts",handler:()=>{window.location.href="/blog/2023/arena/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6C%6D%61%72%65%6E%61.%61%69@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>